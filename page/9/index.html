<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"real-zhangzhe.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhangzhe&#39;s Blog">
<meta property="og:url" content="https://real-zhangzhe.github.io/page/9/index.html">
<meta property="og:site_name" content="Zhangzhe&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhangzhe">
<meta property="article:tag" content="No mistakes in the tango, not like life.">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://real-zhangzhe.github.io/page/9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Zhangzhe's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhangzhe's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The projection of my life.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/archives/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/" class="post-title-link" itemprop="url">ALiBi: Train short, test long: Attention with linear biases enables input length extrapolation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-05 19:02:10" itemprop="dateCreated datePublished" datetime="2024-08-05T19:02:10+08:00">2024-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Position-embedding/" itemprop="url" rel="index"><span itemprop="name">Position embedding</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.12409">https://arxiv.org/pdf/2108.12409</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742">https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出一种比 <code>T5 bias</code> 更简单的 <code>position embedding</code> 方法叫做 <code>ALiBi (Attention with Linear Bias)</code>，简单好用</li>
<li>可以在短数据集上训练，在长数据集上测试，即具有外推性</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="t5-bias"><a class="markdownIt-Anchor" href="#t5-bias"></a> T5 bias</h3>
<ul>
<li>先讲一下 <code>T5 bias</code> 是如何实现 <code>position embedding</code> 的，主要分三步：
<ol>
<li>计算 <code>query / key</code> 的 <code>n * n</code> 相对位置矩阵，形如： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],</span><br><span class="line"> [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8],</span><br><span class="line"> [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7],</span><br><span class="line"> [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6],</span><br><span class="line"> [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5],</span><br><span class="line"> [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4],</span><br><span class="line"> [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3],</span><br><span class="line"> [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2],</span><br><span class="line"> [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1],</span><br><span class="line"> [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0]]</span><br></pre></td></tr></table></figure>
</li>
<li>将相对位置矩阵分桶（超过 <code>num_buckets</code> 的饱和到 <code>num_buckets</code>） <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[ 0, 17, 18, 19, 20, 21, 22, 23, 24, 24],</span><br><span class="line"> [ 1,  0, 17, 18, 19, 20, 21, 22, 23, 24],</span><br><span class="line"> [ 2,  1,  0, 17, 18, 19, 20, 21, 22, 23],</span><br><span class="line"> [ 3,  2,  1,  0, 17, 18, 19, 20, 21, 22],</span><br><span class="line"> [ 4,  3,  2,  1,  0, 17, 18, 19, 20, 21],</span><br><span class="line"> [ 5,  4,  3,  2,  1,  0, 17, 18, 19, 20],</span><br><span class="line"> [ 6,  5,  4,  3,  2,  1,  0, 17, 18, 19],</span><br><span class="line"> [ 7,  6,  5,  4,  3,  2,  1,  0, 17, 18],</span><br><span class="line"> [ 8,  7,  6,  5,  4,  3,  2,  1,  0, 17],</span><br><span class="line"> [ 8,  8,  7,  6,  5,  4,  3,  2,  1,  0]]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里上三角和下三角都有值是因为 <code>encoder bidirection=True</code>，如果是 <code>decoder</code>，则如下：</p>
</blockquote>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [2, 1, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [3, 2, 1, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [4, 3, 2, 1, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [5, 4, 3, 2, 1, 0, 0, 0, 0, 0],</span><br><span class="line"> [6, 5, 4, 3, 2, 1, 0, 0, 0, 0],</span><br><span class="line"> [7, 6, 5, 4, 3, 2, 1, 0, 0, 0],</span><br><span class="line"> [8, 7, 6, 5, 4, 3, 2, 1, 0, 0],</span><br><span class="line"> [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]</span><br></pre></td></tr></table></figure>
</li>
<li>最后是将此 <code>n * n</code> 的 <code>relative position bucket</code> 通过可学习的 <code>embedding</code> 函数变成 <code>n * n * num_heads</code> 的向量，和每个头的 <code>attention score（softmax 之前）</code> 相加，然后通过逐行 <code>softmax</code> 得到 <code>attention weight</code></li>
</ol>
</li>
</ul>
<h3 id="alibi"><a class="markdownIt-Anchor" href="#alibi"></a> ALiBi</h3>
<p><img src="https://s2.loli.net/2024/08/05/R3DH5fENoz8AYQn.png" alt="alibi_1.png" /></p>
<ul>
<li>用数学公式表示：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><msup><mi>K</mi><mi>T</mi></msup><mo>+</mo><mi>m</mi><mo>⋅</mo><mo stretchy="false">[</mo><mo>−</mo><mo stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mo>−</mo><mn>2</mn><mo separator="true">,</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">softmax(q_iK^T+m\cdot[-(i-1),...,-2,-1,0])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.44445em;vertical-align:0em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">−</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></li>
<li><code>ALiBi</code> 的计算和 <code>T5 bias</code> 的前两步几乎一模一样</li>
<li>第三步不再使用可学习的 <code>embedding</code> 函数映射到每个头上，而是将距离矩阵的值和每个头独立的 <strong>不可学习的</strong> 常量 <code>m</code> 值相乘，然后和 <code>attention score</code> 相加</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>h</mi></msub><mo>=</mo><mfrac><mi>b</mi><mrow><mo stretchy="false">(</mo><msup><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>8</mn><mi mathvariant="normal">/</mi><mi>H</mi><mo stretchy="false">)</mo></mrow></msup><mo>⋅</mo><mi>b</mi><msup><mo stretchy="false">)</mo><mi>h</mi></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">m_h = \frac{b}{(2^{(8/H)} \cdot b)^h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4405329999999998em;vertical-align:-0.560425em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.614575em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8220357142857143em;"><span style="top:-2.8220357142857138em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">8</span><span class="mord mtight">/</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight">b</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7820285714285713em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.560425em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>
<ul>
<li><code>b</code> 是一个基数</li>
<li><code>H</code> 是注意力头的数量</li>
<li><code>h</code> 是注意力头的索引（从 <code>0</code> 到 <code>H-1</code>）</li>
</ul>
</li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>标准 <code>attention</code> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>e</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">pe \in \mathbb{R}^{n\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span> 已经慢慢被淘汰了，不管是 <code>RoPE / T5 Bias / ALiBi</code> 都已经逐渐演变成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>e</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">pe \in \mathbb{R}^{n\times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.771331em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span> 直接作用在 <code>attention score</code> 上了</li>
<li><code>ALiBi</code> 的外推性其实本质是强行饱和掉远距离，有点过于粗暴了…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/07/30/KV-Cache-Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/30/KV-Cache-Transformer/" class="post-title-link" itemprop="url">KV Cache Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-30 12:23:17" itemprop="dateCreated datePublished" datetime="2024-07-30T12:23:17+08:00">2024-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/07/30/KV-Cache-Transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/30/KV-Cache-Transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <code>kv cache</code> 是一种生成式大模型加速的方法，没有原创的论文，比较早的提到这项技术的论文是：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.05102">《Efficient Scaling Transformer Inference》</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/f0bc49e7f61f74f055c47ad40e6010f57eed0b0b/src/transformers/models/gpt2/modeling_gpt2.py#L290">https://github.com/huggingface/transformers/blob/f0bc49e7f61f74f055c47ad40e6010f57eed0b0b/src/transformers/models/gpt2/modeling_gpt2.py#L290</a> <code>transformers repo</code> 中的 <code>GPT2</code> 模型代码中包含 <code>use_cache</code> 和 <code>layer_past</code> 就是 <code>kv cache</code> 的实现接口</li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>GPT like</code> 大语言模型目前在生成式大模型领域占了主导地位，这些模型每一次推理只得到一个 <code>token</code>，下一次推理会将得到的 <code>token</code> 放到上一次输入 <code>token</code> 序列的末尾作为新输入，直到达到 <code>max_seq_length</code> 或输出的 <code>token</code> 表示 <code>end of sequence</code></li>
<li>这种 <code>GPT like</code> 的生成机制意味着每次推理需要将之前所有 <code>token</code> 再重复算一次，序列长度越长，计算量越大，耗时越长</li>
<li><code>KV Cache</code> 通过缓存上一次推理过程中每一层 <code>transformer</code> 的 <code>key / value</code>，将推理的时间复杂度从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 降低到了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>，且计算结果完全等价</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<ul>
<li>可以在本时间步使用上一个时间步缓存的 <code>key / value</code> 的必要理论依据是：
<ol>
<li>每个时间步输入的 <code>token</code> 序列除最后一个 <code>token</code> 之外都和上一个时间步输入相同</li>
<li><code>attention mask</code> 是下三角矩阵</li>
<li>对 <code>attention score</code> 矩阵做 <code>softmax</code> 是逐行的而不是全局的或逐列的</li>
<li>除 <code>self-attention</code> 操作之外，<code>GPT like LLM</code> 的其他层（<code>layer norm</code>, <code>gelu</code>, <code>FFN</code> 等）对 <code>seq_len</code> 维度来说都是 <code>element-wise</code> 的，且在推理阶段是静态的</li>
</ol>
</li>
<li>以上四个理论依据在 <code>GPT like</code> 模型中都满足，下面解释四个条件为什么是必不可少的：
<ol>
<li>如果上面第一个条件不满足，那么每次推理都是新的内容，缓存将毫无意义</li>
<li>下三角矩阵和逐行做 <code>softmax</code> 是必不可少的，随着 <code>token</code> 长度加一，每一层的 <code>attention score</code> 矩阵都会在最右和最下分别加一列和一行，由于下三角的存在，除最后一行外，其他每一行的值都不会改变（新加的一列不参与计算），而且逐行做，新加的一列也不影响其他行</li>
<li><code>self-attention</code> 是 <code>GPT like LLM</code> 中唯一一个在 <code>token feature</code> 间做信息融合的算子，其他算子都是在 <code>token feature</code> 内做且都是静态操作，所以随着序列长度变长，之前的序列特征都是静态的</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, layer_idx</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SelfAttention, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.layer_idx = layer_idx</span><br><span class="line">        self.query = nn.Linear(dim, dim)</span><br><span class="line">        self.key = nn.Linear(dim, dim)</span><br><span class="line">        self.value = nn.Linear(dim, dim)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)  <span class="comment"># 重点是这里，逐行进行 softmax，而不是全局</span></span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        q = self.query(x)</span><br><span class="line">        k = self.key(x)</span><br><span class="line">        v = self.value(x)</span><br><span class="line">        scores = torch.matmul(q, k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        scores = scores / torch.sqrt(torch.tensor(self.dim).<span class="built_in">float</span>())</span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">        attention_weights = self.softmax(scores)</span><br><span class="line">        attention_weights = self.dropout(attention_weights)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;layer_idx:&quot;</span>, self.layer_idx)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;q.sum(-1) =&quot;</span>, q.squeeze().<span class="built_in">abs</span>().<span class="built_in">sum</span>(-<span class="number">1</span>).tolist())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;k.sum(-1) =&quot;</span>, k.squeeze().<span class="built_in">abs</span>().<span class="built_in">sum</span>(-<span class="number">1</span>).tolist())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;v.sum(-1) =&quot;</span>, v.squeeze().<span class="built_in">abs</span>().<span class="built_in">sum</span>(-<span class="number">1</span>).tolist())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;attention_weights =\n&quot;</span>, attention_weights.squeeze())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;#&quot;</span> * <span class="number">100</span>)</span><br><span class="line">        output = torch.matmul(attention_weights, v)</span><br><span class="line">        output = self.layer_norm(output + x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GPTModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_layers</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GPTModel, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.attentions = nn.ModuleList(</span><br><span class="line">            [SelfAttention(dim, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)]</span><br><span class="line">        )</span><br><span class="line">        self.linear1 = nn.Linear(dim, dim)</span><br><span class="line">        self.linear2 = nn.Linear(dim, dim)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            x = x + self.attentions[i](x, attn_mask=attn_mask)</span><br><span class="line">            x = self.layer_norm(x)</span><br><span class="line">            x = x + self.linear2(self.dropout(torch.relu(self.linear1(x))))</span><br><span class="line">            x = self.layer_norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">model = GPTModel(dim=<span class="number">64</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">input_data = torch.randn(</span><br><span class="line">    <span class="number">1</span>, <span class="number">3</span>, <span class="number">64</span></span><br><span class="line">)  <span class="comment"># Example input data with shape (batch_size, sequence_length, dim)</span></span><br><span class="line">attn_mask = torch.tril(torch.ones(<span class="number">3</span>, <span class="number">3</span>))  <span class="comment"># Lower triangular attention mask</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;time step 1:&quot;</span>)</span><br><span class="line">output = model(input_data, attn_mask=attn_mask)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n\ntime step 2:&quot;</span>)</span><br><span class="line">input_data = torch.cat((input_data, torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>)), <span class="number">1</span>)</span><br><span class="line">attn_mask = torch.tril(torch.ones(<span class="number">4</span>, <span class="number">4</span>))  <span class="comment"># Lower triangular attention mask</span></span><br><span class="line">output = model(input_data, attn_mask=attn_mask)</span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line"><span class="comment"># time step 1:</span></span><br><span class="line"><span class="comment"># layer_idx: 0</span></span><br><span class="line"><span class="comment"># q.sum(-1) = [31.20478057861328, 34.04328536987305, 33.6611328125]</span></span><br><span class="line"><span class="comment"># k.sum(-1) = [31.446550369262695, 29.15508460998535, 29.265644073486328]</span></span><br><span class="line"><span class="comment"># v.sum(-1) = [30.789640426635742, 33.80058670043945, 32.4859619140625]</span></span><br><span class="line"><span class="comment"># attention_weights =</span></span><br><span class="line"><span class="comment">#  tensor([[1.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.6760, 0.3240, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.2452, 0.4188, 0.3360]], grad_fn=&lt;SqueezeBackward0&gt;)</span></span><br><span class="line"><span class="comment"># ####################################################################################################</span></span><br><span class="line"><span class="comment"># layer_idx: 1</span></span><br><span class="line"><span class="comment"># q.sum(-1) = [30.214162826538086, 34.45262145996094, 26.357181549072266]</span></span><br><span class="line"><span class="comment"># k.sum(-1) = [29.88446617126465, 31.186325073242188, 31.727235794067383]</span></span><br><span class="line"><span class="comment"># v.sum(-1) = [23.99290657043457, 35.66062545776367, 33.28850173950195]</span></span><br><span class="line"><span class="comment"># attention_weights =</span></span><br><span class="line"><span class="comment">#  tensor([[1.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.3310, 0.6690, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.4525, 0.3270, 0.2204]], grad_fn=&lt;SqueezeBackward0&gt;)</span></span><br><span class="line"><span class="comment"># ####################################################################################################</span></span><br><span class="line"><span class="comment"># time step 2:</span></span><br><span class="line"><span class="comment"># layer_idx: 0</span></span><br><span class="line"><span class="comment"># q.sum(-1) = [31.20478057861328, 34.04328536987305, 33.6611328125, 27.077472686767578]</span></span><br><span class="line"><span class="comment"># k.sum(-1) = [31.446550369262695, 29.15508460998535, 29.265644073486328, 27.211992263793945]</span></span><br><span class="line"><span class="comment"># v.sum(-1) = [30.789640426635742, 33.80058670043945, 32.4859619140625, 29.695066452026367]</span></span><br><span class="line"><span class="comment"># attention_weights =</span></span><br><span class="line"><span class="comment">#  tensor([[1.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.6760, 0.3240, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.2452, 0.4188, 0.3360, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.1818, 0.2616, 0.2813, 0.2752]], grad_fn=&lt;SqueezeBackward0&gt;)</span></span><br><span class="line"><span class="comment"># ####################################################################################################</span></span><br><span class="line"><span class="comment"># layer_idx: 1</span></span><br><span class="line"><span class="comment"># q.sum(-1) = [30.214162826538086, 34.45262145996094, 26.357181549072266, 29.209003448486328]</span></span><br><span class="line"><span class="comment"># k.sum(-1) = [29.88446617126465, 31.186325073242188, 31.727235794067383, 26.988731384277344]</span></span><br><span class="line"><span class="comment"># v.sum(-1) = [23.99290657043457, 35.66062545776367, 33.28850173950195, 27.92920684814453]</span></span><br><span class="line"><span class="comment"># attention_weights =</span></span><br><span class="line"><span class="comment">#  tensor([[1.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.3310, 0.6690, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.4525, 0.3270, 0.2204, 0.0000],</span></span><br><span class="line"><span class="comment">#         [0.2740, 0.0698, 0.4069, 0.2492]], grad_fn=&lt;SqueezeBackward0&gt;)</span></span><br><span class="line"><span class="comment"># ####################################################################################################</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以看到随着时间步的推移，每一层的 <code>x / q / k / v / attention weight</code> 的前 <code>n-1</code> 个 <code>token feature</code> 都和上一个时间步相同，因此可以缓存</p>
</blockquote>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>由于在 <code>t</code> 时间步，需要计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">Q^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9879959999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>t</mi></mrow></msup><mo separator="true">,</mo><mtext> </mtext><msup><mi>V</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K^{0,1,...,t},\ V^{0,1,...,t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span></span> 之间的内积，所以需要换成 <code>key / value</code> 的缓存，而不是 <code>query</code> 的缓存</li>
<li><code>paged attention</code> 是通过类似内存分页管理的方式管理 <code>kv cache</code> 的一种方法，动态分配显存，速度更快，支持更高 <code>batch</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/07/18/FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/18/FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning/" class="post-title-link" itemprop="url">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-18 12:50:28" itemprop="dateCreated datePublished" datetime="2024-07-18T12:50:28+08:00">2024-07-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/07/18/FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/18/FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2307.08691">https://arxiv.org/pdf/2307.08691</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/FlashAttention20/blob/main/attention.py">https://github.com/kyegomez/FlashAttention20/blob/main/attention.py</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>FlashAttention-2</code> 是 <code>FlashAttention</code> 一作对前一版的更新，基本思想不变，只优化了部分算法流程，可以比 <code>V1</code> 提速一倍</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<p><img src="https://s2.loli.net/2024/07/18/FkzaMcX5VhjnQWo.png" alt="flash_attention_2.png" /></p>
<ul>
<li><code>Flash Attention 2</code> 前向过程主要优化了几个方面：
<ol>
<li><code>Flash Attention v1</code> 中计算 <code>P_ij</code> 计算过程很绕，需要更新两次全局信息，<code>v2</code> 中进行了优化</li>
<li><code>Flash Attention v1</code> 中内外层循环设置不合理，<code>v2</code> 对调了内外层循环，这样做的好处是：</li>
<li>不再需要重复读入写出 <code>O</code></li>
<li><code>max_value</code> 信息变成一个计算中间变量，不再需要读入写出，<code>backward</code> 过程也无需依赖</li>
</ol>
</li>
<li><code>Flash Attention v2</code> 和 <code>Flash Attention v1</code> 的 <code>IO</code> 量对比
<ul>
<li><code>v1</code>：
<ul>
<li>读：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>N</mi><mi>d</mi><mo>+</mo><mn>2</mn><msub><mi>T</mi><mi>c</mi></msub><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">2Nd+2T_cNd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span></li>
<li>写：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub><mi>N</mi><mi>d</mi><mo>+</mo><mn>2</mn><msub><mi>T</mi><mi>c</mi></msub><mi>N</mi></mrow><annotation encoding="application/x-tex">T_cNd+2T_cN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
</ul>
</li>
<li><code>v2</code>：
<ul>
<li>读：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>d</mi><mo>+</mo><mn>2</mn><msub><mi>T</mi><mi>r</mi></msub><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">Nd+2T_rNd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span></li>
<li>写：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>d</mi><mo>+</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">Nd+N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">BLOCK_SIZE = <span class="number">1024</span></span><br><span class="line">NEG_INF = -<span class="number">1e10</span>  <span class="comment"># -infinity</span></span><br><span class="line">EPSILON = <span class="number">1e-10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flash_attention2_forward</span>(<span class="params">Q, K, V</span>):</span></span><br><span class="line">    batch, seq_len, dim = Q.shape</span><br><span class="line">    O = torch.zeros_like(Q, requires_grad=<span class="literal">True</span>, device=Q.device)</span><br><span class="line">    l = torch.zeros((batch, seq_len, <span class="number">1</span>), device=Q.device)  <span class="comment"># 1 for broadcasting</span></span><br><span class="line">    m = torch.ones((batch, seq_len, <span class="number">1</span>), device=Q.device) * NEG_INF</span><br><span class="line">    Q = <span class="built_in">list</span>(torch.split(Q, BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    K = <span class="built_in">list</span>(torch.split(K, BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    V = <span class="built_in">list</span>(torch.split(V, BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    O = <span class="built_in">list</span>(torch.split(O, BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    l = <span class="built_in">list</span>(torch.split(l, BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    m = <span class="built_in">list</span>(torch.split(m, BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    Tr = <span class="built_in">len</span>(Q)</span><br><span class="line">    Tc = <span class="built_in">len</span>(K)</span><br><span class="line">    scale = <span class="number">1</span> / math.sqrt(dim)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tr):</span><br><span class="line">        Qi_scaled = Q[i] * scale</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Tc):</span><br><span class="line">            S_ij = torch.einsum(<span class="string">&quot;bnd,bmd-&gt;bnm&quot;</span>, Qi_scaled, K[j])</span><br><span class="line">            m_ij, _ = torch.<span class="built_in">max</span>(S_ij, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            mi_new = torch.maximum(m_ij, m[i])</span><br><span class="line">            P_ij = torch.exp(S_ij - mi_new)</span><br><span class="line">            P_ijV_j = torch.einsum(<span class="string">&quot;bnm,bmd-&gt;bnd&quot;</span>, P_ij, V[j])</span><br><span class="line">            exp_row_max_diff = torch.exp(m[i] - mi_new)</span><br><span class="line">            li_new = l[i] * exp_row_max_diff + P_ij.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            O[i] = O[i] * exp_row_max_diff + P_ijV_j</span><br><span class="line">            m[i] = mi_new</span><br><span class="line">            l[i] = li_new</span><br><span class="line">        O[i] = O[i] / l[i]</span><br><span class="line">    O = torch.cat(O, dim=<span class="number">1</span>)</span><br><span class="line">    l = torch.cat(l, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> O, l</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flash_attention2_backward</span>(<span class="params">Q, K, V, O, l, dO</span>):</span></span><br><span class="line">    <span class="comment"># 参考代码中进行了实现，但这里省略，重点关注一下 backward 过程的输入</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="comment"># batch, seq_len, dim</span></span><br><span class="line">    Q = torch.randn(<span class="number">2</span>, <span class="number">4096</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>).to(device)</span><br><span class="line">    K = torch.randn(<span class="number">2</span>, <span class="number">4096</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>).to(device)</span><br><span class="line">    V = torch.randn(<span class="number">2</span>, <span class="number">4096</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>).to(device)</span><br><span class="line">    flash2_out = flash_attention2_forward(Q, K, V)</span><br><span class="line">    <span class="built_in">print</span>(flash2_out[<span class="number">0</span>].<span class="built_in">sum</span>().item())</span><br></pre></td></tr></table></figure>
<ul>
<li><code>backward</code> 过程在 <code>v2</code> 论文中详细的写出来了，如下：<br />
<img src="https://s2.loli.net/2024/07/18/X7e8OgNiIAaysLk.png" alt="flash2_2.png" /></li>
<li>速度效果提升很明显<br />
<img src="https://s2.loli.net/2024/07/18/kQVDHWJI7eAFLXR.png" alt="flash2_3.png" /></li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li><code>Flash Attention v2</code> 看起来特别像 <code>Flash Attention</code> 的紧急修复版本，把之前写的冗余删掉了</li>
<li>甚至完全不了解 <code>Flash Attention</code> 思想的人看 <code>Flash Attention</code> 的实现代码，都可以将其优化到 <code>Flash Attention v2</code>，就是简单的代码优化…</li>
<li><code>Flash Attention v2</code> 彻底把这个 <code>tiling</code> 思路的优势发挥出来了</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/07/17/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/17/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/" class="post-title-link" itemprop="url">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-17 08:48:11" itemprop="dateCreated datePublished" datetime="2024-07-17T08:48:11+08:00">2024-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/07/17/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/17/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.14135">https://arxiv.org/pdf/2205.14135</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/shreyansh26/FlashAttention-PyTorch/blob/master/flash_attention.py">https://github.com/shreyansh26/FlashAttention-PyTorch/blob/master/flash_attention.py</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>Flash Attention</code> 是标准 <code>Attention</code> 的一种完全数学等价的高效实现</li>
<li>标准 <code>Self-attention</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V)=softmax({\frac{QK^T}{\sqrt d}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.627473em;vertical-align:-0.538em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.089473em;"><span style="top:-2.5335085em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.937845em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal mtight" style="padding-left:0.833em;">d</span></span><span style="top:-2.8978450000000002em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.102155em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 时间复杂度的，随着大模型的发展，序列长度 <code>n</code> 越来越大，带来的时间复杂度和 <code>IO</code> 量越来越恐怖</li>
<li><code>GPU</code> 或 <code>NPU</code> 等计算设备的 <code>SRAM</code> 空间有限，无法放下所有后续依赖的计算中间结果，而 <code>softmax</code> 运算又需要全局信息（无法直接简单 <code>tiling</code>），所以这里存在巨大的 <code>IO</code> 量（<code>HBM (high bandwidth memory)</code> 和 <code>SRAM</code> 之间）</li>
<li>本文设计了一种 <code>tiling</code> 的局部计算 <code>self-attention</code> 的方法，这种方法在 <code>forward</code> 和 <code>backward</code> 中都可以使用</li>
<li>在 <code>forward</code> 中，<code>FlashAttention</code> 会计算并存储 <code>softmax</code> 操作的归一化因子（下图中的 <code>l</code> 和 <code>m</code>），在 <code>backward</code> 中，利用这些归一化因子，重新计算所需的中间注意力矩阵，而不需要从 <code>HBM</code> 中读取存储的完整矩阵</li>
<li>上述两点改进可以保证和标准 <code>self-attention</code> 计算结果（包括 <code>forward</code> 和 <code>backward</code>）完全一致的情况下，极大降低 <code>IO</code> 量（代价是计算量的小幅上涨）</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<p><img src="https://s2.loli.net/2025/08/15/2aD8tXuGMUxwWdN.jpg" alt="flashattn_banner.jpg" /></p>
<h3 id="1-standard-attention-implementation"><a class="markdownIt-Anchor" href="#1-standard-attention-implementation"></a> 1. Standard attention implementation</h3>
<p><img src="https://s2.loli.net/2024/07/17/v2wFgNH5Mt8cznu.png" alt="flash_1.png" /></p>
<ul>
<li>这里之所以要频繁和 <code>HBM</code> 以 <code>block</code> 为单位交互是因为有一个限制是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>≤</mo><mi>M</mi><mo>≤</mo><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">d\le M \le Nd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83041em;vertical-align:-0.13597em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.13597em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span>，即 <code>SRAM</code> 可存储的数据量在 <code>d</code> 和 <code>Nd</code> 之间（<code>N</code> 和 <code>d</code> 分别是 <code>seq_len</code> 和 <code>model_dim</code>）</li>
<li><code>IO</code> 总量：
<ul>
<li>读：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">2N^2+3Nd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span>
<ul>
<li>读 <code>Q / K / V</code></li>
<li>读 <code>S / P</code></li>
</ul>
</li>
<li>写：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">2N^2+Nd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span>
<ul>
<li>写 <code>S / P</code></li>
<li>写 <code>O</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-flash-attention-forward-implementation"><a class="markdownIt-Anchor" href="#2-flash-attention-forward-implementation"></a> 2. Flash attention forward implementation</h3>
<p><img src="https://s2.loli.net/2024/07/17/FZuDBUT6zcWvkli.png" alt="flash_2.png" /></p>
<ul>
<li><code>IO</code> 总量：
<ul>
<li>读：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>N</mi><mi>d</mi><mo>+</mo><mn>2</mn><msub><mi>T</mi><mi>c</mi></msub><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">2Nd+2T_cNd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span>
<ul>
<li>读 <code>K / V</code> 一次</li>
<li>读 <code>Q / O</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">T_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次</li>
</ul>
</li>
<li>写：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub><mi>N</mi><mi>d</mi><mo>+</mo><mn>2</mn><msub><mi>T</mi><mi>c</mi></msub><mi>N</mi></mrow><annotation encoding="application/x-tex">T_cNd+2T_cN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>
<ul>
<li>写 <code>O</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">T_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次</li>
<li>写 <code>l / m</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">T_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 次</li>
</ul>
</li>
</ul>
</li>
<li>由上述可知：
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">T_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示 <code>Q</code> 的分块数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">T_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示 <code>K</code> 和 <code>V</code> 的分块数</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>≤</mo><msub><mi>T</mi><mi>c</mi></msub><mo>≤</mo><mn>4</mn><mi>N</mi></mrow><annotation encoding="application/x-tex">4\le T_c\le 4N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li>因此，读写上下限分别为：</li>
<li>读：
<ul>
<li>上限：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>N</mi><mi>d</mi><mo>+</mo><mn>8</mn><msup><mi>N</mi><mn>2</mn></msup><mi>d</mi></mrow><annotation encoding="application/x-tex">2Nd+8N^2d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">8</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span></span></span></span></li>
<li>下限：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">10Nd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span></li>
</ul>
</li>
<li>写：
<ul>
<li>上限：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><msup><mi>N</mi><mn>2</mn></msup><mi>d</mi><mo>+</mo><mn>8</mn><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">4N^2d+8N^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">8</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li>
<li>下限：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>N</mi><mi>d</mi><mo>+</mo><mn>8</mn><mi>N</mi></mrow><annotation encoding="application/x-tex">4Nd+8N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">8</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
</ul>
</li>
<li><code>Block size</code> 越大，总 <code>IO</code> 量越小</li>
</ul>
</li>
</ul>
<h3 id="3-详解-flash-attention-过程"><a class="markdownIt-Anchor" href="#3-详解-flash-attention-过程"></a> 3. 详解 <code>Flash attention</code> 过程</h3>
<ul>
<li>标准 <code>softmax</code> 过程：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>N</mi></msubsup><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>N</mi></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">softmax(x_{ij})=\frac{e^{x_{ij}-max(x)}}{\sum_{i=0}^{N}\sum_{j=0}^{N}e^{x_{ij}-max(x)}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.936237em;vertical-align:-0.837232em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.099005em;"><span style="top:-2.4849949999999996em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8852357142857143em;"><span style="top:-2.1785614285714283em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-2.8971428571428572em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32143857142857146em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8852357142857143em;"><span style="top:-2.1785614285714283em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-2.8971428571428572em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.46032428571428574em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.00715em;"><span style="top:-3.00715em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5091600000000001em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">x</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.00715em;"><span style="top:-3.00715em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.65952em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5091600000000001em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">x</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.837232em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，为了计算过程的数值稳定，需要对每个值减去序列最大值</li>
<li><code>Flash attention</code> 中，会将 <code>QKV</code> 都在时序维度切分成若干块，然后在小块之间计算 <code>Attention</code>，直接计算是不等价的，因为需要全局信息，例如：
<ul>
<li>全局最大值 <code>m(x)</code></li>
<li>指数累加和 <code>l(x)</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">BLOCK_SIZE = <span class="number">1024</span></span><br><span class="line">NEG_INF = -<span class="number">1e10</span>  <span class="comment"># -infinity</span></span><br><span class="line">EPSILON = <span class="number">1e-10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flash_attention_forward</span>(<span class="params">Q, K, V</span>):</span></span><br><span class="line">    batch, seq_len, dim = Q.shape</span><br><span class="line">    O = torch.zeros_like(Q, requires_grad=<span class="literal">True</span>, device=Q.device)</span><br><span class="line">    l = torch.zeros((batch, seq_len, <span class="number">1</span>), device=Q.device)  <span class="comment"># 1 for broadcasting</span></span><br><span class="line">    m = torch.ones((batch, seq_len, <span class="number">1</span>), device=Q.device) * NEG_INF</span><br><span class="line">    Q_BLOCK_SIZE = <span class="built_in">min</span>(BLOCK_SIZE, seq_len)</span><br><span class="line">    KV_BLOCK_SIZE = BLOCK_SIZE</span><br><span class="line">    Q = torch.split(Q, Q_BLOCK_SIZE, dim=<span class="number">1</span>)</span><br><span class="line">    K = torch.split(K, KV_BLOCK_SIZE, dim=<span class="number">1</span>)</span><br><span class="line">    V = torch.split(V, KV_BLOCK_SIZE, dim=<span class="number">1</span>)</span><br><span class="line">    O = <span class="built_in">list</span>(torch.split(O, Q_BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    l = <span class="built_in">list</span>(torch.split(l, Q_BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    m = <span class="built_in">list</span>(torch.split(m, Q_BLOCK_SIZE, dim=<span class="number">1</span>))</span><br><span class="line">    Tr = <span class="built_in">len</span>(Q)</span><br><span class="line">    Tc = <span class="built_in">len</span>(K)</span><br><span class="line">    scale = <span class="number">1</span> / math.sqrt(dim)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Tc):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tr):</span><br><span class="line">            Qi_scaled = Q[i] * scale</span><br><span class="line">            S_ij = torch.einsum(<span class="string">&quot;... i d, ... j d -&gt; ... i j&quot;</span>, Qi_scaled, K[j])</span><br><span class="line">            m_ij, _ = torch.<span class="built_in">max</span>(S_ij, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            P_ij = torch.exp(S_ij - m_ij)</span><br><span class="line">            mi_new = torch.maximum(m_ij, m[i])</span><br><span class="line">            l_ij = (</span><br><span class="line">                torch.<span class="built_in">sum</span>(P_ij * torch.exp(m_ij - mi_new), dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">                + EPSILON</span><br><span class="line">            )</span><br><span class="line">            P_ij_Vj = torch.einsum(</span><br><span class="line">                <span class="string">&quot;... i j, ... j d -&gt; ... i d&quot;</span>, P_ij * torch.exp(m_ij - mi_new), V[j]</span><br><span class="line">            )</span><br><span class="line">            li_new = torch.exp(m[i] - mi_new) * l[i] + l_ij</span><br><span class="line">            O[i] = (O[i] * l[i] * torch.exp(m[i] - mi_new) + P_ij_Vj) / li_new</span><br><span class="line">            l[i] = li_new</span><br><span class="line">            m[i] = mi_new</span><br><span class="line">    O = torch.cat(O, dim=<span class="number">1</span>)</span><br><span class="line">    l = torch.cat(l, dim=<span class="number">1</span>)</span><br><span class="line">    m = torch.cat(m, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> O, l, m</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flash_attention_backward</span>(<span class="params">Q, K, V, O, l, m, dO</span>):</span></span><br><span class="line">    <span class="comment"># 参考代码中进行了实现，但这里省略，重点关注一下 backward 过程的输入</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># batch, seq_len, dim</span></span><br><span class="line">    Q = torch.randn(<span class="number">2</span>, <span class="number">4096</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>).to(device=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    K = torch.randn(<span class="number">2</span>, <span class="number">4096</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>).to(device=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    V = torch.randn(<span class="number">2</span>, <span class="number">4096</span>, <span class="number">128</span>, requires_grad=<span class="literal">True</span>).to(device=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    out = flash_attention_forward(Q, K, V)</span><br><span class="line">    <span class="built_in">print</span>(out[<span class="number">0</span>].<span class="built_in">sum</span>().item())</span><br></pre></td></tr></table></figure>
<ul>
<li>这里最重要也最难理解的一行代码是：<code>O[i] = (O[i] * l[i] * torch.exp(m[i] - mi_new) + P_ij_Vj) / li_new</code>，这个仔细分析一下
<ul>
<li><code>O[i] * l[i]</code> 是计算得到上次外循环（<code>j-1</code>）时的分子</li>
<li><code>O[i] * l[i] * torch.exp(m[i] - mi_new)</code> 是把 <code>j-1</code> 时刻存储的信息更新到最新（调整 <code>max_value</code>）</li>
<li><code>O[i] * l[i] * torch.exp(m[i] - mi_new) + P_ij_Vj</code> 因为 <code>O</code> 的初始值是 <code>0</code>，因此这个公式可以拆解为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>c</mi></msub></msubsup><mfrac><mrow><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∗</mo><msub><mi>V</mi><mi>j</mi></msub></mrow><msub><mi>l</mi><mi>i</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">O_i = \sum_{j=1}^{T_c} \frac{P_{ij} * V_j}{l_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4307509999999999em;vertical-align:-0.44509999999999994em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.13889em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.985651em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.01968em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.50732em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.13889em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.22222em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，通过不断更新 <code>max_value</code> 和累加求和得到最终结果</li>
</ul>
</li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li><code>Flash Attention</code> 的效率和 <code>block size</code> 相关，<code>block size</code> 越大，效率提升越明显；在一些 <code>block size</code> 较小的情况下，效率可能会比标准 <code>Attention</code> 更差</li>
<li><code>Flash Attention</code> 目标是解决 <code>Memory hierarchy</code> 情况下标注 <code>Attention</code> 计算访存比较低导致 <code>IO</code> 开销大的问题，是一种以计算换空间的做法；当 <code>SRAM</code> 相比模型足够大时，标准 <code>Attention</code> 效率比 <code>Flash Attention</code> 更高</li>
<li><code>backward</code> 过程也是通过几乎相同的方式计算的，<code>forward</code> 阶段计算得到的中间 <code>l / m</code> 都会用于计算 <code>backward</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/07/12/GQA-Grouped-Query-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/12/GQA-Grouped-Query-Attention/" class="post-title-link" itemprop="url">GQA: Grouped-Query Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-12 12:55:14" itemprop="dateCreated datePublished" datetime="2024-07-12T12:55:14+08:00">2024-07-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/07/12/GQA-Grouped-Query-Attention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/12/GQA-Grouped-Query-Attention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245">https://arxiv.org/pdf/2305.13245</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本质是标准 <code>Multi-Head Attention</code> 和 <code>Multi-Query Attention</code> 的一个折衷。<br />
<img src="https://s2.loli.net/2024/07/12/zPEebjTWqMphBVt.png" alt="GQA.png" /></li>
<li>目的是 <strong>降低 <code>Attention</code> 过程内存带宽成本</strong>，并没有降低计算复杂度，计算复杂度依然是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiquery_attention_batched</span>(<span class="params">X, M, mask, P_q, P_k, P_v, P_o, num_groups</span>):</span></span><br><span class="line">    <span class="comment"># b: batch size</span></span><br><span class="line">    <span class="comment"># n: number of querys</span></span><br><span class="line">    <span class="comment"># m: number of keys / values</span></span><br><span class="line">    <span class="comment"># d: input dimension</span></span><br><span class="line">    <span class="comment"># H: number of heads</span></span><br><span class="line">    <span class="comment"># g: number of groups</span></span><br><span class="line">    <span class="comment"># h: number of heads per group</span></span><br><span class="line">    <span class="comment"># k: key / query dimention</span></span><br><span class="line">    <span class="comment"># v: value dimention</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> == P_q.size(<span class="number">0</span>) % num_groups</span><br><span class="line">    heads_per_group = P_q.size(<span class="number">0</span>) // num_groups</span><br><span class="line">    <span class="comment"># 应用线性变换获取查询Q（multi-head）</span></span><br><span class="line">    Q = torch.einsum(<span class="string">&quot;bnd,Hdk-&gt;bHkn&quot;</span>, X, P_q).view(</span><br><span class="line">        -<span class="number">1</span>, num_groups, heads_per_group, P_q.size(<span class="number">2</span>), X.size(<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 应用线性变换获取键K和值V（single-head）</span></span><br><span class="line">    K = torch.einsum(<span class="string">&quot;bmd,gdk-&gt;bgmk&quot;</span>, M, P_k)</span><br><span class="line">    V = torch.einsum(<span class="string">&quot;bmd,gdv-&gt;bgmv&quot;</span>, M, P_v)</span><br><span class="line">    <span class="comment"># 计算注意力得分并应用掩码</span></span><br><span class="line">    logits = torch.einsum(<span class="string">&quot;bghkn,bgmk-&gt;bghnm&quot;</span>, Q, K)</span><br><span class="line">    <span class="comment"># mask 分组</span></span><br><span class="line">    mask = mask.view(-<span class="number">1</span>, num_groups, heads_per_group, mask.size(-<span class="number">2</span>), mask.size(-<span class="number">1</span>))</span><br><span class="line">    weights = F.softmax(logits + mask, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算加权的值</span></span><br><span class="line">    O = torch.einsum(<span class="string">&quot;bghnm,bgmv-&gt;bghnv&quot;</span>, weights, V)</span><br><span class="line">    <span class="comment"># P_o 分组</span></span><br><span class="line">    P_o = P_o.view(num_groups, heads_per_group, P_o.size(-<span class="number">2</span>), P_o.size(-<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 应用输出变换</span></span><br><span class="line">    Y = torch.einsum(<span class="string">&quot;bghnv,ghvd-&gt;bnd&quot;</span>, O, P_o)</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"><span class="comment"># 假设参数和维度</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">num_queries = <span class="number">5</span></span><br><span class="line">num_keys = <span class="number">7</span>  <span class="comment"># 同时也是 num_values</span></span><br><span class="line">dim = <span class="number">64</span></span><br><span class="line">key_dim = <span class="number">32</span>  <span class="comment"># 同时也是 query_dim</span></span><br><span class="line">value_dim = <span class="number">48</span></span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line">num_groups = <span class="number">4</span>  <span class="comment"># head 分成多少组，必须整除 num_heads</span></span><br><span class="line"><span class="comment"># 随机初始化输入数据和参数</span></span><br><span class="line">X = torch.randn(batch_size, num_queries, dim)</span><br><span class="line">M = torch.randn(batch_size, num_keys, dim)</span><br><span class="line">mask = torch.randn(batch_size, num_heads, num_queries, num_keys)</span><br><span class="line">P_q = torch.randn(num_heads, dim, key_dim)</span><br><span class="line">P_k = torch.randn(num_groups, dim, key_dim)     <span class="comment"># 组件 key / value 不共享</span></span><br><span class="line">P_v = torch.randn(num_groups, dim, value_dim)</span><br><span class="line">P_o = torch.randn(num_heads, value_dim, dim)</span><br><span class="line"><span class="comment"># 运行多查询注意力机制</span></span><br><span class="line">output = multiquery_attention_batched(X, M, mask, P_q, P_k, P_v, P_o, num_groups)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># 应该输出: torch.Size([2, 5, 64])</span></span><br></pre></td></tr></table></figure>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li><code>GQA</code> 介于 <code>MHA</code> 和 <code>MQA</code> 之间，每个 <code>group</code> 内部实际上是 <code>MQA</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/07/11/MQA-Multi-Query-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/11/MQA-Multi-Query-Attention/" class="post-title-link" itemprop="url">MQA: Multi-Query Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-11 13:24:35" itemprop="dateCreated datePublished" datetime="2024-07-11T13:24:35+08:00">2024-07-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/07/11/MQA-Multi-Query-Attention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/11/MQA-Multi-Query-Attention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.02150">https://arxiv.org/pdf/1911.02150</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本质很简单，就是把标准的 <code>Multi-Head Attention</code> 中的 <code>Key</code> 和 <code>Value</code> 退化为 <code>Single-Head</code>，<code>Query</code> 保留 <code>Multi-Head</code></li>
<li>目的是 <strong>降低 <code>Attention</code> 过程内存带宽成本</strong>，并没有降低计算复杂度，计算复杂度依然是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiquery_attention_batched</span>(<span class="params">X, M, mask, P_q, P_k, P_v, P_o</span>):</span></span><br><span class="line">    <span class="comment"># b: batch size</span></span><br><span class="line">    <span class="comment"># n: number of querys</span></span><br><span class="line">    <span class="comment"># m: number of keys / values</span></span><br><span class="line">    <span class="comment"># d: input dimension</span></span><br><span class="line">    <span class="comment"># h: number of heads</span></span><br><span class="line">    <span class="comment"># k: key / query dimention</span></span><br><span class="line">    <span class="comment"># v: value dimention</span></span><br><span class="line">    <span class="comment"># 应用线性变换获取查询Q（multi-head）</span></span><br><span class="line">    Q = torch.einsum(<span class="string">&quot;bnd,hdk-&gt;bhkn&quot;</span>, X, P_q)</span><br><span class="line">    <span class="comment"># 应用线性变换获取键K和值V（single-head）</span></span><br><span class="line">    K = torch.einsum(<span class="string">&quot;bmd,dk-&gt;bmk&quot;</span>, M, P_k)</span><br><span class="line">    V = torch.einsum(<span class="string">&quot;bmd,dv-&gt;bmv&quot;</span>, M, P_v)</span><br><span class="line">    <span class="comment"># 计算注意力得分并应用掩码</span></span><br><span class="line">    logits = torch.einsum(<span class="string">&quot;bhkn,bmk-&gt;bhnm&quot;</span>, Q, K)</span><br><span class="line">    weights = F.softmax(logits + mask, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算加权的值</span></span><br><span class="line">    O = torch.einsum(<span class="string">&quot;bhnm,bmv-&gt;bhnv&quot;</span>, weights, V)</span><br><span class="line">    <span class="comment"># 应用输出变换</span></span><br><span class="line">    Y = torch.einsum(<span class="string">&quot;bhnv,hvd-&gt;bnd&quot;</span>, O, P_o)</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"><span class="comment"># 假设参数和维度</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">num_queries = <span class="number">5</span></span><br><span class="line">num_keys = <span class="number">7</span>    <span class="comment"># 同时也是 num_values</span></span><br><span class="line">dim = <span class="number">64</span></span><br><span class="line">key_dim = <span class="number">32</span>    <span class="comment"># 同时也是 query_dim</span></span><br><span class="line">value_dim = <span class="number">48</span></span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line"><span class="comment"># 随机初始化输入数据和参数</span></span><br><span class="line">X = torch.randn(batch_size, num_queries, dim)</span><br><span class="line">M = torch.randn(batch_size, num_keys, dim)</span><br><span class="line">mask = torch.randn(batch_size, num_heads, num_queries, num_keys)</span><br><span class="line">P_q = torch.randn(num_heads, dim, key_dim)</span><br><span class="line">P_k = torch.randn(dim, key_dim)</span><br><span class="line">P_v = torch.randn(dim, value_dim)</span><br><span class="line">P_o = torch.randn(num_heads, value_dim, dim)</span><br><span class="line"><span class="comment"># 运行多查询注意力机制</span></span><br><span class="line">output = multiquery_attention_batched(X, M, mask, P_q, P_k, P_v, P_o)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># 输出: torch.Size([2, 5, 64])</span></span><br></pre></td></tr></table></figure>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>本质是标准 <code>Attention</code> 的部分 <code>multi-head</code> 化</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/07/01/LLM-%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/01/LLM-%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/" class="post-title-link" itemprop="url">LLM 面试题汇总</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-01 13:35:13" itemprop="dateCreated datePublished" datetime="2024-07-01T13:35:13+08:00">2024-07-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/07/01/LLM-%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/07/01/LLM-%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="llm-自动驾驶相关"><a class="markdownIt-Anchor" href="#llm-自动驾驶相关"></a> LLM / 自动驾驶相关</h2>
<ol>
<li>LN和BN</li>
<li>介绍RLHF</li>
<li>介绍MoE和变体</li>
<li>介绍LoRA和变体</li>
<li>LoRA 参数更新机制</li>
<li>如何减轻LLM中的幻觉现象？</li>
<li>MLM和MIM的关系和区别?</li>
<li>Stable Diffusion的技术原理</li>
<li>解決LLM Hallucination的方法</li>
<li>Occupancy预测的出发点是什么?</li>
<li>介绍RWKV、Mamba和Mamba-2</li>
<li>2D图像预训练怎么迁移到3D点云任务</li>
<li>为什么现在的LLM都是Decoder only的架构?</li>
<li>把Transformer模型训深的问题有哪些?怎么解决</li>
<li>现在车道线检测的主流的loss是什么?你有哪些想法?</li>
<li>为什么GAN中经常遇到mode collapse，而Diffusion比较少?</li>
</ol>
<h2 id="transformer-相关"><a class="markdownIt-Anchor" href="#transformer-相关"></a> Transformer 相关</h2>
<ol>
<li>介绍Transformer和ViT</li>
<li>介绍Transformer的QKV</li>
<li>介绍Layer Normalization</li>
<li>Transformer训练和部署技巧</li>
<li>介绍Transformer的位置编码</li>
<li>介绍自注意力机制和数学公式</li>
<li>介绍Transformer的Encoder模块</li>
<li>介绍Transformer的Decoder模块</li>
<li>Transformer和Mamba（SSM）的区别</li>
<li>Transformer中的残差结构以及意义</li>
<li>为什么Transformer适合多模态任务？</li>
<li>Transformer的并行化体现在哪个地方？</li>
<li>为什么Transformer一般使用LayerNorm？</li>
<li>Transformer为什么使用多头注意力机制？</li>
<li>Transformer训练的Dropout是如何设定的？</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/06/17/HiPPO-Recurrent-Memory-with-Optimal-Polynomial-Projections/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/17/HiPPO-Recurrent-Memory-with-Optimal-Polynomial-Projections/" class="post-title-link" itemprop="url">HiPPO: Recurrent Memory with Optimal Polynomial Projections</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-17 13:29:21" itemprop="dateCreated datePublished" datetime="2024-06-17T13:29:21+08:00">2024-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/SSM/" itemprop="url" rel="index"><span itemprop="name">SSM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/06/17/HiPPO-Recurrent-Memory-with-Optimal-Polynomial-Projections/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/06/17/HiPPO-Recurrent-Memory-with-Optimal-Polynomial-Projections/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.07669">https://arxiv.org/pdf/2008.07669</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/hippo-code">https://github.com/HazyResearch/hippo-code</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>HiPPO</code> 全称是 <code>High-order Polynomial Projection Operators</code>，是 <code>SSM (State Space Model)</code> 的开山之作，之后作者延续 <code>SSM</code> 思路，做出了可以和 <code>Transformer</code> 结构掰手腕的 <code>Mamba</code></li>
<li><code>HiPPO</code> 的目标是用一个有限维的向量来储存这一段 <code>u(t)</code> 的信息，实现方式是将 <code>u(t)</code> 通过 <code>Legendre</code> (勒让德)多项式展开，用有限维的向量存储勒让德多项式系数，且这些 <strong>向量的值通过求解勒让德多项式得出，不在训练过程中通过梯度下降更新</strong></li>
<li><code>HiPPO</code> 可以给 <code>RNN</code> 提供一种记忆表示方法，因此一个实际的用处是使用 <code>HiPPO</code> 作为 <code>RNN</code> 的记忆存储算子</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<ul>
<li>勒让德多项式本身是定义在连续函数上的，但实际使用中需要记忆的内容是离散的，因此需要离散化过程</li>
<li><code>HiPPO</code> 的记忆更新公式是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mi>m</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>B</mi><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">m(t+1) = Am(t)+Bu(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span>，其中 <code>A</code> 和 <code>B</code> 是 <code>HiPPO</code> 参数，<code>m(t)</code> 表示记忆向量，<code>u(t)</code> 表示更新向量，<code>m(t+1)</code> 表示更新后的记忆向量
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A\in\mathbb R^{N\times N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">B\in\mathbb R^{N\times 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li><code>N</code> 表示记忆单元的参数尺度，类似于 <code>Transformer</code> 的 <code>hidden size</code>，越大记忆能力越强</li>
</ul>
</li>
<li><code>HiPPO</code> 有 <code>LegT (Translated Legendre Measure)</code> 和 <code>LegS (Scaled Legendre Measure)</code> 两种度量方法，二者都使用上述记忆更新公式，只是 <code>A</code> 和 <code>B</code> 参数不同
<ul>
<li><code>LegT</code> 使用 <strong>翻译</strong> 任务的勒让德多项式，本质是一个滑动窗口，只记忆当前时刻前 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> 窗口内容，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> 为超参数
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>n</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>θ</mi></mfrac><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mrow><mi>n</mi><mo>−</mo><mi>k</mi></mrow></msup><mo stretchy="false">(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>n</mi><mo>≥</mo><mi>k</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>n</mi><mo>&lt;</mo><mi>k</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">A_{nk}=\frac{1}{\theta}
\begin{cases}
(-1)^{n-k}(2n+1) &amp; if &amp; n\ge k\\
2n+1 &amp; if &amp; n &lt; k
\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>n</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>θ</mi></mfrac><mo stretchy="false">(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">B_n=\frac{1}{\theta}(2n+1)(-1)^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li><code>LegS</code> 使用 <strong>缩放</strong> 的勒让德多项式，记忆全部时刻的序列内容
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>n</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>θ</mi></mfrac><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo stretchy="false">(</mo><mn>2</mn><mi>k</mi><mo>+</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>n</mi><mo>&gt;</mo><mi>k</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>n</mi><mo>=</mo><mi>k</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>n</mi><mo>&lt;</mo><mi>k</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">A_{nk}=\frac{1}{\theta}
\begin{cases}
(2n+1)^{\frac{1}{2}}(2k+1)^{\frac{1}{2}} &amp; if &amp; n\gt k\\
n+1 &amp; if &amp; n = k\\
0 &amp; if &amp; n &lt; k
\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.32em;vertical-align:-1.9099999999999997em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35002em;"><span style="top:-2.19999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-2.19499em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-2.20499em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-3.15001em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.2950099999999996em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.30501em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.60002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.8500199999999998em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9540200000000001em;"><span style="top:-3.363em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142858em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9540200000000001em;"><span style="top:-3.363em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142858em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-2.97em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-1.5300000000000002em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9099999999999997em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span><span style="top:-2.97em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span><span style="top:-1.5300000000000002em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9099999999999997em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span><span style="top:-2.97em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span><span style="top:-1.5300000000000002em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9099999999999997em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>n</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow><annotation encoding="application/x-tex">B_n=(2n+1)^{\frac{1}{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.20402em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9540200000000001em;"><span style="top:-3.363em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142858em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
</ul>
</li>
<li>以 <code>Permute Mnist</code> 分类任务的例子讲解 <code>HiPPO</code> 如何作为 <code>RNN</code> 的单元参与计算，以及<code>HiPPO</code> 的记忆单元如何更新</li>
<li><code>Permute Mnist</code> 任务是将 <code>28x28</code> 的 <code>Mnist</code> 图像的每一类按照同一种 <code>pattern</code> 进行 <code>shuffle</code>，训练并分类</li>
<li>下图为使用 <code>HiPPO</code> 作为记忆单元的 <code>RNN</code> 网络解决 <code>Permute Mnist</code> 任务的计算过程，<code>input_t</code> 是每次顺序输入图片的一个像素值，是一个时间步总长为 <code>28 * 28 = 784</code> 的 <code>RNN</code> 网络，最后一个 <code>hidden state</code> 输出映射到 <code>class dim</code> 上进行分类</li>
</ul>
<pre class="mermaid">graph TD
    subgraph input;
    input_t([input_t]);
    h_t;
    end;
    subgraph fully_connect;
    W_hxm;
    W_gxm;
    W_uxh;
    end;
    h_t([h_t])-->|512|Concat_1-->|513|W_uxh-->|1|u_t([u_t]);
    input_t([input_t])-->|1|Concat_1[Concat];
    subgraph update_memory;
    A([A])-->|max_length, 512, 512|get_index_A[get_index]-->|512, 512|A_t;
    timestep([timestep])-->get_index_A;
    A_t([A_t])-->|512, 512|MatMul_A[MatMul];
    m_t([m_t])-->|1, 512|MatMul_A-->|1, 512|Add;
    m_t([m_t])-->|1, 512|Add;
    timestep([timestep])-->get_index_B;
    B([B])-->|max_length, 512|get_index_B[get_index]-->|512, 1|B_t;
    B_t([B_t])-->|512, 1|MatMul_B[MatMul];
    u_t([u_t])-->|1|MatMul_B-->|1, 512|Add-->|1, 512|m_t+1([m_t+1]);
    end;
    m_t+1-->|512|Concat_2;
    input_t-->|1|Concat_2[Concat]-->|513|W_hxm-->|512|Tanh-->|512|hidden([hidden]);
    Concat_2-->|513|W_gxm-->|512|gate([gate]);
    h_t-->|512|Alpha_Blending-->|512|h_t+1([h_t+1])-->|512|until_last_h{until_last_h};
    hidden-->|512|Alpha_Blending;
    gate-->|512|Alpha_Blending;
    subgraph output;
    until_last_h-->|512|map_to_class_dim-->|10|classification_result([classification_result]);
    end;</pre>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li><code>LegT</code> 和 <code>LegS</code> 的参数计算过程需要较强的数学功底才能完全理解</li>
<li>如果只把 <code>A</code> 和 <code>B</code> 当做 <strong>万能的不需要梯度下降更新的神经网络记忆力更新参数</strong>，那么实际上并不复杂</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/06/03/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/03/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/" class="post-title-link" itemprop="url">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-03 13:24:49" itemprop="dateCreated datePublished" datetime="2024-06-03T13:24:49+08:00">2024-06-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/06/03/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/06/03/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683">https://arxiv.org/pdf/1910.10683</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/t5">https://github.com/huggingface/transformers/tree/main/src/transformers/models/t5</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>T5</code> 名字的由来是：<code>Text-to-Text Transfer Transformer</code>（<strong>文本到文本转换的 Transformer</strong>）</li>
<li><code>T5</code> 使用了 <code>《Attention is all you need》</code> 中提出的标准 <code>Transformer</code> 网络，没有任何改变</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="example"><a class="markdownIt-Anchor" href="#example"></a> example</h3>
<ul>
<li><code>T5</code> 是一个 <code>encoder-decoder</code> 架构的模型，可以用来做文本翻译，本例子使用 <code>Hello, world!</code> 英语翻译法语为例</li>
</ul>
<h4 id="0-prompt"><a class="markdownIt-Anchor" href="#0-prompt"></a> 0. prompt</h4>
<ul>
<li><code>prompt</code> 的作用是在输入之前加上对任务的描述</li>
<li>比如 <code>english_to_franch(&quot;Hello, world!&quot;) API</code> 会被 <code>prompt</code> 为 <code>&quot;translate English to French: Hello, world!&quot;</code> 纯文本输入到模型</li>
</ul>
<h4 id="1-encoder-input-tokenize"><a class="markdownIt-Anchor" href="#1-encoder-input-tokenize"></a> 1. encoder input tokenize</h4>
<ul>
<li><code>T5</code> 使用的分词算法是 <code>unigram</code>，词表可以在 <a target="_blank" rel="noopener" href="https://huggingface.co/google-t5/t5-base/blob/main/tokenizer.json">https://huggingface.co/google-t5/t5-base/blob/main/tokenizer.json</a> 这里找到</li>
<li><code>&quot;translate English to French: Hello, world!&quot;</code> 会被 <code>tokenize</code> 为：<code>[13959, 1566, 12, 2379, 10, 8774, 6, 296, 55, 1]</code></li>
</ul>
<h4 id="2-encoder-input-token-embedding"><a class="markdownIt-Anchor" href="#2-encoder-input-token-embedding"></a> 2. encoder input token embedding</h4>
<ul>
<li>和 <code>GPT</code> 系列没有区别，需要把 <code>encoder input token id</code> 查表变成 <code>token embedding</code></li>
</ul>
<h4 id="3-encoder-input-position-encoding"><a class="markdownIt-Anchor" href="#3-encoder-input-position-encoding"></a> 3. encoder input position encoding</h4>
<ul>
<li>与 <code>GPT</code> 系列使用可学习的 <code>position embedding</code> 不同，<code>T5</code> 使用的是 <code>position encoding</code></li>
<li>且使用的是相对位置编码，而不是绝对位置编码</li>
<li>与 <code>GPT</code> 系列只在模型 <code>casual decoder</code> 第一层输入加入 <code>position embedding</code> 不同，<code>T5</code> 的 <code>position encoding</code> 是在 <code>encoder</code> 以及 <code>decoder</code> 的每一层都是使用了</li>
</ul>
<h4 id="4-encoder"><a class="markdownIt-Anchor" href="#4-encoder"></a> 4. encoder</h4>
<ul>
<li>与 <code>GPT</code> 系列直接使用 <code>token embedding</code> + <code>position embedding</code> 直接得到 <code>hidden state</code> 来输入 <code>decoder</code> 不同，<code>T5</code> 有 <code>encoder</code> 结构</li>
<li><code>T5</code> 的 <code>encoder</code> 结构采用标准 <code>transformer encoder</code> 结构，<strong>每个 <code>token</code> 可以看到所有 <code>token</code>（双向注意力机制）</strong></li>
<li><code>encoder</code> 一共 <code>12</code> 层，每一层包括如下顺序结构为：
<ul>
<li><code>self attention block</code>
<ul>
<li><code>layer norm</code></li>
<li><code>self attention</code></li>
<li><code>dropout</code></li>
</ul>
</li>
<li><code>FFN</code>
<ul>
<li><code>layer norm</code></li>
<li><code>MLP</code></li>
<li><code>dropout</code></li>
</ul>
</li>
</ul>
</li>
<li><strong><code>encoder</code> 最终输出一个 <code>shape = (batch, input_token_len, encoder_dim)</code> 的 <code>encoder hidden state</code></strong></li>
</ul>
<h4 id="5-decoder-input-token-embedding-and-position-encoding"><a class="markdownIt-Anchor" href="#5-decoder-input-token-embedding-and-position-encoding"></a> 5. decoder input token embedding and position encoding</h4>
<ul>
<li>与 <code>GPT</code> 系列不同之处在于 <code>T5</code> 在 <code>decoder</code> 阶段需要 <code>decoder input</code></li>
<li>通常情况下 <code>decoder input</code> 是 <code>&lt;BOS&gt;</code> 或 <code>&lt;S&gt;</code> 等特殊标记，<code>token</code> 长度仅仅为 <code>1</code>，用于表示序列开始</li>
<li><code>decoder input token embedding</code> 和 <code>position encoding</code> 过程和 <code>encoder input token embedding</code> 和 <code>position encoding</code> 并无区别</li>
<li><strong><code>token embedding</code> + <code>position encoding</code> 得到 <code>decoder hidden state</code>，其 <code>shape = (batch, 1, decoder_dim)</code></strong></li>
</ul>
<h4 id="6-decoder"><a class="markdownIt-Anchor" href="#6-decoder"></a> 6. decoder</h4>
<ul>
<li><code>decoder</code> 一共 <code>12</code> 层，每一层包括如下顺序结构为：
<ul>
<li><code>self attention block</code>
<ul>
<li><code>layer norm</code></li>
<li><code>self attention</code></li>
<li><code>dropout</code></li>
</ul>
</li>
<li><code>cross attention block</code>
<ul>
<li><code>layer norm</code></li>
<li><code>self attention</code></li>
<li><code>dropout</code></li>
</ul>
</li>
<li><code>FFN</code>
<ul>
<li><code>layer norm</code></li>
<li><code>MLP</code></li>
<li><code>dropout</code></li>
</ul>
</li>
</ul>
</li>
<li>其中 <code>self attention</code> 的输入是 <code>decoder hidden state</code>（<strong>注意不是 <code>encoder hidden state</code></strong>），在 <code>self attention</code> 中，和 <code>GPT</code> 类似，采用 <strong>单向注意力</strong></li>
<li><code>decoder hidden state</code> 和 <code>encoder hidden state</code> 输入到 <code>cross attention</code>中，<code>Cross attention</code> 和 <code>Self attention</code> 实际上只有一个区别：
<ul>
<li><strong><code>self attention</code> 的 <code>query / key / value</code> 都由同一个 <code>hidden state</code> 得到，因此称为 <code>self</code></strong></li>
<li><strong><code>cross attention</code> 的 <code>key / value</code> 由同一个 <code>hidden state</code> 得到，<code>query</code> 由另一个 <code>hidden state</code> 得到，因此称为 <code>cross</code></strong></li>
<li><strong>在 <code>encoder-decoder</code> 架构的 <code>transformer</code> 中，<code>decoder</code> 中的 <code>cross attention</code> 的 <code>key / value</code> 通常由 <code>encoder output hidden state</code> 得到，<code>query</code> 通常由 <code>decoder hidden state</code> 得到</strong></li>
<li><code>Cross attention</code> 中每个 <code>decoder hidden state</code> 可以查询到所有的 <code>encoder hidden state</code></li>
</ul>
</li>
<li>重复跑完 <code>12</code> 层，最终输出 <code>shape = (batch, 1, decoder_dim)</code> 的 <code>decoder output hidden state</code></li>
</ul>
<h4 id="7-decoder-output-hidden-state-to-token"><a class="markdownIt-Anchor" href="#7-decoder-output-hidden-state-to-token"></a> 7. decoder output hidden state to token</h4>
<ul>
<li>需要将 <code>docoder output hidden state</code> 用一层 <code>MLP</code> 转化到 <code>vocabulary</code> 空间，找到最可能的一个 <code>token</code></li>
<li>此 <code>token</code> 对应的单词即为模型最终输出的第一个词。</li>
<li>如果这个词是词表中的结束符，则停止输出。如果不是，则用此词替代前一个词，重复上述的 <strong>5. decoder input token embedding and position encoding</strong> 和 <strong>6. decoder</strong> 和 <strong>7. decoder output hidden state to token</strong> 过程，直到达到最长输出长度限制或出现停止符。</li>
</ul>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p>标准 <code>transformer</code> 的行为</p>
<ol>
<li><code>encoder</code> 输入所有文本，<strong>双向注意力</strong>，得到 <code>encoder hidden state</code></li>
<li><code>decoder</code> 输入初始化为 <code>&lt;BOS&gt;</code>，长度为 <code>1</code></li>
<li><code>decoder</code> 每一层包含：
<ol>
<li><code>self attention</code>，<strong>单向注意力</strong></li>
<li><code>cross attention</code>，<strong>双向注意力</strong>，<code>decoder hidden state</code> 做 <code>query</code>，<code>encoder hidden states</code> 做 <code>key and value</code></li>
<li><code>FFN</code></li>
</ol>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/04/24/[WIP]A-Survey-of-Large-Language-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/04/24/%5BWIP%5DA-Survey-of-Large-Language-Models/" class="post-title-link" itemprop="url">A Survey of Large Language Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-04-24 13:24:21" itemprop="dateCreated datePublished" datetime="2024-04-24T13:24:21+08:00">2024-04-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Survey/" itemprop="url" rel="index"><span itemprop="name">Survey</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/04/24/%5BWIP%5DA-Survey-of-Large-Language-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/04/24/%5BWIP%5DA-Survey-of-Large-Language-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h1>
<ul>
<li>paper(en): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a></li>
<li>paper(cn)：<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf">https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf</a></li>
</ul>
<h1 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h1>
<ul>
<li>一篇关于大模型的综述，截止到 2023 年 9 月 ，对现有的大模型做了较为详细的梳理。</li>
</ul>
<h1 id="survey"><a class="markdownIt-Anchor" href="#survey"></a> Survey</h1>
<h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2>
<ul>
<li><code>LLM (large language model)</code> 和 <code>PLM (pretrain language model)</code> 主要有三个区别：
<ol>
<li><code>LLM</code> 表现出一些令人惊讶的涌现能力，这些能力可能在以前较小的 <code>PLM</code> 中没有观察到。</li>
<li><code>LLM</code> 将彻底改变人类开发和使用人工智能算法的方式，与小型 <code>PLM</code> 不同，访问 <code>LLM</code> 的主要方法是通过提示接口（例如 <code>GPT-4 API</code>）。</li>
<li><code>LLM</code> 的发展不再明确区分研究和工程。训练 <code>LLM</code> 需要在大规模数据处理和分布式并行训练方面具有丰富的实践经验。</li>
</ol>
</li>
<li>本文主要从四个方面介绍 <code>LLM</code> 的进展：
<ol>
<li><strong>预训练</strong>：如何训练出一个有能力的 <code>LLM</code></li>
<li><strong>适配微调</strong>：如何从有效性和安全性两个角度有效地微调预训练的 <code>LLM</code></li>
<li><strong>使用</strong>：如何利用 <code>LLM</code> 解决各种下游任务</li>
<li><strong>能力评估</strong>：如何评估 LLM 的能力和现有的经验性发现</li>
</ol>
</li>
</ul>
<h2 id="2-overview"><a class="markdownIt-Anchor" href="#2-overview"></a> 2. Overview</h2>
<h3 id="21-大语言模型的涌现能力"><a class="markdownIt-Anchor" href="#21-大语言模型的涌现能力"></a> 2.1 大语言模型的涌现能力</h3>
<p><code>LLM</code> 的涌现能力（<code>Emergent Abilities</code>）被正式定义为：<strong>在小型模型中不存在但在大型模型中产生的能力</strong>，这里介绍三种典型涌现能力：</p>
<h4 id="1-in-context-learning上下文学习"><a class="markdownIt-Anchor" href="#1-in-context-learning上下文学习"></a> 1. <strong><code>In-context learning</code></strong>（上下文学习）</h4>
<ul>
<li><code>ICL</code> 能力是由 <code>GPT-3</code> 正式引入的：假设已经为语言模型提供了一个自然语言指令和/或几个任务演示，它可以通过完成输入文本的单词序列的方式来为测试实例生成预期的输出，而无需额外的训练或梯度更新。</li>
</ul>
<h4 id="2-instruction-following指令遵循"><a class="markdownIt-Anchor" href="#2-instruction-following指令遵循"></a> 2. <strong><code>Instruction following</code></strong>（指令遵循）</h4>
<ul>
<li>通过使用自然语言描述的混合多任务数据集进行微调（称为指令微调），<code>LLM</code> 在未见过的以指令形式描述的任务上表现出色。</li>
</ul>
<h4 id="3-step-by-step-reasoning逐步推理"><a class="markdownIt-Anchor" href="#3-step-by-step-reasoning逐步推理"></a> 3. <strong><code>Step-by-step reasoning</code></strong>：（逐步推理）</h4>
<ul>
<li>通过使用思维链（<code>Chain-of-Thought, CoT</code>）提示策略，<code>LLM</code> 可以通过利用包含中间推理步骤的提示机制来解决这类任务，从而得出最终答案。<br />
<img src="https://s2.loli.net/2024/05/06/59Wjo8XOhNeETSi.png" alt="LLM_survey_1.png" /></li>
</ul>
<h3 id="22-大语言模型的关键技术"><a class="markdownIt-Anchor" href="#22-大语言模型的关键技术"></a> 2.2 大语言模型的关键技术</h3>
<p><code>LLM</code> 的关键技术主要分为以下五个方面：</p>
<h4 id="1-scaling扩展"><a class="markdownIt-Anchor" href="#1-scaling扩展"></a> 1. <strong><code>Scaling</code>（扩展）</strong></h4>
<ul>
<li><code>Transformer</code> 语言模型存在明显的扩展效应，更大的模型/更大的数据规模/更多的训练计算通常会导致模型能力的提升。</li>
</ul>
<h4 id="2-training训练"><a class="markdownIt-Anchor" href="#2-training训练"></a> 2. <strong><code>Training</code>（训练）</strong></h4>
<ul>
<li>分布式训练算法是学习 <code>LLM</code> 网络参数所必需的，其中通常联合使用各种并行策略。</li>
<li>为了支持分布式训练，已经发布了一些优化框架来促进并行算法的实现和部署，例如 <code>DeepSpeed</code> 和 <code>Megatron-LM</code>。</li>
<li>此外，优化技巧对于训练稳定性和模型性能也很重要，例如预训练以克服训练损失激增和混合精度训练等。</li>
</ul>
<h4 id="3-ability-eliciting能力引导"><a class="markdownIt-Anchor" href="#3-ability-eliciting能力引导"></a> 3. <strong><code>Ability eliciting</code>（能力引导）</strong></h4>
<ul>
<li>在大规模语料库上预训练之后，<code>LLM</code> 具备了作为通用任务求解器的潜在能力。</li>
<li>然而，当 <code>LLM</code> 执行一些特定任务时，这些能力可能不会显式地展示出来。</li>
<li>作为技术手段，设计合适的任务指令或具体的 <code>ICL</code> 策略可以激发这些能力。</li>
</ul>
<h4 id="4-alignment-tuning对齐微调"><a class="markdownIt-Anchor" href="#4-alignment-tuning对齐微调"></a> 4. <strong><code>Alignment tuning</code>（对齐微调）</strong></h4>
<ul>
<li><code>InstructGPT</code> 设计了一种有效的微调方法，使 <code>LLM</code> 能够按照期望的指令进行操作，其中利用了 <strong>基于人类反馈的强化学习技术（RLHF）</strong>，采用精心设计的标注策略，它将人类反馈纳入训练循环中。</li>
</ul>
<h4 id="5-tools-manipulation操作工具"><a class="markdownIt-Anchor" href="#5-tools-manipulation操作工具"></a> 5. <strong><code>Tools manipulation</code>（操作工具）</strong></h4>
<ul>
<li>利用外部工具可以进一步扩展 <code>LLM</code> 的能力。例如，<code>LLM</code> 可以利用计算器进行准确计算，利用搜索引擎检索未知信息，这种机制可以广泛扩展 <code>LLM</code> 的能力范围。</li>
</ul>
<h3 id="23-gpt-系列模型的演进"><a class="markdownIt-Anchor" href="#23-gpt-系列模型的演进"></a> 2.3 GPT 系列模型的演进</h3>
<p><img src="https://s2.loli.net/2024/05/07/awqKH82WlQrif3V.png" alt="LLM_survey_2.png" /></p>
<h4 id="1-gpt-1-2018-年"><a class="markdownIt-Anchor" href="#1-gpt-1-2018-年"></a> 1. <strong><code>GPT-1</code>: 2018 年</strong></h4>
<ul>
<li><code>2018</code> 年，<code>OpenAI</code> 发布了 <code>GPT-1</code>，代表生成式预训练（<code>Generative Pre-Training</code>）。</li>
<li><code>GPT-1</code> 是基于<strong>生成型的、仅含有解码器</strong>的 <code>Transformer</code> 架构开发的，并采用了<strong>无监督预训练和有监督微调</strong>的混合方法。</li>
<li><code>GPT-1</code> 为 <code>GPT</code> 系列模型建立了核心架构，并确立了对自然语言文本进行建模的基本原则，即<strong>预测下一个单词</strong>。</li>
</ul>
<h4 id="2-gpt-2-2019-年"><a class="markdownIt-Anchor" href="#2-gpt-2-2019-年"></a> 2. <strong><code>GPT-2</code>: 2019 年</strong></h4>
<ul>
<li>将参数规模增加到了 <code>15</code> 亿，并使用大规模的网页数据集 <code>WebText</code> 进行训练。</li>
<li>它旨在通过<strong>无监督语言建模来执行任务，而无需使用标记数据进行显式微调</strong>。</li>
<li>尽管 <code>GPT-2</code> 旨在成为一个<strong>无监督的多任务学习器</strong>，但与监督微调的 <code>SOTA</code> 方法相比，其整体性能仍然较差。</li>
</ul>
<h4 id="3-gpt-3-2020-年"><a class="markdownIt-Anchor" href="#3-gpt-3-2020-年"></a> 3. <strong><code>GPT-3</code>: 2020 年</strong></h4>
<ul>
<li>参数规模增加到了 <code>175</code> 亿，引入了 <code>ICL</code> 的概念，它是以小样本或零样本的方式使用 <code>LLM</code>。<code>ICL</code> 可以指导 <code>LLM</code> 理解以自然语言文本的形式给出的任务。</li>
<li><code>GPT-3</code> 不仅在各种 <code>NLP</code> 任务中表现出色，而且在一些需要推理或领域适配能力的特殊设计的任务中也表现出色。</li>
<li><code>GPT-3</code> 可以被视 为从 <code>PLM</code> 到 <code>LLM</code> 进化过程中的一个重要里程碑。它通过实证证明，将神经网络扩展到大的规模可以大幅增加模型的能力。</li>
<li><code>OpenAI</code> 为了提高 <code>GPT-3</code> 的性能，使用了两种策略：
<ol>
<li><strong>使用代码数据进行训练</strong>：
<ul>
<li>原始的 <code>GPT-3</code> 模型（在纯文本上进行预训练）的一个主要限制在于缺乏复杂任务的推理能力，例如完成代码和解决数学问题。</li>
<li><code>OpenAI</code> 在 <code>2021.07</code> 推出了 <code>Codex</code>，这是一个在大量 <code>GitHub</code> 代码上微调的 <code>GPT</code> 模型，<code>Codex</code> 可以解决非常困难的编程问题，并且在数学问题上有显著的性能提升。</li>
<li>实际上，<code>GPT-3.5</code> 模型是在基于代码的 <code>GPT</code> 模型（<code>code-davinci-002</code>）的基础上开发的。</li>
</ul>
</li>
<li><strong>与人类对齐</strong>：
<ul>
<li><code>InstructGPT</code> 在 <code>2022.01</code> 提出，以改进 <code>GPT-3</code> 模型与人类对齐能力，正式建立了一个三阶段的基于人类反馈的强化学习（<code>RLHF</code>）算法。</li>
<li>除了提高指令遵循能力之外，<code>RLHF</code> 算法对于缓解有害或有毒内容的生成问题十分有效，这对于 <code>LLM</code> 在实践中的安全部署至关重要。</li>
<li><code>OpenAI</code> 在对齐研究中的方法，总结了三个有前途的方向：
<ol>
<li>使用人类反馈训练 <code>AI</code> 系统</li>
<li>协助人类评估</li>
<li>做对齐研究</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="4-chatgpt-2022-年"><a class="markdownIt-Anchor" href="#4-chatgpt-2022-年"></a> 4. <strong><code>ChatGPT</code>: 2022 年</strong></h4>
<ul>
<li>它是以类似 <code>InstructGPT</code> 的方式进行训练的（在原始文章中称为“InstructGPT 的姊妹模型”），但专门针对对话能力进行了优化。</li>
<li><code>ChatGPT</code> 训练数据是通过将人类生成的对话（扮演用户和 <code>AI</code> 两个角色）与 <code>InstructGPT</code> 数据集结合起来以对话形式生成。</li>
<li><code>ChatGPT</code> 在与人类的交流中表现出卓越的能力：
<ol>
<li>拥有丰富的知识库</li>
<li>擅长解决数学问题</li>
<li>准确追踪多轮对话中的上下文</li>
<li>与人类的价值观保持一致以确保被安全使用</li>
</ol>
</li>
<li><code>ChatGPT</code> 支持了插件机制，进一步通过已有工具或应用扩展了 <code>ChatGPT</code> 的功能。</li>
</ul>
<h4 id="5-gpt-4-2023-年"><a class="markdownIt-Anchor" href="#5-gpt-4-2023-年"></a> 5. <strong><code>GPT-4</code>: 2023 年</strong></h4>
<ul>
<li>将文本输入扩展到多模态信号，性能有大幅提升。</li>
<li><code>GPT-4</code> 对于具有恶意或挑衅的提问的响应更加安全，并采用了多种干预策略来减轻语言模型的可能问题，如幻觉、隐私和过度依赖。</li>
</ul>
<h2 id="3-大语言模型公开可用资源"><a class="markdownIt-Anchor" href="#3-大语言模型公开可用资源"></a> 3. 大语言模型公开可用资源</h2>
<h3 id="31-公开可用的模型检查点或-api"><a class="markdownIt-Anchor" href="#31-公开可用的模型检查点或-api"></a> 3.1 公开可用的模型检查点或 <code>API</code></h3>
<p><img src="https://s2.loli.net/2024/05/07/1E2x79ysCUiYGea.png" alt="LLM_survey_3.png" /><br />
<img src="https://s2.loli.net/2024/05/07/G8oTZV6sxrq4p2f.png" alt="LLM_survey_4.png" /><br />
<img src="https://s2.loli.net/2024/05/07/r386TGoA2cNUdh9.png" alt="LLM_survey_5.png" /></p>
<h3 id="32-常用预训练语料库"><a class="markdownIt-Anchor" href="#32-常用预训练语料库"></a> 3.2 常用预训练语料库</h3>
<p>常用的<strong>用于预训练</strong>的语料库有：</p>
<h4 id="1-books"><a class="markdownIt-Anchor" href="#1-books"></a> 1. <strong><code>Books</code></strong></h4>
<ol>
<li><code>BookCorpus</code> 是之前小规模模型（如 <code>GPT</code> 和 <code>GPT-2</code>）中常用的预训练数据集，包含<strong>超过 <code>11,000</code> 本电子书</strong>，涵盖广泛的主题和类型（如小说和传记）。</li>
<li><code>Gutenberg</code> 是更大的数据语料库，包含<strong>超过 <code>70,000</code> 本文学作品</strong>，包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共领域的作品。</li>
<li>在 <code>GPT-3</code> 中使用到的 <code>Books1</code> 和 <code>Books2</code> 是比 <code>Gutenberg</code> 大的多的语料库，但并未开源。</li>
</ol>
<h4 id="2-commoncrawl"><a class="markdownIt-Anchor" href="#2-commoncrawl"></a> 2. <strong><code>CommonCrawl</code></strong></h4>
<ol>
<li><code>CommonCrawl</code> 是最大的开源网络爬虫数据库之一，能力达到了百万亿字节级别，已经被广泛运用于训练 <code>LLM</code>。</li>
<li>由于网络数据中存在大量的噪音和低质量信息，因此使用前需要进行数据预处理。目前有四个较为常用的基于 <code>CommonCrawl</code> 的过滤数据集：
<ol>
<li>C4</li>
<li>CC-Stories</li>
<li>CC-News</li>
<li>RealNews</li>
</ol>
</li>
</ol>
<h4 id="3-reddit-link"><a class="markdownIt-Anchor" href="#3-reddit-link"></a> 3. <strong><code>Reddit Link</code></strong></h4>
<ol>
<li><code>Reddit</code> 是一个社交媒体平台，用户可以在上 面提交链接和帖子，其他人可以通过“赞同”或“反对”投票。高赞的帖子通常被认为对多数用户是有帮助的，可以用来创建高质量的数据集。</li>
<li><code>WebText</code> 就是一个著名的基于 <code>Reddit</code> 的 语料库，它由 <code>Reddit</code> 上高赞的链接组成，但<strong>尚未公开</strong>。</li>
<li>作为替代，有一个易于获取的开源替代品叫做 <code>OpenWebText</code>。</li>
<li>另一个从 <code>Reddit</code> 中提取的语料库是 <code>PushShift.io</code>.</li>
</ol>
<h4 id="4-wikipedia"><a class="markdownIt-Anchor" href="#4-wikipedia"></a> 4. <strong><code>Wikipedia</code></strong></h4>
<ol>
<li><code>Wikipedia</code> 是一个在线百科全书，包含大量高质量的文章，涵盖各种主题。其中大部分文章都采用解释性写作风格（并支持引用），覆盖了多种不同语言和广泛的知识领域。</li>
</ol>
<h4 id="5-code"><a class="markdownIt-Anchor" href="#5-code"></a> 5. <strong><code>Code</code></strong></h4>
<ol>
<li>为了收集代码数据，现有工作主要是从互联网上爬取有开源许可证的代码。代码数据有两个主要来源：
<ol>
<li>包括开源许可证的公共代码库（例如 <code>GitHub</code>）</li>
<li>与代码相关的问答平台（例如 <code>StackOverflow</code>）</li>
</ol>
</li>
<li><code>Google</code> 公开发布了 <code>BigQuery</code> 数据集，其中包括各种编程语言的大量开源许可证代码片段， 是一个典型的代码数据集。</li>
</ol>
<h4 id="6-other"><a class="markdownIt-Anchor" href="#6-other"></a> 6. <strong><code>Other</code></strong></h4>
<ol>
<li><code>The Pile</code> 是一个大规模、多样化、开源的文本数据集，有超过 <code>800GB</code> 数据，内容包括书籍、网站、代码、科学论文和社交媒体平台等。它由 <code>22</code> 个多样化的高质量子集构成。</li>
<li><code>ROOTS</code> 由各种较小的数据集（完全为 <code>1.61 TB</code> 的文本）组成，涵盖了 <code>59</code> 种不同的语言（包含自然语言和传统语言）。</li>
</ol>
<h4 id="7-一些经典模型使用的预训练语料库"><a class="markdownIt-Anchor" href="#7-一些经典模型使用的预训练语料库"></a> 7. 一些经典模型使用的预训练语料库</h4>
<ol>
<li><strong><code>GPT-3（175B）</code></strong> 是在混合数据集（共 <code>3000</code> 亿 <code>token</code>）上进行训练的，包括：
<ol>
<li><code>CommonCrawl</code></li>
<li><code>WebText2</code></li>
<li><code>Books1</code></li>
<li><code>Books2</code></li>
<li><code>Wikipedia</code></li>
</ol>
</li>
<li><strong><code>PaLM（540B）</code></strong> 使用了共包含 <code>7800</code> 亿 <code>token</code> 的数据集，包括：
<ol>
<li>社交媒体对话</li>
<li>过滤后的网页</li>
<li>书籍</li>
<li><code>Github</code></li>
<li>多语言维基百科</li>
<li>新闻</li>
</ol>
</li>
<li><strong><code>LLaMA</code></strong> 使用了更多的数据预训练，其中 <code>LLaMA（6B）</code> 和 <code>LLaMA（13B）</code> 的训练数据大小为 <code>1.0</code> 万亿 <code>token</code>，而 <code>LLaMA（32B）</code> 和  <code>LLaMA（65B）</code> 使用了 <code>1.4</code> 万亿 <code>token</code>，包括：
<ol>
<li><code>CommonCrawl</code></li>
<li><code>C4</code></li>
<li><code>Github</code></li>
<li><code>Wikipedia</code></li>
<li>书籍</li>
<li><code>ArXiv</code></li>
<li><code>StackExchange</code></li>
</ol>
</li>
</ol>
<h3 id="33-常用-fine-tuning-语料库"><a class="markdownIt-Anchor" href="#33-常用-fine-tuning-语料库"></a> 3.3 常用 <code>Fine-tuning</code> 语料库</h3>
<h4 id="1-instrction-tuning-指令微调-常用数据集"><a class="markdownIt-Anchor" href="#1-instrction-tuning-指令微调-常用数据集"></a> 1. <strong><code>instrction tuning</code> (指令微调)</strong> 常用数据集</h4>
<p><code>instrction tuning</code> (指令微调) 过程可将预训练好的多任务模型在 <code>Zero-shot</code> 任务上表现更好。</p>
<ol>
<li><strong><code>NLP task dataset</code></strong>
<ol>
<li><code>P3 (Public Pool of Prompts)</code>
<ol>
<li>一个涵盖各种自然语言处理任务的 <code>Prompted</code> 英文数据集集合，<code>Prompt</code> 是输入模板和目标模板的组合。</li>
<li>模板是将数据示例映射到自然语言输入和目标序列的函数。例如，在自然语言推理（<code>NLI</code>）数据集的情况下，数据示例将包括 <strong><code>Premise</code>（前提）、<code>Hypothesis</code>（假设）和 <code>Label</code>（标签）</strong> 字段。</li>
<li>输入模板可以定义为：“如果 <code>&#123;Premise&#125;</code> 为真，则 <code>&#123;Hypothesis&#125;</code> 也为真吗？”，而目标模板可以定义为：选择的选项为 <code>Choices[label]</code>。这里的 <code>Choices</code> 是特定于 <code>Prompt</code> 的元数据，包含对应于标签为包含（0）、中性（1）或矛盾（2）的选项 <code>yes</code>、<code>maybe</code>、<code>no</code>。</li>
</ol>
</li>
<li><code>FLAN</code> 使用的 <code>instrction tunning</code> 数据集
<ol>
<li><code>FLAN</code> 实际上是 <code>google</code> 的 《Finetuned Language Models Are Zero-Shot Learners》提出的模型，该模型用了大量的自然语言理解（<code>NLU</code>）和自然语言生成（<code>NLG</code>）任务数据集做指令微调。<br />
<img src="https://s2.loli.net/2024/05/08/gGScu4r6nFqaseZ.png" alt="LLM_survey_6.png" /></li>
<li>指令微调后的模型在 <code>Zero-shot</code> 任务上表现更好。</li>
</ol>
</li>
</ol>
</li>
<li><strong><code>Daily chat dataset</code></strong>
<ol>
<li><strong><code>ShareGPT</code></strong>：由用户资源上传的和 <code>ChatGPT</code> 或 <code>GPT4</code> 的对话，总量大约 <code>90,000</code> 组对话。</li>
<li><strong><code>OpenAssistant</code></strong>：多语言的人类和 <code>AI</code> 助手的对话，包含 <code>35</code> 种语言，<code>66,497</code> 组对话和 <code>461,292</code> 个人类标注。</li>
<li><strong><code>Dolly</code></strong>：由 <code>Databricks</code> 公司制作的 <code>15,000</code> 条英文人类对话，包含 <code>7</code> 大子类：
<ol>
<li>头脑风暴</li>
<li>内容生成</li>
<li>信息提取</li>
<li>摘要</li>
<li>分类</li>
<li>开卷考试的质量保证 (<code>closed-book quality assurance</code>)</li>
<li>闭卷考试的质量保证 (<code>open-book quality assurance</code>)</li>
</ol>
</li>
</ol>
</li>
<li><strong>合成数据</strong>
<ol>
<li><code>Self-Instruct-52K</code>：一个由 <code>Self-Instruct</code> 框架生成的指令遵循数据集，共 <code>82,000</code> 个实例包含约 <code>52,000</code> 条指令。</li>
<li><code>Alpaca</code>：一个用于训练和评估指令遵循型语言模型的集合，它包含了 <code>52,000</code> 条独特的指令和相应的输出，这些数据是基于 <code>OpenAI</code> 的 <code>text-davinci-003</code> 模型自动生成的。</li>
<li><code>Baize</code>：是一个开源的多轮对话数据集，它通过让 <code>ChatGPT</code> 自我对话生成，旨在为训练和评估对话模型提供高质量的语料资源。对话全部为英文，共包含 <code>111.5K</code> 个实例。</li>
</ol>
</li>
</ol>
<h4 id="2-alignment对齐-常用数据集"><a class="markdownIt-Anchor" href="#2-alignment对齐-常用数据集"></a> 2. <strong><code>Alignment</code>（对齐）</strong> 常用数据集</h4>
<p><code>Alignment</code>（对齐）过程的目的是让 <code>LLM</code> 对齐人类的价值观和偏好。<code>Alignment</code> 数据集需要是高质量的、有帮助的、诚实的、无害的。</p>
<ol>
<li><code>HH-RLHF</code> ：是由 Anthropic 公司收集的，用于训练和评估强化学习中的偏好（或奖励）模型的数据集，包含约 <code>169K</code> 实例。这个数据集包含两部分：
<ol>
<li>有益和无害性的人类偏好数据</li>
<li>红队对抗数据</li>
</ol>
</li>
<li><code>SHP</code>：包含 <code>385,000</code> 条人类偏好的数据集，这些偏好是对 <code>18</code> 个不同主题领域中问题/指令的回答进行的。</li>
<li><code>PKU-SafeRLHF</code>：由北京大学团队开发的，用于支持和推动安全强化学习（<code>RLHF</code>）技术的研究和发展的数据集。这个数据集是目前为止最大的多轮 <code>RLHF</code> 数据集之一，规模达到 <code>100</code> 万条，包含了一系列安全偏好的标注，这些标注覆盖了侮辱、歧视、犯罪、心理伤害、悲观情绪、色情、隐私等多种维度，用于对 <code>RLHF</code> 技术进行细粒度的约束价值对齐。</li>
<li><code>Stack Exchange Preferences</code>：是一个从 <code>Stack Overflow</code> 数据转储中提取的问答对数据集，它被设计用于偏好学习（<code>preference learning</code>）。这个数据集包含了大量的问题和答案，其中答案基于得票数进行了评分。数据集的大小超过了 <code>20GB</code>，并且包含了数百万条问题和答案对。</li>
<li><code>Sandbox Alignment Data</code> 是一个由大型语言模型（<code>LLM</code>）生成的对齐数据集，包含约 <code>169K</code> 个实例，它包含了来自模拟社交互动环境 <code>SANDBOX</code> 的反馈。在 <code>SANDBOX</code> 中，模型模拟了人类社会中的交互，通过这种方式生成的对话数据被用来训练和评估语言模型，使其更好地符合人类的价值观和社交规范。</li>
</ol>
<h3 id="34-常用代码库资源"><a class="markdownIt-Anchor" href="#34-常用代码库资源"></a> 3.4 常用代码库资源</h3>
<h4 id="1-transformers"><a class="markdownIt-Anchor" href="#1-transformers"></a> 1. <code>Transformers</code></h4>
<ul>
<li>一个使用 <code>Transformer</code> 架构构建模型的开源 <code>Python</code> 库，由 <code>Hugging Face</code> 开发和维护。它具有简单和用户友好的 <code>API</code>，方便使用和定制各种预训练模型。</li>
</ul>
<h4 id="2-deepspeed"><a class="markdownIt-Anchor" href="#2-deepspeed"></a> 2. <code>DeepSpeed</code></h4>
<ul>
<li>由 <code>Microsoft</code> 开发的深度学习优化库（与 <code>PyTorch</code> 兼容），已用于训练多个 <code>LLM</code>，例如 <code>MTNLG</code> 和 <code>BLOOM</code>。它提供了各种分布式训练优化技术的支持，例如内存优化（<code>ZeRO</code> 技术、梯度检查点）和管道并行。</li>
</ul>
<h4 id="3-megatron-lm"><a class="markdownIt-Anchor" href="#3-megatron-lm"></a> 3. <code>Megatron-LM</code></h4>
<ul>
<li>由 <code>NVIDIA</code> 开发的深度学习库，用于训练 <code>LLM</code>。它提供了丰富的分布式训练优化技术，包括模型和数据并行、混合精度训练和 <code>FlashAttention</code>。这些优化技术可以大大提高训练效率和速度，并实现 <code>GPU</code> 间的高效分布式训练。</li>
</ul>
<h4 id="4-jax"><a class="markdownIt-Anchor" href="#4-jax"></a> 4. <code>JAX</code></h4>
<ul>
<li>由 <code>Google</code> 开发的用于高性能机器学习算法的 <code>Python</code> 库，允许用户在带有硬件加速（例如 <code>GPU</code> 或 <code>TPU</code>）的情况下进行数组的高效运算。它可以在各种设备上进行高效计算，还支持自动微分和即时编译等特色功能。</li>
</ul>
<h4 id="5-colossal-ai"><a class="markdownIt-Anchor" href="#5-colossal-ai"></a> 5. <code>Colossal-AI</code></h4>
<ul>
<li>由 <code>HPC-AI Tech</code> 开发的用于训练大规模人工智能模型的深度学习库。它基于 <code>PyTorch</code> 实现，并支持丰富的并行训练策略。</li>
</ul>
<h4 id="6-bmtrain"><a class="markdownIt-Anchor" href="#6-bmtrain"></a> 6. <code>BMTrain</code></h4>
<ul>
<li>由 <code>OpenBMB</code> 开发的用于以分布式方式训练大规模参数模型的高效库，强调代码简洁、低资源占用和高可用性。</li>
</ul>
<h4 id="7-fastmoe"><a class="markdownIt-Anchor" href="#7-fastmoe"></a> 7. <code>FastMoE</code></h4>
<ul>
<li>一种专门用于 <code>MoE</code>（即混合专家）模型的训练库。</li>
</ul>
<h4 id="8-vllm"><a class="markdownIt-Anchor" href="#8-vllm"></a> 8. <code>vLLM</code></h4>
<ul>
<li>一个快速、内存高效、易用的 <code>LLM</code> 代码库，用于 <code>LLM</code> 的推理和服务。</li>
</ul>
<h4 id="9-deepspeed-mii"><a class="markdownIt-Anchor" href="#9-deepspeed-mii"></a> 9. <code>DeepSpeed-MII</code></h4>
<ul>
<li><code>DeepSpeed Model Implementations for Inference</code>，一个比 <code>vLLM</code> 更快的模型推理服务框架。</li>
</ul>
<h4 id="10-deepspeed-chat"><a class="markdownIt-Anchor" href="#10-deepspeed-chat"></a> 10. <code>DeepSpeed-Chat</code></h4>
<ul>
<li>基于 <code>DeepSpeed</code> 的一键式训练 <code>RLHF</code>，提速 <code>15</code> 倍。</li>
</ul>
<h2 id="4-预训练"><a class="markdownIt-Anchor" href="#4-预训练"></a> 4. 预训练</h2>
<p>预训练为 <code>LLM</code> 的语言理解和生成能力奠定了基础。</p>
<h3 id="41-数据收集和处理"><a class="markdownIt-Anchor" href="#41-数据收集和处理"></a> 4.1 数据收集和处理</h3>
<h4 id="1-数据来源"><a class="markdownIt-Anchor" href="#1-数据来源"></a> 1. <strong>数据来源</strong></h4>
<ol>
<li>通用文本数据
<ol>
<li><code>Webpages</code></li>
<li><code>Conversation text</code></li>
<li><code>Books</code></li>
</ol>
</li>
<li>专用文本数据
<ol>
<li>多语言文本</li>
<li>科学文本</li>
<li>代码</li>
</ol>
</li>
</ol>
<h4 id="2-数据处理"><a class="markdownIt-Anchor" href="#2-数据处理"></a> 2. <strong>数据处理</strong></h4>
<p>一个好用的数据处理代码库是 <code>Data-Juicer</code></p>
<ol>
<li>质量过滤<br />
为删除语料库中的低质量数据，通过有两种方式：<strong>基于分类器的方法</strong> 和 <strong>基于启发式的方法</strong>
<ol>
<li>基于分类器的方法
<ol>
<li>通常采用高质量文本（例如 <code>wikipedia</code>）作为正样本，候选文本作为负样本，训练二分类器，用于给出一个文本的质量分数。</li>
<li>预计分类器的质量过滤方法可能会 <strong>无意识的删除口语化、方言、社会语言的文本</strong>。</li>
</ol>
</li>
<li>基于启发式的方法<br />
启发式方法通常采用一系列预设的规则来过滤低质量文本，具体的方法有：
<ol>
<li>基于语言的过滤：例如删除小语种文本等。</li>
<li>基于度量的过滤：例如基于困惑度（<code>perplexity</code>）来检测和删除不自然的文本。</li>
<li>基于统计的过滤：例如根据标点符号的分布、符号和单词的比例等删除。</li>
<li>基于关键词的过滤：例如删除 <code>HTML</code> 标签、超链接、模板等。</li>
</ol>
</li>
</ol>
</li>
<li>数据去重
<ol>
<li>现有研究表明，重复数据会降低语料库的多样性，可能导致训练过程不稳定，从而影响模型性能。</li>
<li>三种粒度的数据去重：
<ol>
<li>句子级</li>
<li>文档级</li>
<li>数据集级</li>
</ol>
</li>
</ol>
</li>
<li>隐私去除
<ol>
<li>预训练数据大多来自网络，可能包含敏感隐私信息，存在隐私泄露风险。因此需要从数据集中删除 <code>personally identifiable information</code>（可识别个人信息，<code>PII</code>）。</li>
<li>一种有效的删除方法是通过规则（例如：关键字识别）来检测和删除可识别各人信息（例如：姓名、电话、地址等）。</li>
<li>现有研究表明，<code>LLM</code> 在隐私攻击下的脆弱性可能归因于预训练数据中存在重复的可识别个人信息。</li>
</ol>
</li>
<li>分词（<code>Tokenization</code>）<br />
分词也是数据预处理的关键步骤，它的目的是将原始文本分割成词序列，随后用作 <code>LLM</code> 的输入。常用的分词方式有以下三种：
<ol>
<li><code>BPE (Byte-Pair Encoding tokenization)</code> 字节对编码，计算过程如下：
<ol>
<li>首先将文本拆分成字母和分隔符，并统计每个字母或分隔符出现的频率。</li>
<li>计算任意两个字母/分隔符合并后出现的频率，找到最高频的字母/分隔符对合并，重新统计词频并更新词表。</li>
<li>重复第二步，直到词表大小满足要求。</li>
<li>特点：
<ol>
<li>简单高效</li>
<li>贪心算法，每一步都选择频数最大的相邻字符进行合并，这种做法可能不一定是全局最优、频数也不一定是最好的合并指标。</li>
<li>适合拉丁字母组成的语言，不适合汉字、日韩文字等。</li>
</ol>
</li>
</ol>
</li>
<li><code>WordPiece tokenization</code>，计算过程如下：
<ol>
<li>首先将文本拆分成字母和分隔符。</li>
<li>不同于 <code>BPE</code> 选择合并后词频最大的词作为 <code>Subword</code>，<code>WordPiece</code> 基于语言模型似然概率的最大值生成新的 <code>Subword</code>，具体的<strong>基于语言模型似然概率</strong>的定义如下：
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S=(t_1,t_2,...,t_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，句子 <code>S</code> 由 <code>n</code> 个子词组成。</li>
<li>则句子 <code>S</code> 的似然概率定义为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log P(S)=\sum_{i=1}^n logP(t_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.104002em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
<li>其中 <code>P()</code> 表示一个训练好的语言概率模型。</li>
</ol>
</li>
<li>找到合并后可以使得句子的似然概率最大的子词合并，更新词表，重复这个过程。</li>
</ol>
</li>
<li><code>Unigram LM</code>，计算过程如下：
<ol>
<li><code>Unigram</code> 和 <code>BPE</code> 以及 <code>WordPiece</code> 有个很大的不同是：
<ol>
<li><code>BPE</code> 和 <code>WordPiece</code> 都是初始化小词表逐步变大直到满足词表大小要求。</li>
<li><code>Unigram LM</code> 是初始化大词表逐步删词直到满足词表大小要求。</li>
</ol>
</li>
<li><code>Unigram LM</code> 删词的根据和 <code>WordPiece</code> 一致，都是<strong>基于语言模型似然概率</strong></li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="3-预训练数据对大语言模型的影响"><a class="markdownIt-Anchor" href="#3-预训练数据对大语言模型的影响"></a> 3. <strong>预训练数据对大语言模型的影响</strong></h4>
<p>与小规模的 <code>PLM</code> 不同，由于对计算资源的巨大需求，通常不可能对 <code>LLM</code> 进行多次预训练迭代。因此，在训练 <code>LLM</code> 之前构建一个准备充分的预训练语料库尤为重要。<br />
目前，不同的 <code>LLM</code> 模型预训练采用不同的预训练策略，下图是一些模型的预训练数据分布图：<br />
<img src="https://s2.loli.net/2024/05/16/pbe61vwcSjx9COH.png" alt="LLM_survey_7.png" /><br />
不过，存在几个有效的混合策略：</p>
<ol>
<li>增加数据的多样性（<code>diversity</code>）<br />
预训练阶段，数据的多样性比数据的质量更重要，有实验表明，删除语料库中的多样性数据（例如：<code>Webpages</code>）比删除语料库中的高质量数据（例如：学术语料库）对模型的影响更大。</li>
<li>可以通过数据混合实验优化数据混合策略
<ol>
<li>在大模型上做数据混合实验是非常昂贵且耗时的，通过会在小模型上测试不同的数据混合策略的优劣，再将小模型测试得到的结果用在大模型上。</li>
<li>但小模型得到的数据混合策略结论在大模型上可能是不成立的，越大的小模型得出来结论越可信。</li>
</ol>
</li>
<li>先用 <code>general</code> 数据训练模型通用能力，再用 <code>skill-spcific</code> 数据训练模型专业能力
<ol>
<li>不同的数据训练的模型有不同的能力，比如：
<ol>
<li>用代码训练的模型对数学和编程更擅长。</li>
<li>用书籍训练的模型更擅长从文本中捕捉长期依赖。</li>
</ol>
</li>
<li>先训练 <code>basic skill</code> 再训练 <code>target skill</code> 比直接训练 <code>target skill</code> 模型表现更好</li>
</ol>
</li>
</ol>
<h3 id="42-架构设计"><a class="markdownIt-Anchor" href="#42-架构设计"></a> 4.2 架构设计</h3>
<p><img src="https://s2.loli.net/2024/05/21/QviOjpeFBkPyEg8.png" alt="LLM_survey_8.png" /></p>
<h4 id="1-典型结构"><a class="markdownIt-Anchor" href="#1-典型结构"></a> 1. <strong>典型结构</strong></h4>
<ol>
<li><strong><code>Casual Decoder</code> 结构</strong>
<ol>
<li>参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2022/07/04/GPT-Improving-Language-Understanding-by-Generative-Pre-Training/"><code>GPT</code> 博客</a> 中的 <code>Hello, world!</code> 续写例子</li>
</ol>
</li>
<li><strong><code>Encoder-Decoder</code> 结构</strong>
<ol>
<li>参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/06/03/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/"><code>T5</code> 博客</a> 中的 <code>Hello, world!</code> 英语翻译法语的例子</li>
</ol>
</li>
<li><strong><code>Prefix Decoder</code> 结构</strong>
<ol>
<li>是对 <code>Encoder-Decoder</code> 和 <code>Casual Decoder</code> 的折中，在 <code>prefix</code> 前缀文本中使用双向注意力，在生成后续序列时使用单向注意力</li>
<li>用到 <code>Prefix Decoder</code> 的模型有：
<ul>
<li><code>U-PaLM</code></li>
<li><code>GLM</code> 等</li>
</ul>
</li>
</ol>
</li>
<li><strong><code>Mixture-of-Experts</code> （混合专家）结构</strong>
<ol>
<li>优势：是一种灵活的模型参数扩展方法；性能提高明显；</li>
<li>劣势：由于路由操作复杂等原因，训练容易不稳定；</li>
<li>传言 <code>GPT-4</code> 使用了 <code>MOE</code> 结构</li>
</ol>
</li>
<li><strong><code>Emergent Architectures</code>（新兴结构）</strong><br />
<img src="https://s2.loli.net/2024/06/28/yGowfrAOPvWI7u6.png" alt="LLM_survey_9.png" />
<ol>
<li>现有架构存在的问题：
<ol>
<li><code>Transformer</code> 架构一个很大的问题是推理时复杂度较高，每一个 <code>token</code> 需要计算和之前的每一个 <code>token</code> 计算相关性</li>
<li>相比之下，<code>RNN</code> 架构推理的复杂度就很低，每一个 <code>token</code> 只需要和上一个 <code>token</code> 以及一个不断更新的 <code>hidden state</code> 计算相关性（本质是 <code>hidden state</code> 在某种程度上存储了之前所有 <code>token</code> 的信息）</li>
<li>但 <code>RNN</code> 数据依赖问题非常强，导致训练难以并行化，且通常效果较差</li>
</ol>
</li>
<li>一些新兴架构包括：
<ol>
<li><code>H3/S4</code></li>
<li><code>RWKV</code></li>
<li><code>Hyena</code></li>
<li><code>Mamba</code></li>
<li><code>RetNet</code></li>
</ol>
</li>
<li>这些新兴架构几乎无一例外的想做同一件事：<strong>像 <code>Transformer</code> 一样可以并行训练 + 像 <code>RNN</code> 一样高效推理，且效果可以和 <code>Transformer</code> 媲美</strong></li>
<li>状态空间模型 <code>SSM</code> 论文 <code>HiPPO</code> 可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/06/17/HiPPO-Recurrent-Memory-with-Optimal-Polynomial-Projections/">这篇论文</a></li>
</ol>
</li>
</ol>
<h4 id="2-配置细节"><a class="markdownIt-Anchor" href="#2-配置细节"></a> 2. <strong>配置细节</strong></h4>
<p><img src="https://s2.loli.net/2024/07/02/pWcYQraIxdyeZDF.png" alt="LLM_survey_11.png" /></p>
<ol>
<li><strong>归一化方法</strong>
<ol>
<li><strong>LayerNorm</strong>
<ul>
<li><code>LayerNorm</code> 和 <code>BatchNorm</code> 只有两个差别，为了简化，假设 <code>Channel = 1</code> 即 <code>feature shape = N, 1, H, W</code>：
<ul>
<li>训练时差别：
<ul>
<li><code>BatchNorm</code> 统计整个 <code>feature map</code> 的均值和标准差（<code>scalar</code>），然后在整个 <code>feature map</code> 做标准正态化；同时记录（滑动平均算法）到 <code>running_mean / running_var</code> 参数中</li>
<li><code>LayerNorm</code> 统计每一个样本的均值和标准差（均值和标准差都是长度为 <code>N</code> 的 <code>vector</code>），然后每个样本逐个做标准正态化；均值标准差用完即扔，无需记录</li>
</ul>
</li>
<li>推理时差别：
<ul>
<li><code>BatchNorm</code> 推理时不需要计算 <code>feature</code> 的均值和标准差，而是使用训练统计得到的 <code>running_mean / running_var</code>，因此是个静态行为，可以被前面的 <code>Conv2d</code> 运算吸收</li>
<li><code>LayerNorm</code> 推理阶段和训练阶段运算方式基本一致</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>RMSNorm</strong>
<ul>
<li>假设输入 <code>shape</code> 为 <code>N, C, L</code> 分别表示 <code>batch / feature_dim / seq_length</code></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>c</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>C</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><msubsup><mi>x</mi><mrow><mi>n</mi><mi>l</mi></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">RMS(x_c)=\sqrt{\frac{1}{C}\sum_{n=1}^N\sum_{l=1}^Lx_{nl}^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.84em;vertical-align:-0.5368845em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3031155em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959079999999998em;"><span style="top:-2.398692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30130799999999996em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.2631154999999996em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.8800000000000001em;"><svg width='400em' height='1.8800000000000001em' viewBox='0 0 400000 1944' preserveAspectRatio='xMinYMin slice'><path d='M983 90
l0 -0
c4,-6.7,10,-10,18,-10 H400000v40
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5368845em;"><span></span></span></span></span></span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>c</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msub><mi>x</mi><mi>c</mi></msub><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>∗</mo><mi>k</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">RMSNorm(x_c)=\frac{x_c}{RMS(x_c)}*k+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2314919999999998em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7114919999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></li>
<li>其中，<code>k / b</code> 都是可学习的 <code>per channel</code> 向量</li>
<li>和 <code>LayerNorm</code> 相比 <code>RMSNorm</code> 有如下优势：
<ol>
<li>更小的计算开销：<code>RMSNorm</code> 不需要计算输入数据的均值，因此减少了计算量，使得模型训练更加高效。</li>
<li>训练速度更快：由于减少了计算量，<code>RMSNorm</code> 在训练过程中的速度通常比 <code>LayerNorm</code> 更快。</li>
<li>性能相当或更好：尽管 <code>RMSNorm</code> 的计算更简单，但它在保持与 <code>LayerNorm</code> 相当性能的同时，甚至可能在某些情况下提供更好的性能。</li>
<li>保留重要的不变性：<code>RMSNorm</code> 保留了输入数据的均方根比例不变性，这有助于模型在面对不同尺度的输入数据时保持一致的性能。</li>
<li>隐式学习率适应：<code>RMSNorm</code> 通过归一化输入数据的 <code>RMS</code>，为模型提供了一种隐式的学习率适应能力，有助于模型在训练过程中更好地调整参数。</li>
</ol>
</li>
</ul>
</li>
<li><strong>DeepNorm</strong>
<ul>
<li><code>DeepNorm</code> 可以看做是一种增强型 <code>LayerNorm</code>，对 <code>LayerNorm</code> 改动较小，但效果惊人，<code>DeepNorm</code> 用在 <code>Post-LN</code> 架构上替代传统 <code>LayerNorm</code> 运算，可稳定训练，可训练深度超过 1,000 层的 <code>Transformer</code></li>
<li><code>DeepNorm</code> 对 <code>LayerNorm</code> 的改动如下：<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>→</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>∗</mo><mi>α</mi><mo>+</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">LayerNorm (x + f(x)) \rightarrow LayerNorm(x*\alpha + f(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<img src="https://s2.loli.net/2024/07/02/kc8Q19t3amuS42z.png" alt="LLM_survey_10.png" /></li>
</ul>
</li>
</ol>
</li>
<li><strong>归一化位置</strong>
<ol>
<li><strong>Pre-LN</strong>
<ul>
<li><code>Pre-LN</code> 是一种 <code>Transformer</code> 架构，计算公式是:<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>H</mi><mi>A</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=x+MHA(LayerNorm(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">A</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=x+FFN(LayerNorm(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = LayerNorm(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></li>
</ul>
</li>
<li><strong>Post-LN</strong>
<ul>
<li><code>Post-LN</code> 是另外一种 <code>Transformer</code> 架构，计算公式是：<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>H</mi><mi>A</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=LayerNorm(x+MHA(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">A</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=LayerNorm(x+FFN(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></li>
</ul>
</li>
<li><strong>Sandwich-LN</strong>
<ul>
<li><code>Sandwich-LN</code> 是 <code>Pre-LN</code> 的改进，公式为：<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>M</mi><mi>H</mi><mi>A</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=x+LayerNorm(MHA(LayerNorm(x)))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">A</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=x+LayerNorm(FFN(LayerNorm(x)))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = LayerNorm(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></li>
</ul>
</li>
</ol>
</li>
<li><strong>激活函数</strong>
<ol>
<li><strong>GeLU</strong><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn><mi>x</mi><mo>∗</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>r</mi><mi>f</mi><mo stretchy="false">(</mo><mfrac><mi>x</mi><msqrt><mn>2</mn></msqrt></mfrac><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">GeLU(x)=0.5x*(1+erf(\frac{x}{\sqrt 2}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.2879999999999998em;vertical-align:-0.5379999999999999em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.5510085em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.912845em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;">2</span></span><span style="top:-2.872845em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.12715500000000002em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5379999999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>r</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>2</mn><msqrt><mi>π</mi></msqrt></mfrac><msubsup><mo>∫</mo><mn>0</mn><mi>x</mi></msubsup><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></msup><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">erf(x)=\frac{2}{\sqrt \pi}\int_0^xe^{-t^2}dt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.5249199999999998em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6258665em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8059050000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal mtight" style="margin-right:0.03588em;padding-left:0.833em;">π</span></span><span style="top:-2.765905em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.234095em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8592920000000001em;"><span style="top:-2.34418em;margin-left:-0.19445em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.2579000000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35582em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869199999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class="mord mathnormal">t</span></span></span></span>
<ul>
<li>其中 <code>erf</code> 表示高斯误差函数，值域是 <code>[-1, 1]</code></li>
<li><code>GeLU</code> 是 <code>LLM</code> 中使用最广泛的激活函数</li>
</ul>
</li>
<li><strong>Swish</strong><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Swish(x) = x*sigmoid(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></li>
<li><strong>SwiGLU</strong><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>w</mi><mi>i</mi><mi>G</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>S</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>∗</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">SwiGLU(x_1, x_2) = Swish(x_1) * x_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">i</span><span class="mord mathnormal">G</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><strong>GeGLU</strong><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>e</mi><mi>G</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>G</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>∗</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">GeGLU(x_1, x_2)=GeLU(x_1) * x_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">e</span><span class="mord mathnormal">G</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
</ol>
</li>
<li><strong>位置编码</strong>
<ol>
<li><strong>绝对位置编码</strong>
<ol>
<li>用在传统 <code>Transformer</code> 上，有两种：
<ol>
<li>位置编码（<code>position encoding</code>）：使用正弦得到位置编码<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mfrac><msub><mi>p</mi><mi>j</mi></msub><mrow><mn>1000</mn><msup><mn>0</mn><mfrac><mrow><mn>2</mn><mi>i</mi></mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mfrac></msup></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(j, 2i)=sin(\frac{p_j}{10000^{\frac{2i}{d_{model}}}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.658217em;vertical-align:-0.8495050000000002em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.808712em;"><span style="top:-2.19em;"><span class="pstrut" style="height:3.039505em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight"><span class="mord mtight">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.485007142857143em;"><span style="top:-3.7374928571428576em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size1 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0465200000000001em;"><span style="top:-2.468em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.69444em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34963999999999995em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.387em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.88164em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size1 size6"></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.269505em;"><span class="pstrut" style="height:3.039505em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.546825em;"><span class="pstrut" style="height:3.039505em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8495050000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mfrac><msub><mi>p</mi><mi>j</mi></msub><mrow><mn>1000</mn><msup><mn>0</mn><mfrac><mrow><mn>2</mn><mi>i</mi></mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mfrac></msup></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(j, 2i+1)=cos(\frac{p_j}{10000^{\frac{2i}{d_{model}}}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.658217em;vertical-align:-0.8495050000000002em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.808712em;"><span style="top:-2.19em;"><span class="pstrut" style="height:3.039505em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight"><span class="mord mtight">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.485007142857143em;"><span style="top:-3.7374928571428576em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size1 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0465200000000001em;"><span style="top:-2.468em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.69444em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34963999999999995em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.387em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.88164em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size1 size6"></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.269505em;"><span class="pstrut" style="height:3.039505em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.546825em;"><span class="pstrut" style="height:3.039505em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8495050000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>
<ul>
<li>其中
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE(j,2i)/PE(j,2i+1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> 分别表示位置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 对应的位置编码向量的第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2i/2i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mord">/</span><span class="mord">2</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 位置的值</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> 个位置</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示向量维度</li>
</ul>
</li>
</ul>
</li>
<li>可学习位置编码（<code>position embedding</code>）：使用可学习的 <code>embedding</code> 编码：<code>position_embedding = nn.Embedding(max_seq_length, d_model)</code></li>
</ol>
</li>
<li>绝对位置编码 <code>position embedding</code> 通常会 <code>element-wise</code> 加到输出 <code>token embedding</code> 作为 <code>Transformer</code> 输入，即：<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i=x_i + p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示 <code>token embedding</code>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示 <code>position embedding</code></li>
</ol>
</li>
<li><strong>相对位置编码</strong>
<ol>
<li>从计算方法来说，相对位置编码需要：
<ol>
<li>先构造相对位置矩阵：<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>2</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}0&amp;-1&amp;-2\\1&amp;0&amp;-1\\2&amp;1&amp;0\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.6010299999999997em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.0510099999999998em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.8099900000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.05101em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.0510099999999998em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.8099900000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.05101em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span></li>
<li><strong>可学习</strong> 的相对位置偏置表：
<ol>
<li><code>shape = [2 * n - 1, 2 * n - 1, num_head] = [5, 5, num_head]</code></li>
<li>其中，坐标 <code>[i, j, k]</code> 表示第 <code>k</code> 的 <code>self-attention</code> 头上 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">K_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 的相对位置</li>
</ol>
</li>
</ol>
</li>
<li>从使用方法来说：
<ol>
<li>相对位置编码不再需要显式的将 <code>position embedding</code> 加上 <code>token embedding</code> 作为 <code>Transformer</code> 输入</li>
<li>而是将相对位置编码加到 <code>self-attention</code> 上，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mi>q</mi></msub><msub><mi>x</mi><mi>i</mi></msub><msubsup><mi>x</mi><mi>j</mi><mi>T</mi></msubsup><msubsup><mi>W</mi><mi>k</mi><mi>T</mi></msubsup><mo>+</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_{ij}=W_qx_ix_j^TW_k^T+r_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.236103em;vertical-align:-0.394772em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4168920000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">K_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 的 <code>self-attention score</code></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">r_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">K_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 的相对位置</li>
</ol>
</li>
<li>因此也被称为 <code>RelativePositionBias</code> （相当于 <code>self-attention</code> 的 <code>bias</code>）</li>
</ol>
</li>
<li>可以参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/"><code>ALiBi</code> 博客</a></li>
</ol>
</li>
<li><strong>旋转位置编码 <code>RoPE</code></strong>
<ol>
<li>是一种不可学习的 <code>position encoding</code> 位置编码方式，编码本身是绝对位置编码，但通过 <code>self-attention</code> 之后，<code>key</code> 和 <code>query</code> 会产生相对位置关系</li>
<li>计算过程：
<ol>
<li>首先以固定间隔角度计算旋转矩阵，矩阵中每个元素是复数形式，<code>shape = [max_seq_len, d_model]</code></li>
<li>对计算 <code>self-attention</code> 之前的 <code>key / query</code> 分别进行 <strong>旋转</strong>（本质是和旋转矩阵做复数乘法，然后再转换到实数空间），此步骤不改变 <code>key / query</code> 的 <code>shape</code></li>
<li>然后做标准的 <code>self-attention</code></li>
</ol>
</li>
</ol>
</li>
<li>总结
<ol>
<li><strong>绝对位置编码</strong>：<code>position embedding</code> 和 <code>token embedding</code> 的 <code>shape</code> 相同，作用在 <code>query / key / value</code> 生成之前</li>
<li><strong>旋转位置编码</strong>：<code>position embedding</code> 和 <code>token embedding</code> 的 <code>shape</code> 相同，作用在 <code>self-attention</code> 之前，只作用在 <code>key / query</code> 上，不对 <code>value</code> 生效</li>
<li><strong>相对位置编码</strong>：<code>position embedding</code> 和 <code>token embedding</code> 的 <code>shape</code> 不同，和 <code>attention map</code> 的 <code>shape</code> 相同，作用在 <code>self-attention</code> 生成的注意力矩阵上</li>
</ol>
</li>
</ol>
</li>
<li><strong>Attention 方式</strong>
<ol>
<li><strong>full attention</strong>
<ol>
<li>就是标准的 <code>self-attention</code>，即<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt {d_k}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.627473em;vertical-align:-0.538em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.089473em;"><span style="top:-2.5864385em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622307142857143em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222307142857144em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17776928571428574em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span><br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mi mathvariant="normal">/</mi><mi>K</mi><mi mathvariant="normal">/</mi><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q/K/V\in\mathbb{R}^{n\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></li>
<li>在 <code>LLM</code> 语境下通常 <code>n &gt;&gt; d</code>，<code>softmax</code> 是非线性运算，所以需要先计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>∗</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">QK^T\in\mathbb{R}^{n*n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68889em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>，这决定了 <code>full attention</code> 的计算复杂度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，因此 <code>full attention</code> 复杂度也被称为 <code>softmax</code> 计算复杂度</li>
<li>假如没有 <code>softmax</code>，那么 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi></mrow><annotation encoding="application/x-tex">QK^TV</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 可以通过结合律先计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K^TV\in\mathbb{R}^{d\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.880431em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>，然后再计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">QK^TV\in\mathbb{R}^{n\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>，计算复杂度退化为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>，变成了线性复杂度</li>
</ol>
</li>
<li><strong>sparse attention</strong>
<ol>
<li>在标准 <code>self-attention</code> 的基础上，提前生成一个固定的二值的 <code>sparse attention mask</code>，将 <code>self-attention</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>O</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal(O)(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的计算复杂度变成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>O</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>n</mi><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal(O)(n\sqrt n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.05028em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8002800000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="padding-left:0.833em;">n</span></span><span style="top:-2.76028em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.23972em;"><span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
<li>固定的二值的 <code>sparse attention mask</code> 可以有多种形式，比如：</li>
</ol>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1, 0, 0, 0, 0, 0, 0, 0, 0, 0    |   1, 0, 0, 0, 0, 0, 0, 0, 0, 0</span><br><span class="line">0, 1, 0, 0, 0, 0, 0, 0, 0, 0    |   1, 1, 0, 0, 0, 0, 0, 0, 0, 0</span><br><span class="line">0, 0, 1, 0, 0, 0, 0, 0, 0, 0    |   1, 1, 1, 0, 0, 0, 0, 0, 0, 0</span><br><span class="line">1, 0, 0, 1, 0, 0, 0, 0, 0, 0    |   0, 1, 1, 1, 0, 0, 0, 0, 0, 0</span><br><span class="line">0, 1, 0, 0, 1, 0, 0, 0, 0, 0    |   0, 0, 1, 1, 1, 0, 0, 0, 0, 0</span><br><span class="line">0, 0, 1, 0, 0, 1, 0, 0, 0, 0    |   0, 0, 0, 1, 1, 1, 0, 0, 0, 0</span><br><span class="line">1, 0, 0, 1, 0, 0, 1, 0, 0, 0    |   0, 0, 0, 0, 1, 1, 1, 0, 0, 0</span><br><span class="line">0, 1, 0, 0, 1, 0, 0, 1, 0, 0    |   0, 0, 0, 0, 0, 1, 1, 1, 0, 0</span><br><span class="line">0, 0, 1, 0, 0, 1, 0, 0, 1, 0    |   0, 0, 0, 0, 0, 0, 1, 1, 1, 0</span><br><span class="line">1, 0, 0, 1, 0, 0, 1, 0, 0, 1    |   0, 0, 0, 0, 0, 0, 0, 1, 1, 1</span><br></pre></td></tr></table></figure>
</li>
<li><strong>Multi-query/grouped-query attention</strong>
<ol>
<li>简单来说，<code>MQA</code> 是将标准 <code>MHA</code> 中的 <code>key / value</code> 的 <code>Multi-Head Attention</code> 退化成 <code>Single-Head</code>，<code>query</code> 保持不变</li>
<li><code>GQA</code> 是将 <code>Multi-Head</code> 分组，每个组内是 <code>MQA</code></li>
<li>具体可以参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/07/11/MQA-Multi-Query-Attention/">Multi-query attention</a> 和 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/07/12/GQA-Grouped-Query-Attention/">Grouped-query attention</a> 两篇博客</li>
</ol>
</li>
<li><strong>Flash attention</strong>
<ol>
<li>简单来说 <code>Flash attention</code> 和 <code>Flash attention v2</code> 是标准 <code>self-attention</code> 的完全等价的高效实现，通过更合理的实现大幅降低了 <code>SRAM</code> 和 <code>HBW</code> 之间的 <code>IO</code> 量，从而大幅加速训练 / 推理速度</li>
<li>详细分析可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/07/17/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/">flash attention</a>  和 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/07/18/FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning/">flash attention v2</a> 两篇博客</li>
</ol>
</li>
<li><strong>Paged attention</strong>
<ol>
<li><code>Paged attention</code> 的目的是解决生成式模型 <code>kv cache</code> 占用太多显存导致显存利用率低的问题，<code>kv cache</code> 详细分析可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/07/30/KV-Cache-Transformer/">这篇博客</a></li>
<li><code>Paged attention</code> 通过类似分页内存管理的方法管理 <code>kv cache</code> 显存，随着序列变长，动态分配显存，显存物理空间不连续，最多只浪费一页，极大提高了显存利用率，使得可以运行更大的 <code>batch size</code></li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="3-预训练任务"><a class="markdownIt-Anchor" href="#3-预训练任务"></a> 3. <strong>预训练任务</strong></h4>
<ol>
<li><strong>Language modeling（语言建模）</strong>
<ol>
<li>语言建模任务是给定一个序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">X = \{x_1, x_2, ..., x_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>，使得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>X</mi><mo>&lt;</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{LM}=\sum_{i=1}^n log P(x_i|X&lt;i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.104002em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span> 此极大似然估计最大，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>&lt;</mo><mi>i</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">X&lt;i=\{x_1,...,x_i\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></li>
<li>语言建模预训练任务是 <code>LLM</code> 最常用的预训练任务，在很多下游任务上甚至无需 <code>fine-tuning</code></li>
</ol>
</li>
<li><strong>Denoising Autoencoding（去噪自编码）</strong>
<ol>
<li>去噪自编码是在一个句子中随机替换 <code>token</code> 为噪声，然后训练模型去恢复原始信息</li>
</ol>
</li>
<li><strong>Mixture-of-Denoisers（混合降噪器）</strong>
<ol>
<li><code>MoD</code> 是出自 <code>Google UL2</code> 大模型，此大模型使用了 <code>MoD</code> 作为预训练任务，是前面提到的两种预训练任务的组合<br />
<img src="https://s2.loli.net/2024/07/31/eJvRXQnEjrLfN3M.png" alt="UL2.png" /></li>
</ol>
</li>
</ol>
<h4 id="4-长文本建模"><a class="markdownIt-Anchor" href="#4-长文本建模"></a> 4. <strong>长文本建模</strong></h4>
<p>长文本建模是指模型在推理阶段实际输入的序列长度比训练阶段更长，即 <code>train short, test long</code>，主要有两种范式：</p>
<ol>
<li><strong>扩展位置嵌入（<code>Scaling Position Embeddings</code>）</strong>
<ol>
<li>直接模型微调：通过多阶段逐步增加上下文长度来适应长文本。</li>
<li>位置插值：通过调整位置索引来扩展上下文窗口，相比直接模型微调更有效。</li>
<li>位置截断：截断超出预定义窗口长度的位置索引，保留局部位置关系。</li>
<li>基础修改：通过调整基础值来改变波长，以适应更长的文本。例如修改 <code>RoPE</code> 的旋转角度间隔等。</li>
<li>基础截断：截断超出特定范围的基础值，避免在更大位置索引处出现分布外的旋转角度。</li>
</ol>
</li>
<li><strong>适应上下文窗口（Adapting Context Window）</strong>
<ol>
<li>并行上下文窗口：将输入文本分割成多个段，独立编码并修改注意力掩码以访问先前段的令牌。</li>
<li>Λ 形上下文窗口：基于 <code>LLMs</code> 倾向于更多关注起始和最近的令牌，采用选择性保留初始和最近令牌的注意力掩码。</li>
<li>外部记忆：存储过去的键（<code>keys</code>）在外部记忆，并使用 <code>k</code>-最近邻搜索方法检索最相关的令牌。</li>
</ol>
</li>
</ol>
<h4 id="5-解码策略"><a class="markdownIt-Anchor" href="#5-解码策略"></a> 5. <strong>解码策略</strong></h4>
<ol>
<li>背景
<ol>
<li>对于绝大多数的文本生成任务，<code>language modeling</code> 解码策略就足够了，即预测下一个概率最大的词，这种解码策略也被称为 <code>Greedy search</code></li>
<li><code>language modeling</code> 解码策略对于比如机器翻译、文本总结等任务是够用的，但是对于比如故事生成、对话等任务就有问题，会输出大量重复的、无意义的文本</li>
<li>因此，一种简单有效的策略是根据输出的词表中每一个词的概率随机选择一个词（即从 <code>argmax</code> 变成 <code>softmax</code>），这种策略被称为 <code>Random sampling</code></li>
</ol>
</li>
<li><strong><code>Greedy search</code> 的改进</strong>
<ol>
<li><code>Beam search</code>：集束搜索可以缓解模型预测陷入局部最优解，<code>beam size</code> 通常设置在 <code>3 ~ 6</code> 之间，太大会降低模型效果</li>
<li><code>Length penalty</code>：对于集束搜索的搜索空间进行长度惩罚，最终加权选择最优生成序列</li>
</ol>
</li>
<li><strong><code>Random sampling</code> 的改进</strong><br />
<code>Random sampling</code> 是根据预测概率随机选，因此每个词都有可能被选中（即使概率很低），这可能会导致输出完全不合逻辑的文本序列，因此需要改进
<ol>
<li><code>Temperature sampling</code>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><mi>x</mi><mo>&lt;</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>l</mi><mi>j</mi></msub><mi mathvariant="normal">/</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mo>∑</mo><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>l</mi><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi mathvariant="normal">/</mi><mi>t</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(x_j|x&lt;i)=\frac{exp(l_j/t)}{\sum_{j&#x27;}exp(l_{j&#x27;}/t)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.699547em;vertical-align:-0.667227em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.03232em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28539000000000003em;"><span style="top:-2.2853899999999996em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.6068285714285713em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8495600000000001em;"><span style="top:-2.84956em;margin-right:0.1em;"><span class="pstrut" style="height:2.55556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.46032428571428574em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.19516666666666668em;"></span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3447999999999998em;margin-left:-0.01968em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.6068285714285713em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8495600000000001em;"><span style="top:-2.84956em;margin-right:0.1em;"><span class="pstrut" style="height:2.55556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4009142857142858em;"><span></span></span></span></span></span></span><span class="mord mtight">/</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.50732em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.01968em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span><span class="mord mtight">/</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.667227em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，其中 <code>t</code> 代表温度</li>
<li>是对 <code>softmax</code> 得到概率分布的优化，加入了温度系数，可有效缓解随机采样导致的采样到不合理值的问题</li>
<li>当 <code>t = 1</code> 时，退化为 <code>random sampling</code></li>
<li>当 <code>t</code> 趋近于 <code>0</code> 时，退化为 <code>greedy search</code></li>
</ol>
</li>
<li><code>top-k sampling</code>
<ol>
<li>只保留可能性最高的 <code>k</code> 种取值</li>
<li>概率重新归一化后根据概率随机选</li>
</ol>
</li>
<li><code>top-p sampling</code>(<code>nucleus sampling</code>)
<ol>
<li>只保留累计概率不超过 <code>p</code> 的最小集；也就是按照预测概率降序，保留累计概率小于 <code>p</code> 的样本</li>
<li>概率重新归一化后根据概率随机选</li>
</ol>
</li>
<li><code>contrastive search</code>
<ol>
<li>参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/08/A-Contrastive-Framework-for-Neural-Text-Generation/">这篇博客</a></li>
<li>道理上讲得通，但模型推理阶段计算量翻了一倍</li>
</ol>
</li>
<li><code>typical sampling</code>
<ol>
<li>参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/09/Locally-Typical-Sampling/">这篇博客</a></li>
<li><code>top-p</code> 算法的升级版本</li>
</ol>
</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\eta-sampling</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span>
<ol>
<li>参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/09/Truncation-Sampling-as-Language-Model-Desmoothing/">这篇博客</a></li>
<li>算是一种动态自适应 <code>top-p</code> 算法，参考了 <code>typical sampling</code></li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="6-解码效率"><a class="markdownIt-Anchor" href="#6-解码效率"></a> 6. <strong>解码效率</strong></h4>
<p>作为主流的 <code>GPT-like LLM</code>，解码（推理）可以分为两个阶段：</p>
<ol>
<li><code>prefill stage</code>
<ol>
<li>预装填阶段，即将 <code>input / prompt</code> 输入到模型中计算 <code>hidden state</code> 的阶段</li>
<li>此阶段的计算访存比比较高，以 <code>A100 80GB GPU + LLaMA-13B</code> 为例，计算访存比为 <code>113.78</code>（硬件理论最高效计算访存比为 <code>156</code>）</li>
</ol>
</li>
<li><code>incremental decoding stage</code>
<ol>
<li>增量解码阶段，即累加上一轮输出持续解码直到 <code>&lt;EOS&gt;</code> 或 <code>max_length</code></li>
<li>此阶段计算访存比比较低，以 <code>A100 80GB GPU + LLaMA-13B</code> 为例，计算访存比为 <code>1.97</code><br />
因此，通过优化解码效率主要是针对 <code>incremental decoding stage</code>，对于增量解码阶段的优化主要有两个方向：</li>
</ol>
</li>
<li>减少数据 <code>IO</code>
<ol>
<li>这里的数据 <code>IO</code> 是指 <code>HBM</code> 和 <code>SRAM</code> 之间</li>
<li><code>Flash Attention</code> 通过分块计算 <code>Attention</code>，可大幅降低数据 <code>IO</code> 量</li>
<li><code>Multi-Query Attention</code> 和 <code>Grouped-Query Attention</code> 通过共享/分组共享 <code>KV</code> 来减小 <code>IO</code></li>
</ol>
</li>
<li>解码策略优化
<ol>
<li>投机解码：用小模型预测序列，让大模型评判是接受还是拒绝，可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/">这篇博客</a></li>
<li>树式投机解码 <code>SpecInfer</code>：在投机解码基础上升级，用树结构来存储候选 <code>token</code>，可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/15/SpecInfer-Accelerating-Large-Language-Model-Serving-with-Tree-based-Speculative-Inference-and-Verification/">这篇博客</a></li>
<li>跳跃解码 <code>SkipDecode</code>：在模型中跳过一些层的计算以减小计算量，和投机解码的无损加速（和原解码方式输出完全一致）不同，<code>SkipDecode</code> 是有损加速</li>
</ol>
</li>
</ol>
<h4 id="7-实际设置"><a class="markdownIt-Anchor" href="#7-实际设置"></a> 7. <strong>实际设置</strong></h4>
<ol>
<li><strong><code>T5</code></strong>
<ol>
<li>默认使用贪心解码算法</li>
<li>对于翻译和总结任务，使用 <code>beam size = 4</code> 的 <code>beam search</code>，长度惩罚系数 <code>0.6</code></li>
</ol>
</li>
<li><strong><code>GPT-3</code></strong>
<ol>
<li>对于所有任务，使用 <code>beam size = 4</code> 的 <code>beam search</code>，长度惩罚系数 <code>0.6</code></li>
</ol>
</li>
<li><strong><code>Alpaca</code></strong>
<ol>
<li>对于开放式生成任务，使用随机解码，<code>top-k (k = 50)</code> 和 <code>top-p (p = 0.9)</code> 以及 <code>0.7</code> 的温度</li>
</ol>
</li>
<li><strong><code>LLaMA</code></strong>
<ol>
<li>对于问答任务，使用贪心解码算法</li>
<li>对于代码生成任务，使用随机解码，温度设置为 <code>0.1</code> 和 <code>0.8</code>（分别为 <code>Pass@1</code> 和 <code>Pass@100</code>）</li>
</ol>
</li>
<li><strong><code>Open-AI API</code></strong>
<ol>
<li>支持多种解码方式：
<ol>
<li><code>greedy search</code>：通过设置温度为 <code>0</code></li>
<li><code>beam search</code>：设置 <code>best_of</code></li>
<li><code>temperature sampling</code>：设置 <code>temperature</code></li>
<li><code>nucleus sampling</code>：设置 <code>top-p</code></li>
</ol>
</li>
<li>支持多种控制重复度的方法：
<ol>
<li>设置 <code>presence_penalty</code></li>
<li>设置 <code>frequency_penalty</code></li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="8-架构选择"><a class="markdownIt-Anchor" href="#8-架构选择"></a> 8. <strong>架构选择</strong></h4>
<ol>
<li>目前 <code>causal decoder</code> 是主流的大模型架构，主要原因有两点：
<ol>
<li>有证据表明，对于没有 <code>fine-tuning</code> 的情况下，<code>causal decoder</code> 表现比 <code>encoder decoder</code> 架构更好，即对于 <code>zero-shot / few-shot</code> 任务，<code>causal decoder</code> 效果更好</li>
<li><code>scaling law</code> 在 <code>causal decoder</code> 中表现明显，对于 <code>encoder decoder</code> 的研究缺乏</li>
</ol>
</li>
</ol>
<h3 id="43-模型训练"><a class="markdownIt-Anchor" href="#43-模型训练"></a> 4.3 模型训练</h3>
<h4 id="1-训练配置"><a class="markdownIt-Anchor" href="#1-训练配置"></a> 1. <strong>训练配置</strong></h4>
<ol>
<li><code>Batch Training</code>
<ol>
<li>对于 <code>Language model pre-training</code> 任务（例如 <code>bert</code> 等），通常用较大的 <code>batch size</code>，例如 <code>2048 examples</code> 或者 <code>4M tokens</code>，这样通常可以提高训练稳定性和吞吐量。</li>
<li>对于 <code>LLM</code> 任务（例如 <code>GPT-3 / PaLM</code>），引入了动态 <code>batch size</code> 的做法，<code>GPT-3</code> 在训练过程中会将 <code>batch size</code> 从 <code>32K tokens</code> 逐步增加到 <code>3.2M tokens</code>，动态 <code>batch size</code> 可提高 <code>LLM</code> 训练稳定性</li>
</ol>
</li>
<li><code>Learning Rate</code>
<ol>
<li><code>LLM</code> 的训练通常采用 <code>warmup</code> + <code>decay</code> 的训练策略</li>
<li><code>warmup</code> 过程通常占总训练 <code>step</code> 数量的 <code>0.1% - 0.5%</code>，学习率逐步从 <code>0</code> 提高到最大值，最大值通常在 <code>5e-5 ~ 1e-4</code> 之间（<code>GPT-3</code> 是 <code>6e-5</code>）。</li>
<li><code>decay</code> 过程通常使用 <code>cosine decay</code> 策略，在剩余的 <code>training step</code> 中逐步将 <code>learning rate</code> 降低到最大值的 <code>10%</code></li>
</ol>
</li>
<li><code>Optimizer</code>
<ol>
<li>通常使用 <code>Adam / AdamW / Adafactor</code> 优化器中的一种</li>
<li>对于 <code>Adam / AdamW</code>，超参数配置为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn><mo separator="true">,</mo><mtext> </mtext><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn><mo separator="true">,</mo><mtext> </mtext><mi>ϵ</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\beta_1=0.9,\ \beta_2=0.95,\ \epsilon =10^{-8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>，<code>GPT-3</code> 就使用了这组配置。</li>
<li><code>Adafactor</code> 优化器相比 <code>Adam / AdamW</code>，优点是训练过程更省显存，超参数配置为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn><mo separator="true">,</mo><mtext> </mtext><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>1.0</mn><mo>−</mo><msup><mi>k</mi><mrow><mo>−</mo><mn>0.8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\beta_1=0.9,\ \beta_2=1.0-k^{-0.8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0</span><span class="mord mtight">.</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>，<code>k</code> 表示训练步数，<code>PaLM / T5</code> 就使用了 <code>Adafactor</code> 优化器。</li>
</ol>
</li>
<li>训练稳定化
<ol>
<li><code>LLM</code> 训练不稳定是常见情况，可以会导致训练崩溃</li>
<li>通常做法有：
<ol>
<li><code>weight decay 0.1</code></li>
<li><code>gradient clipping to 1.0</code></li>
</ol>
</li>
<li>损失函数尖峰（<code>loss spike</code>）也会导致训练不稳定，通常做法有：
<ol>
<li>模型回退到 <code>loss spike</code> 出现之前最近一次 <code>checkpoint</code>，且弃用导致 <code>loss spike</code> 的数据，<code>PaLM / OPT</code> 使用了这种方法</li>
<li><code>GLM</code> 模型发现 <code>Embedding layer</code> 不正常梯度容易导致 <code>loss spike</code>，因此缩小了 <code>embedding layer gradient</code></li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="2-可拓展性训练技术"><a class="markdownIt-Anchor" href="#2-可拓展性训练技术"></a> 2. <strong>可拓展性训练技术</strong></h4>
<ul>
<li><code>LLM</code> 训练会有两个技术难题：<em>训练吞吐量的增加</em> 和 <em>加载大模型到显存</em>，解决方法包含以下几种（以下几种并不冲突，可以同时使用）：
<ol>
<li><code>3D Parallelism</code>
<ol>
<li><code>Data parallelism</code>
<ol>
<li>主流框架都已有实现（例如 <code>pytorch</code> 的 <code>ddp</code>）</li>
<li>划分数据到不同 <code>GPU</code>，每个 <code>GPU</code> 独立前向反向</li>
<li>每个 <code>step</code> 中 <code>gradient</code> 都需要聚合一次，以确保每个 <code>GPU</code> 上参数更新保持一致</li>
</ol>
</li>
<li><code>Pipeline parallelism</code>
<ol>
<li>模型不同的层放到不同的 <code>GPU</code> 上训练，由于 <code>LLM</code> 基本都是 <code>Transformer</code> 架构，所以一个 <code>GPU</code> 通常会加载连续的几层，这样可以有效降低 <code>hiddent state</code> 和 <code>gradient</code> 通信量</li>
<li><code>GPU one-by-one</code> 计算会导致 <code>GPU</code> 利用率很低（流水存在大量气泡），需要有效的排计算流水</li>
<li><code>Gpipe</code> 和 <code>PipeDream</code> 可以通过重整数据 <code>batch</code> 和异步梯度更新提高 <code>GPU</code> 利用率
<ul>
<li><code>Gpipe</code> 可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/">这篇博客</a></li>
<li><code>PipeDream</code> 可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/">这篇博客</a></li>
</ul>
</li>
</ol>
</li>
<li><code>Tensor parallelism</code>
<ol>
<li><code>Tensor parallelism</code> 是把模型一层的参数切成多份，每份独立计算，因此也叫做 <code>Model parallelism</code>。</li>
<li>即 <code>Pipeline parallelism</code> 是横向把模型切成几段，而 <code>Tensor parallelism</code> 是纵向把模型切成几段。</li>
<li>以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi mathvariant="normal">@</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">Y=X @ A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord mathnormal">A</span></span></span></span> 矩阵乘运算为例，<code>Tensor parallelism</code> 是将 <code>A</code> 纵向切分为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>A</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[A_1,A_2]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> 放到不同的 <code>GPU</code> 上独立运算，则 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">[</mo><mi>X</mi><mi mathvariant="normal">@</mi><msub><mi>A</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>X</mi><mi mathvariant="normal">@</mi><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Y=[X@A_1,X@A_2]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">@</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li>
<li><code>Tensor parallelism</code> 需要分析每个算子的数据依赖情况，比如矩阵乘的右矩阵可以按列划分，但不可以按行划分。</li>
<li>目前一些开源框架都支持了 <code>Tensor parallelism</code>，例如 <code>Megatron-LM</code> (可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/21/Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism/">这篇博客</a>) 和 <code>Colossal-AI</code></li>
</ol>
</li>
</ol>
</li>
<li><code>ZeRO</code>
<ol>
<li>通过分片存储等方法降低冗余，<code>ZeRO-DP</code> 和 <code>ZeRO-R</code> 可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/21/ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/">这篇博客</a></li>
<li><code>FSDP</code> 是 <code>PyTorch</code> 官方提出和实现的类似于 <code>ZeRO</code> 的算法，全称是 <code>Fully Sharded Data Parallel</code>。
<ol>
<li>代码：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fairscale">https://github.com/facebookresearch/fairscale</a></li>
<li>文章：<a target="_blank" rel="noopener" href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">https://engineering.fb.com/2021/07/15/open-source/fsdp/</a></li>
</ol>
</li>
</ol>
</li>
<li>混合精度训练
<table>
<thead>
<tr>
<th style="text-align:center">Format</th>
<th style="text-align:center">Bits</th>
<th style="text-align:center">Exponent</th>
<th style="text-align:center">Fraction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">FP32</td>
<td style="text-align:center">32</td>
<td style="text-align:center">8</td>
<td style="text-align:center">23</td>
</tr>
<tr>
<td style="text-align:center">FP16</td>
<td style="text-align:center">16</td>
<td style="text-align:center">5</td>
<td style="text-align:center">10</td>
</tr>
<tr>
<td style="text-align:center">BF16</td>
<td style="text-align:center">16</td>
<td style="text-align:center">8</td>
<td style="text-align:center">7</td>
</tr>
</tbody>
</table>
<ol>
<li><code>FP16</code> 和 <code>FP32</code> 混合训练是比较常见的操作，但最近有研究发现 <code>FP16</code> 会造成模型精度降低。</li>
<li><code>BF16</code> 是为了解决 <code>FP16</code> 造成精度降低而提出的数据类型，全称是 <code>Brain Floating Point</code>，这种数据类型牺牲了表达精度，提高了表达范围。</li>
</ol>
</li>
<li>总体训练建议
<ol>
<li><code>3D Parallelism</code>
<ol>
<li>在实际应用中，<code>3D Parallelism</code> 通常是联合使用的，例如 <code>BLOOM</code> 模型，是用了：
<ul>
<li><code>4</code> 路 <code>Tensor Parallelism</code></li>
<li><code>8</code> 路 <code>Data Parallelism</code></li>
<li><code>12</code> 路 <code>Pipeline Parallelism</code><br />
共 <code>384 (384=4 x 8 x 12)</code> 块 <code>A100 GPU</code> 训练得到的。</li>
</ul>
</li>
<li><code>DeepSpeed</code>、<code>Colossal-AI</code>、<code>Alpa</code> 等开源库都对三种并行支持较好</li>
</ol>
</li>
<li>减小显存占用
<ol>
<li><code>ZeRO</code> 和 <code>FSDP</code> 等算法可有效降低大模型训练过程中的显存占用，从而可以训练参数量非常大的模型。</li>
<li><code>DeepSpeed</code>、<code>PyTroch</code>、<code>Megatron-LM</code> 等开源库都已集成此功能。</li>
</ol>
</li>
<li><code>predictable scaling</code>
<ol>
<li><code>GPT-4</code> 提出的，在小模型上等效预测放大后模型的效果，对超大规模网络训练来说必不可少。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h2 id="5-llm-的适配微调"><a class="markdownIt-Anchor" href="#5-llm-的适配微调"></a> 5. <code>LLM</code> 的适配微调</h2>
<ul>
<li>大模型在预训练结束之后，需要适配微调来使得大模型释放全部能力以及对齐人类的三观，主要的微调方式有两种：
<ul>
<li><code>Instruction Tuning</code>（指令微调）：用来提高或解锁大模型在某些方面的能力。</li>
<li><code>Alignment Tuning</code>（对齐微调）：对齐人类三观。</li>
</ul>
</li>
</ul>
<h3 id="51-instruction-tuning-指令微调"><a class="markdownIt-Anchor" href="#51-instruction-tuning-指令微调"></a> 5.1 <code>Instruction Tuning</code> 指令微调</h3>
<ol>
<li><code>Instruction Tuning</code> 可以大幅提高 <code>LLM</code> 的指令遵循能力，即让 <code>LLM</code> 可以解决具体的任务（指令）</li>
<li><code>Instruction Tuning</code> 通常需要两步：
<ol>
<li>收集指令数据：通常是构造结构化数据</li>
<li>使用收集到的指令数据用监督学习的方法微调 <code>LLM</code></li>
</ol>
</li>
</ol>
<h4 id="1-格式化实例的构造"><a class="markdownIt-Anchor" href="#1-格式化实例的构造"></a> 1. 格式化实例的构造</h4>
<p><img src="https://s2.loli.net/2024/09/05/aiP1VtZQzDSIpHT.png" alt="LLM_survey_12.png" /></p>
<ul>
<li>本文用三种（主要的）数据上格式化实例的构造举例：
<ol>
<li>格式化 <code>NLP</code> 任务数据（如图 <code>11.a</code>）</li>
<li>格式化日常对话数据（如图 <code>11.b</code>）</li>
<li>格式化人工合成数据（如图 <code>11.c</code>）</li>
</ol>
</li>
</ul>
<ol>
<li><strong>格式化 <code>NLP</code> 任务数据</strong>
<ol>
<li>这里的 <code>NLP</code> 任务是指常见的自然语言任务，例如：
<ol>
<li>文本总结（<code>text summarization</code>）</li>
<li>文本分类（<code>text classification</code>）</li>
<li>翻译（<code>translation</code>）</li>
</ol>
</li>
<li>格式化的 <code>NLP</code> 任务数据通常由四部分组成：
<ol>
<li>任务描述（<code>Task description</code>）</li>
<li>示例（<code>Demonstrations</code>）</li>
<li>输入</li>
<li>输出</li>
</ol>
</li>
<li>任务描述部分很重要，去掉之后微调效果下降明显</li>
<li><code>PromptSource</code> 是个很好用的格式化 <code>NLP</code> 任务数据的开源工具，提供了很多任务模板，同时提供了一个开源 <code>Prompt</code> 集合数据集称为 <code>Public Pool of Prompts (P3)</code></li>
</ol>
</li>
<li><strong>格式化日常对话数据集</strong>
<ol>
<li><code>NLP</code> 数据集上的格式化实例缺乏 <code>Prompt</code> 的多样性，且和人类实际需求不匹配</li>
<li>日常对话数据集主要包含两部分：
<ol>
<li>任务描述（<code>Task description</code>）</li>
<li>输出</li>
</ol>
</li>
<li>这两部分都由人类完成，其中：
<ol>
<li>任务描述来自于 <code>ChatGPT API</code> 收集到的问题以及人类标注员，内容包含：
<ol>
<li>开放内容生成（<code>opened generation</code>）</li>
<li>开放问答（<code>open question answering</code>）</li>
<li>头脑风暴（<code>brainstorm</code>）</li>
<li>聊天（<code>chatting</code>）</li>
</ol>
</li>
<li>输出全部由人类标注员完成</li>
</ol>
</li>
</ol>
</li>
<li><strong>格式化人工合成数据</strong>
<ol>
<li>以上两种数据集的格式化实例的方法要么需要大量的标注员，要么需要大量的手动收集，格式化人工合成数据是个半自动方法。</li>
<li>原理是通过将已收集的格式化数据输入到大模型中，让大模型给出相似的实例。</li>
<li>步骤：
<ol>
<li>首先用少量人工标注的格式化数据作为样本放入样本库</li>
<li>大模型从样本库中随机采样，仿照样本格式输出另外一个相似的实例</li>
<li>过滤生成实例的质量，如果没问题则加入样本库中，重复第一步</li>
</ol>
</li>
</ol>
</li>
<li><strong>格式化数据过程中的关键因素</strong>
<ol>
<li>指令的缩放法则（<code>Scaling the instructions</code>）
<ol>
<li>指令微调数据集中涉及到的任务种类越多，多样性越丰富，效果就越好</li>
<li>一种任务中的实例数量不需要很多，大模型容易饱和</li>
</ol>
</li>
<li>格式设计（<code>Formatting design</code>）
<ol>
<li>必须包含任务描述，任务描述对 <code>LLM</code> 理解任务至关重要</li>
<li>适当数量的演示示例可以降低模型对指令工程的敏感性</li>
<li>引入 <code>CoT (Chain-of-Thought)</code> 和非 <code>CoT</code> 数据会让 <code>LLM</code> 在需要多跳推理能力的任务和不需要多跳推理的任务上表现好</li>
<li>引入其他信息（例如： 避免的事项、原因和建议等）对 <code>LLM</code> 性能影响较小或会产生负面影响</li>
</ol>
</li>
<li>总结
<ol>
<li>指令数据集的多样性和质量比数量更重要</li>
<li>标注员编写人类需求实例比特定于数据集的任务更有用，但人力成本较高</li>
<li>为减少人力成本，<code>LLM</code> 自动构建指令数据集是一种有效的方法</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="2-指令微调策略"><a class="markdownIt-Anchor" href="#2-指令微调策略"></a> 2. 指令微调策略</h4>
<ol>
<li><strong>平衡数据分布</strong>
<ol>
<li>最简单的也是应用最广泛的方法是：混合所有数据，然后平等的采样每一个样本</li>
<li>有研究表明，对于高质量的数据集，应该适当提高其采样概率</li>
<li>需要设置采样上限，防止较大数据集主导整个指令微调过程</li>
<li>每个数据集所侧重的能力不同，用单一数据集微调无法全面增强模型容量，通常需要多种类型的数据混合微调，例如：
<ol>
<li><code>NLP</code> 任务数据集（<code>FLAN v2</code>）</li>
<li>聊天数据集（<code>ShareGPT</code>）</li>
<li>合成数据（<code>GPT4-Alpaca</code>）</li>
</ol>
</li>
</ol>
</li>
<li><strong>预训练与指令微调联合训练</strong>
<ol>
<li>有些模型预训练阶段是个多任务模型，一个任务是常规预训练，另一个任务是指令对齐任务，例如 <code>OPT-IML</code> 模型</li>
<li>有些模型预训练阶段的数据中混入了一小部分格式化后的指令对齐数据，在预训练过程中就完成了指令对齐，例如 <code>GLM-130B</code> 和 <code>Galactica</code> 等</li>
</ol>
</li>
<li><strong>多阶段指令微调</strong>
<ol>
<li>指令微调数据集大体上分为两种：任务格式化指令对齐数据和日常对话指令微调数据</li>
<li>其中任务格式化指令数据通常数据量更大，数据集的平衡是非常重要的，因此通常需要非常小心的平衡两种数据</li>
<li>另外一种更简单的方法是将两种数据分成两个阶段分别训练，例如：先用任务格式化指令数据微调，然后再用日常对话指令数据微调</li>
<li>实际应用中为了防止能力遗忘问题（<code>Capacity forgetting issue</code>），通常在日常对话指令数据微调过程中加入少量任务格式化指令数据</li>
</ol>
</li>
<li><strong>一些其他微调技巧</strong>
<ol>
<li>多轮对话数据的高效微调方法
<ol>
<li>对于人机多轮对话数据，通常的做法是将其拆分成多组问答对，然后每对对话数据单独训练，这种训练方式会有对话内容重叠的问题</li>
<li>一种高效的多轮对话数据微调方法是将整个对话一次送入 <code>LLM</code> 微调，且只对 <strong>机器输出的对话部分计算损失</strong></li>
</ol>
</li>
<li>建立 <code>LLM</code> 的自我认知
<ol>
<li>为 <code>LLM</code> 建立自我认知对于部署应用 <code>LLM</code> 很有必要，例如：名称、开发者和隶属关系</li>
<li>一种实用的方法是 <strong>构建自我认知相关的微调指令</strong>，也可以加入带有自我认知相关的提示前缀，例如：”以下是人类和 <code>AI</code> 助手之间的对话“</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="3-指令微调的作用"><a class="markdownIt-Anchor" href="#3-指令微调的作用"></a> 3. 指令微调的作用</h4>
<ol>
<li><strong>提升模型表现</strong>
<ol>
<li>指令微调已经成为提升模型能力和解锁模型部分能力的重要方法</li>
<li>大模型和小模型都可以在指令微调过程中获得收益，模型越大收益越大</li>
<li>经过指令微调过的小模型比没有经过指令微调的大模型效果更好</li>
<li>指令微调不止可以用于 <code>LLM</code>，预训练 <code>NLP</code> 模型也可以用</li>
<li>此外指令微调的计算代价和数据量和预训练过程相比很小</li>
</ol>
</li>
<li><strong>任务泛化</strong>
<ol>
<li>大量研究表明，指令微调可以使大模型在见过和没有见过的任务上都表现良好（也可以认为指令微调可以使大模型获得任务泛化的能力）</li>
<li>指令微调甚至可以缓解大模型的部分缺点，例如重复生成</li>
<li>指令微调可以让大模型泛化到跨语言种类的相似任务上，例如 <code>BLOOMZ-P3</code> 是在 <code>BLOOM</code> 上仅用纯英文数据 <code>P3</code> 微调得到的，经测试发现在多语种语句补全任务上，<code>BLOOMZ-P3</code> 比 <code>BLOOM</code> 提升超过 <code>50%</code>，表明即使用纯英文数据做指令微调，可以提高大模型在其他语言上的表现</li>
<li>研究表明，仅用纯英文数据做指令微调，即可解决多语种任务的性能</li>
</ol>
</li>
<li><strong>任务专业化</strong>
<ol>
<li>在专业领域指令数据上上做指令微调，大模型可以变成专业模型，例如：在医疗、法律、金融等指令微调数据集上做微调</li>
</ol>
</li>
</ol>
<h4 id="4-指令微调经验性的分析"><a class="markdownIt-Anchor" href="#4-指令微调经验性的分析"></a> 4. 指令微调经验性的分析</h4>
<ol>
<li><strong>指令微调数据集</strong>
<ol>
<li>任务相关的指令数据
<ul>
<li>常用的数据集是 <code>FLAN-T5</code> 包含 <code>1836</code> 个任务的超过 <code>15M</code> 条指令数据</li>
</ul>
</li>
<li>日常对话指令数据
<ul>
<li>常用的数据集是 <code>ShareGPT</code>，包含 <code>63K</code> 条真实用户指令</li>
</ul>
</li>
<li>合成指令数据
<ul>
<li>常用的数据集是 <code>Self-Instruct-52K</code> 包含 <code>52K</code> 条指令对共计 <code>82K</code> 个实例的输入输出</li>
</ul>
</li>
</ol>
</li>
<li><strong>微调效果提升策略</strong>
<ol>
<li>提升指令的复杂性
<ul>
<li>可以参考 <code>WizardLM</code> 设计，通过增加约束、提高推理步骤、提高输入复杂度的方式，逐步提高指令的复杂性</li>
</ul>
</li>
<li>提升指令数据集话题的多样性
<ul>
<li>合成数据通常会有话题多样性差的问题，可以通过更强的 <code>LLM</code> 对其进行话题多样性扩写，例如用 <code>ChatGPT</code> 扩写 <code>Self-Instruct-52K</code> 数据</li>
</ul>
</li>
<li>提升指令数据量
<ul>
<li>在指令质量不下降的情况下，尽可能提高指令数据量</li>
</ul>
</li>
<li>平衡指令的困难程度
<ol>
<li>指令太简单或太复杂都会影响模型微调，容易出现训练不稳定或过拟合的问题</li>
<li>需要去除数据集中太简单或太复杂的指令</li>
<li>可以通过用 <code>LLaMa</code> 推理指令输出的困惑度来对指令的困难程度排序，删除过难过易的样本</li>
</ol>
</li>
</ol>
</li>
<li><strong>实验设置和结果分析</strong>
<ol>
<li>本节是 <code>Survey</code> 的作者亲自做的指令微调实验的设置和结果分析</li>
<li>实验设置
<ol>
<li>模型：对 <code>LLaMa</code> 做指令微调，包括 <code>LLaMa-7B</code> 和 <code>LLaMa-13B</code> 两个模型</li>
<li><code>code base</code>: <code>[YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat)</code> 一个作者实验室开源的代码</li>
<li>硬件环境：一台 <code>8</code> 卡 <code>A800-80G GPU</code> 服务器</li>
<li>超参数：和 <code>[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)</code> 指令微调超参数相同（<code>Stanford Alpaca</code> 是一个基于 <code>LLaMa</code> 的指令跟随模型）</li>
<li>微调配置：分成 <code>Chat setting</code> 和 <code>QA setting</code> 两个微调配置
<ol>
<li><code>Chat setting</code>:
<ol>
<li>利用来自日常对话的用户指令和请求做微调</li>
<li>微调实验：
<ol>
<li><code>baseline</code>（对照组）：在 <code>Self-Instruct-52K</code> 上微调的 <code>LLaMa-7B</code> 和 <code>LLaMa-13B</code> 模型</li>
<li>实验组：用上面提到的更科学的数据集和数据集混合方法得到的指令微调数据</li>
</ol>
</li>
<li><code>benchmark</code>：<code>[AlpacaFarm evaluation set](https://github.com/tatsu-lab/alpaca_farm)</code></li>
<li>对比方法：对于 <code>benchmark</code> 的每一条数据，用实验组和对照组的两个模型分别推理给出结果，让 <code>ChatGPT</code> 评判哪个更好，最后比较胜率</li>
</ol>
</li>
<li><code>QA setting</code>:
<ol>
<li>利用来自已有的 <code>NLP</code> 任务的问答数据集做微调</li>
<li><code>benchmark</code>：<code>[MMLU (Massive Multitask Language Understanding)](https://arxiv.org/abs/2009.03300)</code> 和 <code>[BBH (BIG-Bench Hard)](https://arxiv.org/abs/2210.09261)</code> 数据集</li>
</ol>
</li>
</ol>
</li>
<li><code>Prompt</code>：
<ol>
<li>“The following is a conversation between a human and an AI assistant. The AI assistant giveshelpful, detailed, and polite answers to the user’s questions.\n[|Human|]:{input}\n[|AI|]:”.</li>
<li>在微调训练和 <code>benchmark</code> 推理时都加</li>
</ol>
</li>
</ol>
</li>
<li>结果分析
<ol>
<li><code>NLP</code> 任务格式化指令数据对 <code>QA setting</code> 任务有效，对 <code>Chat setting</code> 任务无效</li>
<li>混合多种不同的指令数据对提高模型理解能力有帮助</li>
<li>提高指令的复杂性和多样性主导了指令微调效果的提升</li>
<li>简单提高指令的数量可能没有收益，平衡指令的困难程度也不总是有效的</li>
<li>模型越大，指令微调效果越好</li>
</ol>
</li>
<li>指令微调建议
<ol>
<li>提前准备计算资源（<code>GPU</code>）</li>
<li>用 <code>Alpha</code> 开源库做微调</li>
<li>选择合适的开源预训练 <code>LLM</code> 模型和指令微调数据集，开始微调</li>
<li>如果计算资源有限，可以考虑 <code>LoRA</code> 等微调方式</li>
<li>对于部署，可考虑使用低比特量化等方法减小计算量</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="52-alignment-tuning-对齐微调"><a class="markdownIt-Anchor" href="#52-alignment-tuning-对齐微调"></a> 5.2 <code>Alignment Tuning</code> 对齐微调</h3>
<h4 id="1-背景和对齐标准"><a class="markdownIt-Anchor" href="#1-背景和对齐标准"></a> 1. 背景和对齐标准</h4>
<ol>
<li><strong>背景</strong>
<ol>
<li>大模型在很多 <code>NLP</code> 任务中表现出非凡的能力，但有时会生成一些负面的内容，例如：
<ol>
<li>伪造虚假信息</li>
<li>追求不正确的目标</li>
<li>生成错误、误导或带有偏见的表达</li>
</ol>
</li>
<li>出现这种问题的原因通常是在预训练数据中混入了负面内容，但预训练数据太大，清洗是不现实的</li>
<li>因此才出现了对齐微调，对齐微调的目标是 <code>3H</code>（<code>helpfulness / honesty / harmlessness</code>）</li>
<li>对齐微调可能会损伤大模型原有的生成能力，这也被称为 <code>alignment tax</code>（对齐税）</li>
</ol>
</li>
<li><strong>对齐标准</strong>
<ol>
<li><code>Helpfulness</code> 帮助性
<ol>
<li>尽可能简洁高效的帮助人类解决任务或回答问题</li>
<li>当需要更高层次的阐明时，大模型应展现出提出相关的问题来获取额外信息的能力，并且在这个过程中表现出适当的敏感度、洞察力和审慎性</li>
<li>做帮助行为对齐是一件有挑战的事，因为很难精确定义和衡量用户的意图</li>
</ol>
</li>
<li><code>Honesty</code> 诚实性
<ol>
<li>应该给出诚实的回答而不是伪造的信息</li>
<li>为避免产生任何形式的欺诈和虚假陈述，大模型应该适当传达出其输出的不确定性</li>
<li>模型应该了解自身的能力边界</li>
<li>与 <code>Helpfulness</code> 和 <code>Harmlessness</code> 相比，<code>Honesty</code> 更客观，因此对齐需要的人力付出会更少</li>
</ol>
</li>
<li><code>Harmlessness</code> 无害性
<ol>
<li>模型输出不应包含冒犯或歧视</li>
<li>当模型被诱导做出危险行为时，应礼貌的拒绝</li>
<li>但如何定义有害涉及到使用模型的用户、问题类型以及上下文</li>
</ol>
</li>
<li>总结
<ol>
<li>以上三种对齐标准都比较主观，都是基于人类感知定义的，因此直接量化然后去优化是比较难的</li>
<li>一种有效的方法是：<strong>红队对抗</strong>，即让人类对于输入产生违反上述对齐标准的输出，然后让模型将这些作为反面教材去优化</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="2-收集人类反馈"><a class="markdownIt-Anchor" href="#2-收集人类反馈"></a> 2. 收集人类反馈</h4>
<p>人类反馈在对齐微调大模型的过程中至关重要</p>
<ol>
<li><strong>人类标注员应如何选择</strong>
<ol>
<li>基本要求：
<ol>
<li>学历好：至少本科学历</li>
<li>英文好：英语母语</li>
</ol>
</li>
<li>为了减小研究员和标注员的不匹配，需要研究员和标注者分别标注少量样本，然后选择标注员中和研究员标注相似性高的作为后续标注员</li>
<li>在标注员中选择一个优秀的超级标注员团队，这个团队更高优先级的为大模型标注对齐数据</li>
</ol>
</li>
<li><strong>如何收集人类反馈</strong>
<ol>
<li>基于排序的方法
<ol>
<li>最早的标注方式是：仅选择候选列表中标注员认为最优的选项，且由于每个标注员的偏好不同，这会导致人类的反馈不准确或不完整</li>
<li>基于排序的方法是：系统会自动从候选列表中选择两个选项，标注员选择哪个更好，系统根据标注员的选择对整个候选列表排序，这样可以有效降低不同标注员偏好不同导致的偏差，标注信息更完整</li>
</ol>
</li>
<li>基于问题的方法
<ol>
<li>标注员回答研究员提问的问题，这种类型的标注信息细节丰富，涵盖了对齐标准和其他约束</li>
</ol>
</li>
<li>基于规则的方法
<ol>
<li>通过人为书写的规则自动判断模型输出是否有毒有害</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="3-rlhf-人类反馈的强化学习"><a class="markdownIt-Anchor" href="#3-rlhf-人类反馈的强化学习"></a> 3. <code>RLHF</code> 人类反馈的强化学习</h4>
<ol>
<li><strong><code>RLHF</code> 系统</strong>
<ol>
<li>包含三个关键组成部分
<ol>
<li>一个用于对齐微调的预训练 <code>LLM</code></li>
<li>一个基于人类反馈训练的奖励模型</li>
<li>一套强化学习算法</li>
</ol>
</li>
<li>预训练模型作为对齐的起点
<ol>
<li><code>InstructGPT 175B</code> 是在预训练的 <code>GPT-3</code> 上对齐的</li>
<li><code>GopherCite 280B</code> 是在 <code>Gopher</code> 上对齐的</li>
</ol>
</li>
<li>奖励模型用于评价人类对模型输出的偏好程度
<ol>
<li>使用人类偏好数据，可以是一个 <code>LM</code> 的微调，也可以是 <code>LM</code> 从头训练</li>
<li>通常比 <code>LLM</code> 参数量少，例如 <code>OpenAI</code> 用 <code>6B GPT-3</code> 作为 <code>Reward Model</code>，<code>DeepMind</code> 用 <code>7B Gopher</code> 作为 <code>Reward Model</code></li>
</ol>
</li>
<li>强化学习算法用奖励模型的信号去微调预训练大模型
<ol>
<li>近端策略优化算法 <code>PPO (Porximal Policy Optimization)</code> 比较常用</li>
</ol>
</li>
</ol>
</li>
<li><strong><code>RLHF</code> 关键步骤</strong><br />
<img src="https://s2.loli.net/2024/09/15/rvDQYzftMBw68l2.png" alt="LLM_survey_13.png" />
<ol>
<li>有监督微调
<ol>
<li>通常以“指令 —— 输出”数据对形式构造，做有监督微调</li>
<li>通常由人类标注员标注得到，需要包含较丰富的多样性</li>
<li>在一些特定场景的 <code>RLHF</code> 中，此步骤可以跳过</li>
</ol>
</li>
<li>训练奖励模型
<ol>
<li>通常由 <code>LM</code> 生成对每个指令生成一系列输出，由人类标注员对输出进行 <strong>偏好排序</strong> 作为训练数据</li>
<li>奖励模型实际是在模仿人类标注员的偏好</li>
<li>通常训练得到的奖励模型更偏向于无害但无帮助的回答，这被称为 <strong>规避问题（evasion problem）</strong></li>
<li>最近有一些工作用 <code>AI Agent</code> 代替标注员对 <code>LM</code> 的输出进行偏好排序（<code>RLAIF</code>）<s>开始套娃</s>，<code>AI Agent</code> 的排序原则是预设的指令对齐原则，这种方式的优势是：
<ol>
<li>可以缓解规避问题，生成无害且有帮助的回答</li>
<li>降低对标注员的依赖，降本增效</li>
</ol>
</li>
</ol>
</li>
<li>强化学习微调
<ol>
<li>用强化学习的方式对第一步微调后的 <code>LM</code> 进行训练，其中强化学习的各个要素分别为：
<ol>
<li>策略（<code>policy</code>）：微调后的 <code>LM</code></li>
<li>动作空间（<code>action space</code>）：<code>LM</code> 的词表</li>
<li>状态（<code>state</code>）：目前生成的 <code>token</code> 序列</li>
<li>奖励（<code>reward</code>）：由第二步训练得到的 <code>RM</code> 提供</li>
<li>为了防止模型退化（逐渐偏离预训练行为），通常在奖励函数中加入一项惩罚项，例如：<code>InstructGPT</code> 通过在奖励函数中加入当前模型输出和微调之前的模型输出之间的 <code>KL</code> 散度，缓解模型的退化问题</li>
<li>通常重复迭代第二步（<code>RM</code> 的训练）和本步（强化学习微调），可以得到较好的模型</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><strong><code>RLHF</code> 实践策略</strong><br />
成功实现 <code>RLHF</code> 是比较困难的，这里提供一些实践上的策略和技巧
<ol>
<li>奖励模型如何有效训练
<ol>
<li>大的奖励模型在评判大模型输出质量方面更优
<ol>
<li>在实践中，奖励模型有小模型（例如：<code>InstructGPT</code> 用 <code>6B GPT</code> 作为奖励模型）和大模型（和对齐模型一样大或者更大的模型作为奖励模型）两条路线，其中大模型路线在评判 <code>LLM</code> 输出质量方面更优</li>
<li>可以用 <code>LLM</code> 预训练过程中的检查点参数初始化 <code>RM</code>，这样可以缓解由于预训练数据不一致导致的对齐策略不匹配问题，<code>LLaMa 2</code> 就是这么做的</li>
<li>大的奖励模型训练容易过拟合，用人类标注的对齐数据的提示词首选响应作为正则化项可以缓解过拟合</li>
</ol>
</li>
<li>训练一个满足所有对齐标准的奖励模型很难，可以用多个对齐模型的组合来实现，每个对齐模型只关注部分对齐标准
<ol>
<li>例如每个奖励模型只关注一项标准，然后用某种策略（例如平均或加权）组合每一种奖励模型的奖励值作为最终奖励</li>
<li>使用这种方法可以手动条件每一项对齐标注的严格程度（通过调节权重）</li>
</ol>
</li>
</ol>
</li>
<li>强化学习过程如何有效训练
<ol>
<li>强化学习通常是不稳定且超参数敏感的，因此需要在强化学习前模型已经得到充分的有监督训练</li>
<li>常用的方法是：
<ol>
<li>收集多样性较为丰富的对齐数据输入，通过采样策略让 <code>LLM</code> 对每个输入推理得到 <code>N</code> 个输出</li>
<li>用奖励模型评判 <code>N</code> 个输出，得到最优的输出</li>
<li>用“输入 —— 最优输出”数据对有监督训练 <code>LLM</code>，知道模型的表现不再提升（收敛）</li>
</ol>
</li>
<li>需要有部分人类标注的对齐数据 <code>benchmark</code> 用来真实地评价模型对齐效果</li>
<li>通常需要奖励模型和强化学习优化过程的多轮迭代</li>
</ol>
</li>
<li>强化学习过程如何高效训练
<ol>
<li>强化学习过程和奖励模型训练多轮迭代会带来巨大的存储和计算开销</li>
<li>可以将奖励模型部署在单独的服务器上，通过 <code>API</code> 调用</li>
<li>对齐模型对于每个指令需要同时输出多组候选输出，用集束搜索算法（<code>beam search</code>）效率更高，且可以提高输出的质量和多样性</li>
</ol>
</li>
</ol>
</li>
<li><strong><code>RLHF</code> 的过程监督</strong>
<ol>
<li><code>RLHF</code> 的监督可以分为两种：
<ol>
<li>对结果监督：监督 <code>LLM</code> 对于输入指令的完整输出</li>
<li>对过程监督：更细粒度的监督 <code>LLM</code> 的输出过程，例如输出的每个句子、每个单词、每个推理步等</li>
</ol>
</li>
<li>过程监督数据集：<code>PRM800K</code>
<ol>
<li><code>PRM800K</code> 是 <code>OpenAI</code> 开源的 <code>RLHF</code> 过程监督数据集，可以用其训练 <code>PRM (Process-supervised Reward Model)</code> 过程监督奖励模型</li>
<li><code>PRM800K</code> 包含：
<ol>
<li><code>12K</code> 个过程标注的数学问题</li>
<li><code>75K</code> 个由 <code>LLM</code> 生成的解法</li>
<li><code>800K</code> 个过程正确性标签，标签分为：
<ol>
<li><code>Positive</code> 正面</li>
<li><code>Negative</code> 负面</li>
<li><code>Neutral</code> 中性</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>过程监督可以和专家迭代强化学习算法（<code>Expert Iteration (EXIT)</code>）结合</li>
</ol>
</li>
</ol>
<h4 id="4-不用-rlhf-的对齐"><a class="markdownIt-Anchor" href="#4-不用-rlhf-的对齐"></a> 4. 不用 <code>RLHF</code> 的对齐</h4>
<ol>
<li><strong><code>RLHF</code> 做对齐微调的痛点</strong>
<ol>
<li>需要训练被对齐模型和奖励模型，同时需要频繁推理参考模型（对齐前的模型，不训练，用于防止对齐过程导致模型退化），存储和计算代价大</li>
<li><code>RLHF</code> 过程中常用的 <code>PPO</code> 算法非常复杂，且对超参数敏感</li>
</ol>
</li>
<li><strong>不用 <code>RLHF</code> 的对齐如何做？</strong>
<ol>
<li>用 <code>SFT</code> 代替 <code>RLHF</code>，即用基于有监督微调代替基于人类反馈的强化学习</li>
<li>这种方法假设 <code>LLM</code> 可以从对齐数据集中学习到对齐行为</li>
<li>这种方法包含两个关键：<strong>构建对齐数据集</strong> 和 <strong>设计微调损失函数</strong></li>
</ol>
</li>
<li>构建对齐数据集<br />
对齐数据集的构建主要包含三种方法：<strong>基于奖励模型的方法</strong>、<strong>基于大模型的生成方法</strong> 和 <strong>基于大模型的互动方法</strong>
<ol>
<li>基于奖励模型的方法
<ol>
<li>从生成模型中采样一批数据</li>
<li>使用奖励模型对其进行评分，筛选出高质量样本（或对样本进行质量分组）</li>
<li>用奖励模型的输出做为监督信息，微调 <code>LLM</code></li>
<li>使用这种方法的算法有：
<ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.06767"><code>RAFT</code></a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.13636"><code>Quark</code></a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.16755"><code>ILF</code></a></li>
</ol>
</li>
<li>一些好用的开源奖励模型：
<ol>
<li><code>OpenAssistant</code> 开源的 <code>DeBERTa</code> 系列奖励模型</li>
<li>复旦大学开源的 <code>Moss-7B</code> 奖励模型</li>
<li>斯坦福大学开源的 <code>Flan-T5-xl</code> 奖励模型</li>
</ol>
</li>
<li>基于奖励模型的方法存在的问题：
<ol>
<li>奖励模型本身的训练需要大量对齐标注数据，这些数据是难以获得且昂贵的</li>
<li>尽管已有的奖励模型可以复用，但它们可以无法准确捕捉到另外一个独立训练的大模型的不对齐行为</li>
</ol>
</li>
</ol>
</li>
<li>基于大模型生成的方法
<ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.08073"><code>Constitutional AI</code></a> 将人类监督映射为一组原则（自然语言指令的集合），让 <code>LLM</code> 重复的用这组原则批判并修正自己的输出</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.03047"><code>Self-Align</code></a> 是先用 <code>Self-Instruct</code> 方法生成多样性强的指令数据集，然后模型会被提示多个人类编写的原则，这些原则描述了期望的模型规则和行为，以生成符合 <code>3H</code> 标准的数据用于对齐训练</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.04072"><code>FIGA</code></a> 采用正样本（<code>LLM</code> 精炼后的输出）和负样本（原始到低质量输出）对比学习的方法，缓解了 <code>SFT</code> 只能用正样本监督学习的问题，使 <code>LLM</code> 能够深入理解哪些细粒度的修改实际上导致了良好的响应</li>
</ol>
</li>
<li>基于大模型交互的方法
<ol>
<li>传统大模型训练都是孤立的，没有外部反馈信号来改进自己</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.16960"><code>Stable Alignment</code></a> 通过多个 <code>LLM</code> 交互来获得改进的反馈，从而实现自我改进</li>
</ol>
</li>
</ol>
</li>
<li><strong>有监督对齐微调</strong><br />
有监督对齐微调有点类似于指令微调，主要的训练损失函数依然是 <code>Sequence-to-Sequence</code> 的交叉熵损失，但一些研究提出对主要训练目标的改机，并提出了一些辅助损失，可用于挖掘对齐数据集的潜力，提高对齐质量
<ol>
<li>主要训练目标
<ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.02676"><code>CoT (Chain of Hindsight)</code></a> 对有监督微调数据集中的正负样本分别添加“一个有帮助的回答”和“一个没有帮助的回答”两种指令，且仅对有特殊掩码的响应标记计算损失</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.13636"><code>Quark</code></a> 将模型响应数据（对齐监督训练数据）按对齐质量分成不同的分位数，每个监督数据前加入一个特殊的奖励 <code>token</code>，用于表示响应的奖励等级</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.18290"><code>DPO (Direct preference optimization)</code></a> 将大模型本身用于奖励模型，从而不需要显式的奖励模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.04072"><code>FIGA</code></a> 定义细粒度对比学习损失</li>
</ol>
</li>
<li>辅助训练目标
<ol>
<li>排序损失
<ol>
<li>由于奖励模型可以给每一个响应评分，所以收集同一条指令的多组响应可以排序，可以计算排序辅助损失</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.05302"><code>RRHF</code></a> 模型就在 <code>SFT</code> 对齐训练中加入了排序辅助损失</li>
</ol>
</li>
<li>相似性损失
<ol>
<li>模型的响应和人类偏好的相似性可以通过计算潜在空间中的距离来得到</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10425"><code>SLiC-HF</code></a> 就增加了相似性损失函数来对齐响应和人类偏好</li>
</ol>
</li>
<li>对比学习损失
<ol>
<li>对比学习可以用来提高正确的 “指令 —— 响应” 对的概率，降低错误的 “指令 —— 响应” 对的概率，这样做可以使模型学会指令和响应之间的正确相关性。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><strong>总结</strong>
<ol>
<li><code>RLHF</code> 是用对齐数据训练一个 <code>RM</code>，然后用 <code>RM</code> 指导模型微调（给模型的响应打分）</li>
<li><code>SFT</code> 是直接用对齐数据微调模型</li>
<li>二者对比来看，<code>RLHF</code> 学习的是响应级监督，而 <code>SFT</code> 学习的是 <code>token</code> 级监督</li>
<li><code>SFT</code> 微调的模型，在遇到对齐微调数据集之外的数据时，更倾向产生幻觉，尤其是在对齐微调数据集是由大模型生成的时候</li>
<li>由于没有奖励模型做标注质量过滤，<code>SFT</code> 对标注质量更敏感</li>
<li><code>RLHF</code> 对缓解有害输出和增强模型能力方面非常有效，在 <code>LLaMa 2</code> 中，<code>RLHF</code> 同时提高了模型的帮助性和无害性得分，并将其归功于 <code>RLHF</code> 实现了一种更优的 “LLM —— 人类” 交互方式</li>
<li><code>RLHF</code> 实际上是通过鼓励 <code>LLM</code> 通过对比自己生成内容的 “好” 和 “不好” 评价，找到一种纠正策略；而不是像 <code>SFT</code> 一样，强制让模型去模仿 “好” 的外部内容（非自我生成）</li>
<li><code>RLHF</code> 可以减轻幻觉行为，这一点在 <code>GPT-4</code> 中被证实</li>
<li>传统强化学习的缺点，<code>RLHF</code> 一样不少，比如样本效率低下和训练稳定性差</li>
<li><code>RLHF</code> 的 <code>LLM</code> 必须先用 <code>SFT</code> 预训练收敛的参数初始化</li>
<li><code>RLHF</code> 需要标注员参与到复杂的迭代过程中，不能像 <code>SFT</code> 一样标注和训练解耦</li>
<li><code>RLHF</code> 的结果对实验过程细节非常敏感，例如：
<ol>
<li>提示词的选择</li>
<li>奖励模型和 <code>PPO</code> 的训练策略</li>
<li>超参数设置</li>
</ol>
</li>
<li><code>SFT</code> 可以提高预训练模型的容量，<code>RLHF</code> 可能会提高 <code>SFT</code> 的模型容量</li>
<li><code>RLHF</code> 过程还比较原始，需要探索和优化的点还有很多</li>
</ol>
</li>
</ol>
<h3 id="53-参数高效的模型适配微调"><a class="markdownIt-Anchor" href="#53-参数高效的模型适配微调"></a> 5.3 参数高效的模型适配微调</h3>
<ol>
<li><strong>参数高效的微调方法</strong><br />
大模型参数量太多，微调全部参数对大多数用户不可实现，因此 <strong>参数高效的微调方法</strong> 主要是通过降低微调过程中可学习的参数量实现参数高效的微调，主要方法有四种，如图：<br />
<img src="https://s2.loli.net/2024/09/26/TpXSCOGfjocHkJR.png" alt="LLM_survey_14.png" />
<ol>
<li><code>Adapter Tuning</code> 适配器微调
<ol>
<li>适配器微调论文出自 <code>2019</code> 年 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.00751"><code>Parameter-Efficient Transfer Learning for NLP</code></a>，是 <code>Bert</code> 时代的产物</li>
<li>本质是在 <strong>标准 <code>Transformer</code> 模型预训练结束后</strong>，在每个 <code>Transformer block</code> 中加入两个 <code>Adapter block</code>，其他参数冻结，只微调 <code>Adapter block</code></li>
<li><code>Adapter block</code> 中包含降采样(<code>Down project</code>)算子，因此计算量和参数量都较小</li>
<li><code>Adapter block</code> 在 <code>Transformer block</code> 中的位置以及 <code>Adapter block</code> 长什么样？<br />
<img src="https://s2.loli.net/2024/09/26/qvkPDF8ZfOKpaet.png" alt="LLM_survey_15.png" />
<blockquote>
<p>图中所有的绿色块表示微调过程中可训练的参数</p>
</blockquote>
</li>
</ol>
</li>
<li><code>Prefix Tuning</code> / <code>Prompt Tuning</code> 前缀微调
<ol>
<li>前缀微调的基本假设是：预训练之后的大模型实际上已经完全具备解决任何下游任务的能力，只需要找到合适的提示词（不一定是词表中的词）即可。</li>
<li>开山之作是斯坦福的 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.00190"><code>Prefix Tuning</code></a>，详细分析可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/09/26/Prefix-Tuning-Optimizing-Continuous-Prompts-for-Generation/">这篇博客</a></li>
<li>然后是清华大学提出的 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.10385"><code>P-Tuning</code></a>，详细可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/09/29/GPT-Understands-Too/">这篇博客</a></li>
<li>然后是谷歌对 <code>Prefix Tuning</code> 的小修改版本，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08691"><code>Prompt Tuning</code></a>，详细分析可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/09/27/The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning/">这篇博客</a></li>
<li>清华大学还对 <code>P-Tuning</code> 进行了升级 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.07602"><code>P-Tuning v2</code></a>，详细可参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/10/08/P-Tuning-v2-Prompt-Tuning-Can-Be-Comparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks/">这篇博客</a></li>
</ol>
</li>
<li><code>Low-Rank Adaptation (LoRA)</code>
<ul>
<li>具体参考 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/10/12/LoRA-Low-Rank-Adaptation-of-Large-Language-Models/">这篇博客</a></li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="6-使用"><a class="markdownIt-Anchor" href="#6-使用"></a> 6. 使用</h2>
<h1 id="wip"><a class="markdownIt-Anchor" href="#wip"></a> WIP</h1>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhangzhe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">176</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">105</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhangzhe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js', () => {
    // 初始化 Mermaid 配置
    mermaid.initialize({
      theme    : 'dark',  // 设置主题
      logLevel : 3,  // 设置日志等级
      flowchart: { curve: 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 },
      themeVariables: {
        'fontFamily': 'Microsoft YaHei, Arial, sans-serif',  // 设置中文字体
      }
    });

    // 初始化 Mermaid 图表
    mermaid.init(undefined, document.querySelectorAll('pre.mermaid'));
  }, window.mermaid);
}
</script>


  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">


  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'eK3W25jybCO5jVUrYBBpAPqM-gzGzoHsz',
      appKey     : 'F4KVyUj9wHI5c80Bhz7O2uhq',
      placeholder: "说点什么再走吧...",
      avatar     : 'hide',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
