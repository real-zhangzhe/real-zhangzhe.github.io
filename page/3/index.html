<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"real-zhangzhe.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhangzhe&#39;s Blog">
<meta property="og:url" content="https://real-zhangzhe.github.io/page/3/index.html">
<meta property="og:site_name" content="Zhangzhe&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhangzhe">
<meta property="article:tag" content="No mistakes in the tango, not like life.">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://real-zhangzhe.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Zhangzhe's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhangzhe's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The projection of my life.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/archives/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/07/01/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/07/01/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models/" class="post-title-link" itemprop="url">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-07-01 11:16:29" itemprop="dateCreated datePublished" datetime="2025-07-01T11:16:29+08:00">2025-07-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/07/01/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/07/01/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.11903">https://arxiv.org/pdf/2201.11903</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文是 <code>Google Research</code> 团队发表的一篇论文，这篇论文是在已开源的 <strong>非推理</strong> 模型上，不做微调，而是通过 <code>Chain-of-Thought Prompting</code> 的方式来引导模型进行推理，取得了很好的效果。</li>
<li>具体来说，就是在输入 <code>prompt</code> 中加入少量的 <strong>输入-思维链-输出</strong> 三元组示例，引导模型生成中间推理步骤，最终给出答案。</li>
<li>在算术、常识和符号推理等任务上，<code>Chain-of-Thought Prompting</code> 的效果都非常好，比直接给出答案的效果好很多。</li>
<li><code>Chain-of-Thought Prompting</code> 是在模型规模达到一定程度（<code>&gt;= 100B</code>）后，才涌现出的能力，小模型没有这个能力。</li>
</ul>
<h2 id="论文详情"><a class="markdownIt-Anchor" href="#论文详情"></a> 论文详情</h2>
<h3 id="1-核心方法思维链提示chain-of-thought-prompting"><a class="markdownIt-Anchor" href="#1-核心方法思维链提示chain-of-thought-prompting"></a> 1. 核心方法：思维链提示（Chain-of-Thought Prompting）</h3>
<ul>
<li>论文提出一种简单方法：在提示（<code>prompt</code>）中提供 <strong>输入-思维链-输出</strong> 的三元组示例，引导大型语言模型生成一系列中间推理步骤（称为 “思维链”），再得出最终答案。</li>
<li>思维链类似于人类逐步推理的过程（例如，解决数学题时先分解步骤：“先计算A，再计算B，最后得出答案”）。</li>
</ul>
<h3 id="2-关键优势"><a class="markdownIt-Anchor" href="#2-关键优势"></a> 2. 关键优势</h3>
<ul>
<li>提升复杂推理能力：在算术（如数学题 <code>GSM8K</code>）、常识（如 <code>CSQA</code>）和符号推理（如字母拼接游戏）任务上，思维链提示显著优于标准提示（<code>standard prompting</code>）。</li>
<li>在 <code>GSM8K</code> 数学题基准上，<code>PaLM 540B</code> 模型使用思维链提示后，准确率从 <code>17.9%</code> 提升至 <code>56.9%</code>，甚至超过微调的 <code>GPT-3</code> 模型。</li>
<li>在常识推理任务（如 <code>StrategyQA</code>）上，准确率从 <code>68.6%</code> 提升至 <code>77.8%</code>。</li>
<li>模型规模涌现特性：思维链推理是大型模型（约 <code>100B</code> 参数以上）的 “涌现能力” —— <strong>小模型无法生成逻辑思维链</strong>，但足够大的模型（如 <code>GPT-3 175B</code>、<code>PaLM 540B</code>）能自然学习此模式。</li>
<li>无需微调：仅需在提示中添加少量示例（如 <code>8</code> 个），即可激发模型能力，无需额外训练或数据标注。</li>
<li>可解释性与泛化性：思维链提供透明推理路径，便于调试；且适用于多种任务（数学、常识、符号等），甚至能泛化到更长序列。</li>
</ul>
<h3 id="3-实验验证"><a class="markdownIt-Anchor" href="#3-实验验证"></a> 3. 实验验证</h3>
<ul>
<li>任务覆盖：
<ul>
<li>算术推理：在 <code>GSM8K</code>、<code>SVAMP</code> 等数据集上，思维链提示将性能提升高达 <code>39%</code>（<code>PaLM 540B</code>）。</li>
<li>常识推理：在 <code>StrategyQA</code>、<code>Date Understanding</code> 等任务上，模型表现接近或超越人类水平。</li>
<li>符号推理：在硬币翻转（<code>coin flip</code>）和字母拼接（<code>last letter concatenation</code>）任务中，模型能处理未见过的长序列。</li>
</ul>
</li>
<li>鲁棒性：不同注释者编写的思维链示例均有效，且对示例顺序、数量变化不敏感。</li>
</ul>
<h3 id="4-局限性与启示"><a class="markdownIt-Anchor" href="#4-局限性与启示"></a> 4. 局限性与启示</h3>
<ul>
<li>模型规模依赖：思维链仅在大型模型（<code>≥100B</code> 参数）中有效，小模型生成逻辑混乱。</li>
<li>潜在错误：生成的推理路径可能不准确（如算术计算错误或语义误解），需外部验证（如添加计算器）。</li>
<li>应用意义：该方法拓展了提示技术的边界，证明大型模型能通过自然语言示例学习复杂推理，减少对标注数据的依赖。</li>
</ul>
<h3 id="论文核心贡献"><a class="markdownIt-Anchor" href="#论文核心贡献"></a> 论文核心贡献</h3>
<ul>
<li>思维链提示是一种低成本、高效的方法，通过模拟人类逐步推理过程，释放大型语言模型在复杂任务上的潜力。论文强调，这是 “模型规模涌现” 的典型例子——推理能力随模型增大而自然出现，为未来 <code>AI</code> 推理研究提供了新方向。</li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>一定要注意，这篇论文讨论的对象 <strong>不是 Reasoning 模型</strong>（这个论文出来的时候还没有 <code>Reasoning</code> 模型的概念），而是普通的 <code>LLM</code> 模型。</li>
<li>本质是一种通过 <code>prompt</code> 引导模型通过增加推理计算预算的方式，来提升模型的推理能力的方法。</li>
<li>依托于 <code>LLM</code> 恐怖的指令遵循和上下文学习能力。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/05/28/Large-Language-Diffusion-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/28/Large-Language-Diffusion-Models/" class="post-title-link" itemprop="url">Large Language Diffusion Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-28 22:19:52" itemprop="dateCreated datePublished" datetime="2025-05-28T22:19:52+08:00">2025-05-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/05/28/Large-Language-Diffusion-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/05/28/Large-Language-Diffusion-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.09992">https://arxiv.org/pdf/2502.09992</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-demo/">https://ml-gsai.github.io/LLaDA-demo/</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>LLaDA</code> 提出了一个新概念，叫 “扩散语言模型”，和主流的自回归语言模型 <code>predict next token</code> 的方式不同，<code>LLaDA</code> 使用类似 <code>Diffusion</code> 去噪的方法，一次性生成多个 <code>token</code>，通过多次生成，得到一个完整的生成文本。</li>
<li>但细看就会发现，<code>Diffusion</code> 就是一个彻头彻尾的噱头，和经典的热力学扩散过程没有鸡毛关系，<code>LLaDA</code> 本质就是一个大 <code>BERT</code> 模型，用完形填空的方式来生成文本（一次可以做多个完形填空），只是下图所示的每轮迭代的过程看起来有点像 <code>Diffusion</code> 的去噪（没关系硬蹭）。<br />
<img src="https://github.com/ML-GSAI/LLaDA/raw/main/imgs/example_gradio.gif" alt="llada" /></li>
</ul>
<blockquote>
<p>上图来自官方 <code>repo</code> 的 <code>README</code></p>
</blockquote>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="总体流程"><a class="markdownIt-Anchor" href="#总体流程"></a> 总体流程</h3>
<ul>
<li>虽然多少有点标题党，但这篇论文本身是值得一读的，将文本生成任务做了重新定义，确实可大幅提高生成速度。</li>
</ul>
<h4 id="模型架构"><a class="markdownIt-Anchor" href="#模型架构"></a> 模型架构</h4>
<ul>
<li>纯纯 <code>Transformer encoder</code> 架构，和 <code>BERT</code> 类似，双向注意力，模型参数规模达 <code>8B</code></li>
</ul>
<h4 id="训练过程"><a class="markdownIt-Anchor" href="#训练过程"></a> 训练过程</h4>
<ol>
<li>预训练
<ul>
<li>使用随机 <code>mask</code> 一定比例的 <code>token</code>，然后使用 <code>Transformer</code> 预测被 <code>mask</code> 的 <code>token</code>（完形填空）</li>
<li>损失函数：<code>mask</code> 部分的 <code>cross-entropy</code> 损失</li>
<li>数据规模：<code>2.3</code> 万亿 <code>token</code>，包含通用文本、代码和多语言数据</li>
</ul>
</li>
<li><code>SFT</code>
<ul>
<li>目标：使模型具备指令跟随能力</li>
<li>数据格式：成对数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo separator="true">,</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(p_\theta,r_\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">p_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是指令，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">r_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是响应</li>
<li>掩码策略：仅对响应部分掩码，保持指令完整</li>
<li>损失函数：仅对响应部分计算 <code>cross-entropy</code> 损失</li>
<li>数据规模：<code>450</code> 万对指令响应对，涵盖代码、数学和多轮对话</li>
</ul>
</li>
</ol>
<h4 id="推理与生成"><a class="markdownIt-Anchor" href="#推理与生成"></a> 推理与生成</h4>
<ul>
<li>过程：从全掩码的响应开始，逐步预测并更新掩码 <code>token</code>，直到生成完整响应</li>
<li>重掩码策略（预测之后 <code>mask</code> 一部分生成结果做二次生成）：
<ul>
<li>随机重掩码：基础策略，与扩散过程对齐</li>
<li>低置信度重掩码：优先掩码预测置信度低的 <code>token</code></li>
<li>半自回归策略（<code>SFT</code>后）：分块生成，块内并行预测以提高效率</li>
</ul>
</li>
<li>生成效果：支持多轮对话、多语言翻译和复杂推理任务</li>
</ul>
<h3 id="和自回归模型对比"><a class="markdownIt-Anchor" href="#和自回归模型对比"></a> 和自回归模型对比</h3>
<table>
<thead>
<tr>
<th style="text-align:center">特性</th>
<th style="text-align:center">自回归模型（如<code>GPT</code>）</th>
<th style="text-align:center"><code>LLaDA</code></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">生成顺序</td>
<td style="text-align:center">严格从左到右逐 <code>token</code> 生成</td>
<td style="text-align:center">并行预测 + 动态调整</td>
</tr>
<tr>
<td style="text-align:center">计算效率</td>
<td style="text-align:center">需串行预测多次</td>
<td style="text-align:center">仅需少量迭代（块级并行）</td>
</tr>
<tr>
<td style="text-align:center">错误修正能力</td>
<td style="text-align:center">无法修改已生成 <code>token</code></td>
<td style="text-align:center">通过重掩码可修正低置信度位置</td>
</tr>
<tr>
<td style="text-align:center">逆向推理支持</td>
<td style="text-align:center">受限于单向建模</td>
<td style="text-align:center">双向注意力机制支持逆向推理</td>
</tr>
</tbody>
</table>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>预测下一个词的大模型范式是否一定是最优的？可能未必。这篇论文就提出了一个不错的思路</li>
<li><code>make bert great again</code> <s>手动滑稽</s></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/05/11/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/11/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models/" class="post-title-link" itemprop="url">High-Resolution Image Synthesis with Latent Diffusion Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-11 17:00:32" itemprop="dateCreated datePublished" datetime="2025-05-11T17:00:32+08:00">2025-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Diffusion/" itemprop="url" rel="index"><span itemprop="name">Diffusion</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/05/11/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/05/11/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.10752">https://arxiv.org/pdf/2112.10752</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>LDM</code> 是 <code>stable diffusion</code> 系列的开山之作，让 <code>Diffusion Model</code> 在 <code>Image Synthesis</code> 领域大放异彩</li>
<li>传统的 <code>Diffusion Model</code> 有两大问题：
<ol>
<li>没办法控制生成内容，只能确保生成的内容和训练数据集风格比较类似</li>
<li>在像素尺度上做去噪，计算量大，导致只能生成较小分辨率的图，且很慢</li>
</ol>
</li>
<li><code>LDM</code> 解决了上述的两个问题：
<ol>
<li>通过编码器将 文本 / <code>mask</code> / <code>bbox</code> 等条件信息转成 <code>conditioning embedding</code>，再通过 <code>cross attention</code> 机制将条件信息和 <code>latent space</code> 中的噪声结合起来做去噪，让条件信息可引导图片生成</li>
<li>通过 <code>VAE</code> 将图片压缩到 <code>latent space</code> 中，再进行去噪，计算量小，速度快</li>
</ol>
</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="总体流程"><a class="markdownIt-Anchor" href="#总体流程"></a> 总体流程</h3>
<p><img src="https://s2.loli.net/2025/05/22/Vlkz7YnqIDxTWhH.png" alt="LDM.png" /></p>
<ol>
<li>生成隐空间下的随机噪声</li>
<li>将条件信息通过各自类型的编码器编码成 <code>conditioning embedding</code>，例如文本编码使用 <code>CLIP text encoder</code></li>
<li>将 <code>latent noise</code> 和 <code>conditioning embedding</code> 和 <code>timestep embedding</code> 输入到 <code>UNet</code> 中进行多轮迭代去噪（<code>50</code> step）</li>
<li>去噪后的 <code>latent</code> 通过 <code>VAE decoder</code> 解码成图片</li>
</ol>
<h3 id="conditioning-unet"><a class="markdownIt-Anchor" href="#conditioning-unet"></a> Conditioning UNet</h3>
<ul>
<li>通过 <strong>交叉注意力机制</strong> 将 <strong>文本条件</strong> 和 <strong>时序</strong> 通过 <code>UNet</code> 嵌入到 <code>Noised Latent</code> 中</li>
<li>具体来说：<strong>TimeEmbedding 直接和图像特征相加，TextEmbedding 和图像特征做 CrossAttention，（TextEmbedding 作为 KV，图像特征作为 Q）</strong></li>
<li><code>Conditioning UNet</code> 示意图，两次下采样 + 中间块 + 两次上采样</li>
</ul>
<pre class="mermaid">graph TD
    Input[Noised Latent: 32x32x4] --> DownBlock1[CrossAttnDownBlock2D]
    DownBlock1 --> DownBlock2[CrossAttnDownBlock2D]
    DownBlock2 --> MidBlock[UNetMidBlock2DCrossAttn]
    MidBlock --> UpBlock1[CrossAttnUpBlock2D]
    UpBlock1 --> UpBlock2[CrossAttnUpBlock2D]
    UpBlock2 --> Output[Denoised Latent: 32x32x4]
  
    TextEncoder[Text Encoder] -->|Text Embedding| DownBlock1
    TextEncoder -->|Text Embedding| DownBlock2
    TextEncoder -->|Text Embedding| MidBlock
    TextEncoder -->|Text Embedding| UpBlock1
    TextEncoder -->|Text Embedding| UpBlock2
  
    Time[Timestep] -->|Time Embedding| DownBlock1
    Time -->|Time Embedding| DownBlock2
    Time -->|Time Embedding| MidBlock
    Time -->|Time Embedding| UpBlock1
    Time -->|Time Embedding| UpBlock2</pre>
<ul>
<li><code>CrossAttnBlock2D</code> 结构示意</li>
</ul>
<pre class="mermaid">graph TD
    %% 输入节点
    Input[输入特征图 h_in] --> ResNet
    TimeEmb[时间嵌入 t_emb] --> MLP
    TextEmb[文本条件 y_text] --> ProjText
  
    %% 主干计算路径
    ResNet[ResNet块] --> Add
    MLP[MLP时间投影] --> Add
    Add[逐元素相加] --> GroupNorm
    GroupNorm[GroupNorm] --> Conv1
    Conv1[Conv2D 1x1] --> CrossAttn
  
    %% 交叉注意力分支
    ProjText[文本投影 W_k/W_v] --> CrossAttn
    Conv2[Conv2D 1x1] --> Merge
    CrossAttn[交叉注意力层] --> Merge
  
    %% 残差连接
    Input --> Conv2
    Merge[特征合并] --> LayerNorm
    LayerNorm[LayerNorm] --> Output[输出特征图 h_out]</pre>
<ul>
<li><code>DecoderAttentionBlock2D</code> 结构示意</li>
</ul>
<pre class="mermaid">graph TD
    X[("Input x<br>Shape: 1,512,32,32")] --> Norm["Normalize (GroupNorm)<br>Output: 1,512,32,32"]
  
    Norm --> Q["Q Conv2d(1x1)<br>Output: 1,512,32,32"]
    Norm --> K["K Conv2d(1x1)<br>Output: 1,512,32,32"]
    Norm --> V["V Conv2d(1x1)<br>Output: 1,512,32,32"]
  
    Q --> ReshapeQ["Reshape & Permute<br>1,512,32,32 → 1,1024,512"]
    K --> ReshapeK["Reshape<br>1,512,32,32 → 1,512,1024"]
  
    ReshapeQ --> MatmulQK["Matmul(Q,K)<br>1,1024,512 × 1,512,1024 → 1,1024,1024"]
    ReshapeK --> MatmulQK
  
    MatmulQK --> Scale["Scale (×1/√512)<br>1,1024,1024"]
    Scale --> Softmax["Softmax<br>1,1024,1024"]
  
    V --> ReshapeV["Reshape<br>1,512,32,32 → 1,512,1024"]
    Softmax --> PermuteSoftmax["Permute<br>1,1024,1024 → 1,1024,1024"]
  
    ReshapeV --> MatmulVW["Matmul(V, Softmax)<br>1,512,1024 × 1,1024,1024 → 1,512,1024"]
    PermuteSoftmax --> MatmulVW
  
    MatmulVW --> ReshapeOut["Reshape<br>1,512,1024 → 1,512,32,32"]
  
    ReshapeOut --> ProjOut["Proj_out Conv2d(1x1)<br>1,512,32,32"]
    ProjOut --> Add["Add (x + h_)<br>1,512,32,32"]
  
    X --> Add
    Add --> Output[("Final Output<br>1,512,32,32")]</pre>
<ul>
<li>下面附上 <code>Conditioning UNet block</code> 的实现代码，可以看出非常优雅：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_dim, context_dim=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.to_q = nn.Linear(in_dim, in_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_k = nn.Linear(</span><br><span class="line">            context_dim <span class="keyword">if</span> context_dim <span class="keyword">else</span> in_dim, in_dim, bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        self.to_v = nn.Linear(</span><br><span class="line">            context_dim <span class="keyword">if</span> context_dim <span class="keyword">else</span> in_dim, in_dim, bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        self.to_out = nn.Sequential(nn.Linear(in_dim, in_dim), nn.Dropout(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        q = self.to_q(x)</span><br><span class="line">        k = self.to_k(context <span class="keyword">if</span> context <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> x)</span><br><span class="line">        v = self.to_v(context <span class="keyword">if</span> context <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> x)</span><br><span class="line"></span><br><span class="line">        attn = torch.einsum(<span class="string">&quot;b i d, b j d -&gt; b i j&quot;</span>, q, k) * (x.shape[-<span class="number">1</span>] ** -<span class="number">0.5</span>)</span><br><span class="line">        attn = F.softmax(attn, dim=-<span class="number">1</span>)</span><br><span class="line">        out = torch.einsum(<span class="string">&quot;b i j, b j d -&gt; b i d&quot;</span>, attn, v)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GEGLU</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_dim, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.proj = nn.Linear(in_dim, hidden_dim * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x_proj = self.proj(x)</span><br><span class="line">        x1, x2 = x_proj.chunk(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x1 * F.gelu(x2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_dim, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            GEGLU(in_dim, hidden_dim), nn.Dropout(<span class="number">0.0</span>), nn.Linear(hidden_dim, in_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = nn.LayerNorm(dim, eps=<span class="number">1e-5</span>)</span><br><span class="line">        self.attn1 = Attention(dim)</span><br><span class="line">        self.norm2 = nn.LayerNorm(dim, eps=<span class="number">1e-5</span>)</span><br><span class="line">        self.attn2 = Attention(dim, context_dim=<span class="number">768</span>)</span><br><span class="line">        self.norm3 = nn.LayerNorm(dim, eps=<span class="number">1e-5</span>)</span><br><span class="line">        self.ff = FeedForward(dim, <span class="number">1280</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># Self attention</span></span><br><span class="line">        x = self.attn1(self.norm1(x)) + x</span><br><span class="line">        <span class="comment"># Cross attention</span></span><br><span class="line">        x = self.attn2(self.norm2(x), context=context) + x</span><br><span class="line">        <span class="comment"># Feed forward</span></span><br><span class="line">        x = self.ff(self.norm3(x)) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer2DModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.GroupNorm(<span class="number">32</span>, in_channels, eps=<span class="number">1e-6</span>, affine=<span class="literal">True</span>)</span><br><span class="line">        self.proj_in = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(in_channels)])</span><br><span class="line">        self.proj_out = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        b, c, h, w = x.shape</span><br><span class="line">        x_in = x</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.proj_in(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(b, h * w, c)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">            x = block(x, context)</span><br><span class="line"></span><br><span class="line">        x = x.reshape(b, h, w, c).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = self.proj_out(x)</span><br><span class="line">        <span class="keyword">return</span> x + x_in</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetBlock2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = nn.GroupNorm(<span class="number">32</span>, in_channels, eps=<span class="number">1e-5</span>, affine=<span class="literal">True</span>)</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.time_emb_proj = nn.Linear(<span class="number">1280</span>, in_channels)</span><br><span class="line">        self.norm2 = nn.GroupNorm(<span class="number">32</span>, in_channels, eps=<span class="number">1e-5</span>, affine=<span class="literal">True</span>)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.0</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.nonlinearity = nn.SiLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, time_emb=<span class="literal">None</span></span>):</span></span><br><span class="line">        h = x</span><br><span class="line">        h = self.norm1(h)</span><br><span class="line">        h = self.nonlinearity(h)</span><br><span class="line">        h = self.conv1(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> time_emb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            time_emb = self.nonlinearity(time_emb)</span><br><span class="line">            time_emb = self.time_emb_proj(time_emb)[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            h = h + time_emb</span><br><span class="line"></span><br><span class="line">        h = self.norm2(h)</span><br><span class="line">        h = self.nonlinearity(h)</span><br><span class="line">        h = self.dropout(h)</span><br><span class="line">        h = self.conv2(h)</span><br><span class="line">        <span class="keyword">return</span> h + x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Downsample2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttnDownBlock2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels=<span class="number">320</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attentions = nn.ModuleList(</span><br><span class="line">            [Transformer2DModel(in_channels) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line">        )</span><br><span class="line">        self.resnets = nn.ModuleList([ResnetBlock2D(in_channels) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">        self.downsamplers = nn.ModuleList([Downsample2D(in_channels)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span>, time_emb=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> attn, resnet <span class="keyword">in</span> <span class="built_in">zip</span>(self.attentions, self.resnets):</span><br><span class="line">            x = attn(x, context)</span><br><span class="line">            x = resnet(x, time_emb)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> downsampler <span class="keyword">in</span> self.downsamplers:</span><br><span class="line">            x = downsampler(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试代码</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    block = CrossAttnDownBlock2D(in_channels=<span class="number">320</span>)</span><br><span class="line">    x = torch.randn(<span class="number">1</span>, <span class="number">320</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">    context = torch.randn(<span class="number">1</span>, <span class="number">77</span>, <span class="number">768</span>)  <span class="comment"># 文本条件，77 个 token 组成的文本序列经过 CLIP 编码成向量 </span></span><br><span class="line">    time_emb = torch.randn(<span class="number">1</span>, <span class="number">1280</span>)  <span class="comment"># 时间嵌入，一个时间步（例如：961）变成一个 enbedding</span></span><br><span class="line"></span><br><span class="line">    output = block(x, context, time_emb)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;输入形状: <span class="subst">&#123;x.shape&#125;</span> -&gt; 输出形状: <span class="subst">&#123;output.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 预期输出: torch.Size([1, 320, 32, 32])</span></span><br></pre></td></tr></table></figure>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>论文思路无比清晰，且说服力很强，把很多领域的知识结合起来，真正把图像生成在实用性方面推到了一个新的高度</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/05/10/Denoising-Diffusion-Implicit-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/10/Denoising-Diffusion-Implicit-Models/" class="post-title-link" itemprop="url">Denoising Diffusion Implicit Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-10 10:17:21" itemprop="dateCreated datePublished" datetime="2025-05-10T10:17:21+08:00">2025-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Diffusion/" itemprop="url" rel="index"><span itemprop="name">Diffusion</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/05/10/Denoising-Diffusion-Implicit-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/05/10/Denoising-Diffusion-Implicit-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.02502">https://arxiv.org/pdf/2010.02502</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>这篇论文提出了去噪扩散隐式模型 <code>Denoising Diffusion Implicit Models (DDIM)</code> 模型，可以看作是对 <code>Denoising Diffusion Probabilistic Models (DDPM)</code> 模型的改进。</li>
<li><code>DDPM</code> 的采样过程是一个 <code>Markov</code> 过程，<code>Markov</code> 过程只有知道 <code>t</code> 时刻的状态才能计算第 <code>t-1</code> 时刻，而 <code>DDIM</code> 的采样过程是一个非 <code>Markov</code> 过程。</li>
<li><code>DDIM</code> 的优势是：
<ol>
<li>去噪速度更快。可以在采样（去噪）时使用更少的时间步数，从而加快采样速度，并且用 <code>DDPM</code> 训练的模型可以直接用于 <code>DDIM</code> 的采样，二者可以无缝衔接。</li>
<li>确定性。在 <code>DDIM</code> 中，给定一个模型和一个噪声图像和时间步数，可以确定性地生成一个图像（运行再多次也是同一个图）。在 <code>DDPM</code> 中，给定一个模型和一个噪声图像和时间步数，生成的图像是随机的（每次跑都不一样）。</li>
</ol>
</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<ul>
<li><code>DDIM</code> 的公式是从 <code>DDPM</code> 公式通过复杂推导得到的，推导过程比较复杂，这里不做详细介绍，重点讲二者逆向过程（去噪）公式区别和使用上的区别。</li>
</ul>
<h3 id="ddpm-逆向过程公式"><a class="markdownIt-Anchor" href="#ddpm-逆向过程公式"></a> DDPM 逆向过程公式</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>1</mn><msqrt><msub><mi>α</mi><mi>t</mi></msub></msqrt></mfrac><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><mfrac><msub><mi>β</mi><mi>t</mi></msub><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub></mrow></msqrt></mfrac><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{t-1}=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\cdot \epsilon_\theta(x_t,t))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3221600000000002em;vertical-align:-1.00072em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.72528em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.68528em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.31472em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.00072em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.30144em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.27778em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322200000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.79222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20777999999999996em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>其中：
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\epsilon_\theta(x_t,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 是模型预测的噪声，即 <code>model(x_t, t)</code></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t=1-\beta_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msub><mi>α</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\bar\alpha_t=\prod_{s=1}^t\alpha_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.71778em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.233166em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.933456em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><code>t</code> 取值是 <code>999, 998,..., 1, 0</code>，即从 <code>T-1</code> 到 <code>0</code> 逐步去噪</li>
</ul>
</li>
</ul>
<h3 id="ddim-逆向过程公式"><a class="markdownIt-Anchor" href="#ddim-逆向过程公式"></a> DDIM 逆向过程公式</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo stretchy="false">(</mo><mfrac><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><msqrt><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mo>+</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{t-1}=\sqrt {\bar\alpha_t-1}(\frac{x_t-\sqrt {\bar\alpha_t-1}\cdot\epsilon_\theta(x_t,t)}{\sqrt {\bar\alpha_t}})+\sqrt{1-\bar\alpha_{t-1}}\cdot\epsilon_\theta(x_t,t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.4413300000000002em;vertical-align:-0.9321100000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8810950000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.841095em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15890499999999996em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.50922em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.79389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.75389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24611000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322200000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.79222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20777999999999996em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9321100000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.2880705000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9519294999999999em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9119295000000003em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2880705000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>其中：
<ul>
<li>大多数符号含义都和 <code>DDPM</code> 一样</li>
<li>只有 <code>t</code> 取值是 <code>980, 960,..., 20, 0</code> 这种非连续的取值</li>
</ul>
</li>
</ul>
<h3 id="ddim-公式拆解"><a class="markdownIt-Anchor" href="#ddim-公式拆解"></a> <code>DDIM</code> 公式拆解：</h3>
<h4 id="1-预测原输入"><a class="markdownIt-Anchor" href="#1-预测原输入"></a> 1. 预测原输入</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><msub><mi>d</mi><mrow><mi>x</mi><mn>0</mn></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><msqrt><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">pred_{x0}=\frac{x_t-\sqrt {\bar\alpha_t-1}\cdot\epsilon_\theta(x_t,t)}{\sqrt{\bar\alpha_t}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.4413300000000002em;vertical-align:-0.9321100000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.50922em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.79389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.75389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24611000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322200000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.79222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20777999999999996em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9321100000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>通过当前噪声隐变量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和模型预测的噪声 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\epsilon_\theta(x_t,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 估计原始输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，即去噪</li>
</ul>
<h4 id="2-计算调整方向"><a class="markdownIt-Anchor" href="#2-计算调整方向"></a> 2. 计算调整方向</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>d</mi><mi>i</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mo>=</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">direction\_point=\sqrt{1-\bar\alpha_{t-1}}\cdot\epsilon_\theta(x_t,t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.2880705000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9519294999999999em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9119295000000003em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2880705000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>根据噪声预测结果，计算从当前时间步 <code>t</code> 到前一个时间步 <code>t-1</code> 的调整方向，这一方向结合了噪声预测和噪声调度参数，用于引导隐变量的更新。</li>
</ul>
<h4 id="3-更新隐变量"><a class="markdownIt-Anchor" href="#3-更新隐变量"></a> 3. 更新隐变量</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo>⋅</mo><mi>p</mi><mi>r</mi><mi>e</mi><msub><mi>d</mi><mrow><mi>x</mi><mn>0</mn></mrow></msub><mo>+</mo><mi>d</mi><mi>i</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">x_{t-1}=\sqrt {\bar\alpha_t-1}\cdot pred_{x0}+direction\_point
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.15890499999999996em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8810950000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.841095em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15890499999999996em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span></span></p>
<ul>
<li>将预测的原始输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><msub><mi>d</mi><mrow><mi>x</mi><mn>0</mn></mrow></msub></mrow><annotation encoding="application/x-tex">pred_{x0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 与调整方向结合，生成前一时刻的隐变量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>。此步骤通过线性组合逐步去噪，最终逼近目标数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li>
</ul>
<h3 id="ddpm-和-ddim-使用上的区别"><a class="markdownIt-Anchor" href="#ddpm-和-ddim-使用上的区别"></a> DDPM 和 DDIM 使用上的区别</h3>
<ul>
<li><code>DDPM</code> 去噪过程</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">model = UNet2DModel.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>).to(device)</span><br><span class="line">scheduler = DDPMScheduler.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>)</span><br><span class="line"><span class="comment"># Get precalculated alphas and alpha bars from the scheduler</span></span><br><span class="line">alphas = scheduler.alphas</span><br><span class="line">alphas_cumprod = scheduler.alphas_cumprod</span><br><span class="line"><span class="comment"># Initialize sample with static random noise</span></span><br><span class="line">sample = torch.load(<span class="string">&quot;random_noise.pt&quot;</span>).to(device)</span><br><span class="line"><span class="comment"># DDPM denoising loop</span></span><br><span class="line"><span class="comment"># scheduler.timesteps = [999, 998, ..., 1, 0]</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm.tqdm(scheduler.timesteps):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># Model prediction (noise residual)</span></span><br><span class="line">        residual = model(sample, t).sample</span><br><span class="line">        <span class="comment"># DDPM denoising formula</span></span><br><span class="line">        sample = (</span><br><span class="line">            sample - (<span class="number">1</span> - alphas[t]) / torch.sqrt(<span class="number">1</span> - alphas_cumprod[t]) * residual</span><br><span class="line">        ) / torch.sqrt(alphas[t])</span><br><span class="line">        <span class="comment"># Add random noise only for t &gt; 1</span></span><br><span class="line">        <span class="keyword">if</span> t &gt; <span class="number">1</span>:</span><br><span class="line">            noise = torch.randn_like(sample).to(device)</span><br><span class="line">            sample += torch.sqrt(<span class="number">1</span> - alphas[t]) * noise</span><br></pre></td></tr></table></figure>
<ul>
<li><code>DDIM</code> 去噪过程</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">model = UNet2DModel.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>).to(device)</span><br><span class="line">scheduler = DDIMScheduler.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>)</span><br><span class="line"><span class="comment"># set inference steps</span></span><br><span class="line">scheduler.set_timesteps(num_inference_steps=<span class="number">50</span>)</span><br><span class="line"><span class="comment"># Initialize sample with static random noise</span></span><br><span class="line">sample = torch.load(<span class="string">&quot;random_noise.pt&quot;</span>).to(device)</span><br><span class="line"><span class="comment"># DDIM denoising loop</span></span><br><span class="line"><span class="comment"># scheduler.timesteps = [980, 960,..., 20, 0]</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm.tqdm(scheduler.timesteps)):</span><br><span class="line">    <span class="comment"># 将时间步转换为LongTensor并确保在正确设备上</span></span><br><span class="line">    t = t.to(device).long()</span><br><span class="line">    <span class="comment"># 获取当前和上一步的alpha累积乘积</span></span><br><span class="line">    alpha_cumprod_t = scheduler.alphas_cumprod[t]</span><br><span class="line">    alpha_cumprod_prev = (</span><br><span class="line">        scheduler.alphas_cumprod[scheduler.timesteps[i + <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">if</span> i + <span class="number">1</span> &lt; <span class="built_in">len</span>(scheduler.timesteps)</span><br><span class="line">        <span class="keyword">else</span> torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 将alpha值转换到相同设备</span></span><br><span class="line">    alpha_cumprod_t = alpha_cumprod_t.to(device)</span><br><span class="line">    alpha_cumprod_prev = alpha_cumprod_prev.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 1. 预测噪声残差</span></span><br><span class="line">        residual = model(sample, t).sample</span><br><span class="line">        <span class="comment"># 2. 计算预测的原始图像x0（去噪后的图像）</span></span><br><span class="line">        pred_x0 = (sample - torch.sqrt(<span class="number">1.0</span> - alpha_cumprod_t) * residual) / torch.sqrt(</span><br><span class="line">            alpha_cumprod_t</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 3. 计算下一步的样本方向</span></span><br><span class="line">        direction_xt = torch.sqrt(<span class="number">1.0</span> - alpha_cumprod_prev) * residual</span><br><span class="line">        <span class="comment"># 4. 组合得到新的样本</span></span><br><span class="line">        sample = torch.sqrt(alpha_cumprod_prev) * pred_x0 + direction_xt</span><br></pre></td></tr></table></figure>
<h3 id="二者对比分析"><a class="markdownIt-Anchor" href="#二者对比分析"></a> 二者对比分析</h3>
<ol>
<li><code>DDIM</code> 只需要 <code>50</code> 次迭代就能生成高质量的图像，而 <code>DDPM</code> 需要 <code>1000</code> 次迭代。</li>
<li>生成的图像质量相似，<code>DDIM</code> 生成的图像质量略高。</li>
<li>上面的代码中加载的噪声图是静态的，<code>DDIM</code> 跑多次生成的图像是一样的，而 <code>DDPM</code> 跑多次生成的图像是不一样的。</li>
<li>二者去噪结果对比，左侧是 <code>DDIM</code>，右侧是 <code>DDPM</code>：<br />
<img src="https://s2.loli.net/2025/05/15/SCknWUVJcxiHYr1.png" alt="concat.png" /></li>
</ol>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li><code>DDIM</code> 解决了 <code>DDPM</code> 的两大痛点，算是一个很好的改进。</li>
<li>为后续的 <code>LDM</code> 等模型打下了基础。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/" class="post-title-link" itemprop="url">Qwen3 技术报告先导篇</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-05 21:40:44" itemprop="dateCreated datePublished" datetime="2025-05-05T21:40:44+08:00">2025-05-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>blog: <a target="_blank" rel="noopener" href="https://qwenlm.github.io/zh/blog/qwen3/">https://qwenlm.github.io/zh/blog/qwen3/</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>Qwen3</code> 系列模型四月二十九日正式发布，但到目前为止，还没发布技术报告，只有一篇官方博客，介绍了 <code>Qwen3</code> 的一些基本信息</li>
<li>本文围绕着官方博客介绍，结合实际使用情况，给出一些个人的理解</li>
</ul>
<h2 id="qwen3-系列模型"><a class="markdownIt-Anchor" href="#qwen3-系列模型"></a> Qwen3 系列模型</h2>
<ul>
<li>本次 <code>Qwen3</code> 主要发布了八个版本的模型，其中包含两个 <code>MOE</code> 模型和六个 <code>dense</code> 模型：
<ul>
<li><code>Qwen3-235B-A22B</code>：<code>MOE</code> 模型，<code>235B</code> 参数量，<code>22B</code> 激活参数量</li>
<li><code>Qwen3-30B-A3B</code>：<code>MOE</code> 模型，<code>30B</code> 参数量，<code>3B</code> 激活参数量</li>
<li><code>Qwen3-32B</code>：<code>dense</code> 模型，<code>32B</code> 参数量</li>
<li><code>Qwen3-14B</code>：<code>dense</code> 模型，<code>14B</code> 参数量</li>
<li><code>Qwen3-8B</code>：<code>dense</code> 模型，<code>8B</code> 参数量</li>
<li><code>Qwen3-4B</code>：<code>dense</code> 模型，<code>4B</code> 参数量</li>
<li><code>Qwen3-1.7B</code>：<code>dense</code> 模型，<code>1.7B</code> 参数量</li>
<li><code>Qwen3-0.6B</code>：<code>dense</code> 模型，<code>0.6B</code> 参数量</li>
</ul>
</li>
<li>还有对应的 <code>Base</code> 模型（只经过预训练）和 <code>fp8 Quantized</code> 模型（量化模型）</li>
<li><code>Qwen3</code> 相较于上一代 <code>Qwen2.5</code>，一个较大的技术进步是：<strong>统一了 Reasoning 和非 Reasoning 模式</strong>，即不再区分<strong>推理模型和非推理模型（或者叫思考模型和非思考模型）</strong>，而是一个模型可以通过 <code>prompt</code> 来选择推理模式和非推理模式</li>
</ul>
<h2 id="qwen3-模型的主要特性"><a class="markdownIt-Anchor" href="#qwen3-模型的主要特性"></a> Qwen3 模型的主要特性</h2>
<h3 id="预训练"><a class="markdownIt-Anchor" href="#预训练"></a> 预训练</h3>
<ul>
<li>相较于上一代 <code>Qwen2.5</code> 使用了 <code>18</code> 万亿个 <code>token</code> 做预训练，<code>Qwen3</code> 使用了 <code>36</code> 万亿个 <code>token</code> 做预训练（整整翻了一倍，这是多大的数据团队才能搞出来的，<s>太壕了</s>）</li>
<li>包含了 <code>119</code> 种语言和方言，有大量数据是合成数据和在 <code>PDF</code> 上识别的文本数据</li>
<li>预训练分成了三个阶段：
<ul>
<li>第一阶段：模型在超过 <code>30</code> 万亿个 <code>token</code> 上进行了预训练，上下文长度为 <code>4K token</code></li>
<li>第二阶段：通过增加知识密集型数据（如 <code>STEM</code>、编程和推理任务）的比例来改进数据集，在 <code>5</code> 万亿个 <code>token</code> 上进行了预训练</li>
<li>第三阶段：使用高质量的长上下文数据将上下文长度扩展到 <code>32K token</code></li>
</ul>
</li>
<li>预训练实际效果：官方的说法是 <code>Qwen3-1.7B/4B/8B/14B/32B-Base</code> 分别与 <code>Qwen2.5-3B/7B/14B/32B/72B-Base</code> 表现相当</li>
</ul>
<h3 id="后训练"><a class="markdownIt-Anchor" href="#后训练"></a> 后训练</h3>
<p><img src="https://s2.loli.net/2025/05/05/eEUjxX8bvydgNqt.png" alt="post-training.png" /></p>
<ul>
<li>简单概括：用 <code>SFT → RL → SFT+RL 混合 → RL</code> 四个阶段后训练 <code>Qwen3-235B-A22B</code> 和 <code>Qwen3-32B</code> 模型，然后蒸馏得到其他小尺寸模型（大 <code>MOE</code> 蒸小 <code>MOE</code>，大 <code>dense</code> 蒸小 <code>dense</code>）</li>
<li>四个阶段分别是：
<ul>
<li>长思维链冷启动 ：使用监督微调（<code>SFT</code>）训练模型生成初步的长思维链推理能力，作为初始阶段的基础。</li>
<li>长思维链强化学习 ：通过强化学习（<code>RL</code>）进一步提升模型在复杂推理任务中的表现，优化生成思维链的质量和连贯性。</li>
<li>思维模式融合 ：结合 <code>SFT</code> 和 <code>RL</code> 的混合策略，将不同思维模式（如逻辑推理、知识检索等）整合到统一框架中，增强模型的灵活性。</li>
<li>通用强化学习 ：以 <code>RL</code> 为主导，对模型进行全局优化，强化其在多样化任务中的通用性和鲁棒性。</li>
</ul>
</li>
</ul>
<h2 id="qwen3-使用"><a class="markdownIt-Anchor" href="#qwen3-使用"></a> Qwen3 使用</h2>
<ul>
<li>使用上和 <code>Qwen2.5</code> 没有太大区别，主要是增加了 <code>Reasoning</code> 模式的选择开关</li>
<li>这个开关也非常简便，大概有两种方式可以选择：
<ol>
<li>在最新版 <code>Transformer</code> 库中 <code>tokenizer.apply_chat_template</code> 中可以设置 <code>enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.</code></li>
<li>另外一种方式就是始终保持上面的开关常开，然后在 <code>prompt</code> 中添加 <code>/no_think</code> 来关闭推理模式。<code>/no_think</code> 可加到 <code>system content</code> 或 <code>user content</code> 结尾。</li>
</ol>
</li>
<li>在一些下游任务上做了 <code>SFT</code> 微调测试，发现 <code>Qwen3-1.7B</code> 经过 <code>SFT</code> 之后和 <code>Qwen2.5-3B-Instruct</code> 经过 <code>SFT</code> 之后的效果差不多，这一点和官方对预训练的说法一致</li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>在当今这个大模型结构同质化时代，预训练数据越多，模型的能力越强（同样效果下数据翻一倍模型尺寸可缩减一半！！），所以小厂没有这么多数据工程师，搞不到大量的高质量的数据，是很难在大模型上追赶大厂</li>
<li>到今天为止，<code>Llama</code> 系列官方模型都不能做到原生支持中文，和 <code>Qwen</code> 系列模型原生支持 <code>119</code> 种语言和方言相比，感觉非常小家子气，注定会被扫进历史的垃圾堆里</li>
<li><code>DeepSeek</code> 对 <code>Qwen</code> 的影响很大，比如：
<ul>
<li>从 <code>DeepSeek v3</code> 之后，<code>Qwen</code> 系列模型预训练和最终版的命名从：<code>Qwen2.5-7B/Qwen2.5-7B-Instruct</code> 变成了 <code>Qwen3-8B-Base/Qwen3-8B</code></li>
<li>广泛的使用了<strong>模型蒸馏</strong>，确实极大的提高了小尺寸模型的能力（之前 <code>Qwen2.5-0.5B-Instruct</code> 基本就是个答非所问的傻子，现如今的 <code>Qwen3-0.6B</code> 在不开推理模式的情况下也可以解决很多数学问题，非常强）</li>
<li>四段式后训练也和 <code>DeepSeek</code> 使用的后训练非常相似</li>
</ul>
</li>
<li>合并推理和非推理模型的做法，是今后大模型的趋势，有不少模型都在朝着这个方向发展</li>
<li>官方表示 <code>Qwen</code> 系列后续会向着 <code>Agent</code> 方向发展，铺垫了很久的 <code>MCP</code> 可能会产生新的 <code>Agent</code> 应用变革</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">CUDA 学习笔记 01 —— CUDA 基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-24 09:57:45" itemprop="dateCreated datePublished" datetime="2025-03-24T09:57:45+08:00">2025-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0-学习-cuda-的目的"><a class="markdownIt-Anchor" href="#0-学习-cuda-的目的"></a> 0. 学习 CUDA 的目的</h2>
<ul>
<li>作为一个算法工程师，平时接触 <code>HPC (High Performance Computing)</code> 的机会并不多，那为什么还要学习 <code>CUDA</code> 呢？</li>
<li>学习 <code>CUDA</code> 的目的不是为了用 <code>CUDA</code> 做模型加速，而是从 <code>CUDA</code> 角度理解目前较新的大模型设计理念，这些高性能模型是如何从原理上做到又快又好的。</li>
<li>例如火出圈的 <code>DeepSeek</code> 系列模型，在模型设计角度做了较多创新，并开源了部分 <code>CUDA</code> 代码，对于不了解 <code>CUDA</code> 的工程师，很难 get 到算法设计的优雅之处。</li>
<li>反观某家大模型基座公司，曾开源某个模型结构，论文中一通自夸，分析理论计算量有多低。但很多人实测发现速度并没有很快，究其原因，实际上是这家公司还用的小模型时代的旧思维，即：一个模型理论计算量低，那就是快。</li>
<li>大模型时代不了解硬件，不尊重硬件，在算法创新上不太可能走的远。</li>
</ul>
<h2 id="1-hello-world"><a class="markdownIt-Anchor" href="#1-hello-world"></a> 1. Hello World</h2>
<h3 id="cuda-代码"><a class="markdownIt-Anchor" href="#cuda-代码"></a> cuda 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="comment">// cuda 中 host 表示 cpu 端，device 表示 gpu 端</span></span><br><span class="line"><span class="comment">// __device__ 是设备函数的声明符号，表明该函数在 device 执行，且只能在 device</span></span><br><span class="line"><span class="comment">// 中调用</span></span><br><span class="line"><span class="function">__device__ <span class="keyword">const</span> <span class="keyword">char</span> *<span class="title">device_hello_world</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">&quot;GPU: Hello world!\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// __host__ 是主机函数的声明符号，表明该函数在 host 执行，且只能在 host 中调用</span></span><br><span class="line"><span class="function">__host__ <span class="keyword">const</span> <span class="keyword">char</span> *<span class="title">host_hello_world</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123; <span class="keyword">return</span> <span class="string">&quot;CPU: Hello world!\n&quot;</span>; &#125;</span><br><span class="line"><span class="comment">// __global__ 是核函数的声明符号，表明该函数在 device 执行，且只能在 host 中调用</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_world</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *str = <span class="built_in">device_hello_world</span>();</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, str);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, <span class="built_in">host_hello_world</span>());</span><br><span class="line">  <span class="comment">// &lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt; 是核函数的调用符号，表示启动 grid_dim 个 block，</span></span><br><span class="line">  <span class="comment">// 每个 block 有 block_dim 个线程</span></span><br><span class="line">  hello_world&lt;&lt;&lt;<span class="number">1</span>, <span class="number">10</span>&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cuda</code> 的三个函数声明符号：
<ul>
<li><code>__host__</code>：主机函数，表示该函数在 CPU 上执行，且只能在 CPU 中调用</li>
<li><code>__device__</code>：设备函数，表示该函数在 GPU 上执行，且只能在 GPU 中调用</li>
<li><code>__global__</code>：核函数，表示该函数在 GPU 上执行，且只能在 CPU 中调用</li>
</ul>
</li>
<li>其中 <code>__global__</code> 声明的函数类型被称为 <strong>核函数</strong>，是 <code>CUDA</code> 中最重要的函数类型
<ul>
<li>核函数通过 <code>&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;</code> 的方式调用，其中 <code>&lt;&lt;&lt;&gt;&gt;&gt;</code> 是 <code>cuda</code> 扩展关键字</li>
<li><code>grid_dim</code> 表示启动的 <code>block</code> 数量，<code>block_dim</code> 表示每个 <code>block</code> 中的线程数量</li>
<li><code>grid_dim</code> 和 <code>block_dim</code> 都是 <code>dim3</code> 类型的变量，表示三维数组，如果使用整形则模型 <code>y</code> 和 <code>z</code> 维度都为 1</li>
</ul>
</li>
</ul>
<h3 id="编译"><a class="markdownIt-Anchor" href="#编译"></a> 编译</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc hello_world.cu -g -o hello_world</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a class="markdownIt-Anchor" href="#运行"></a> 运行</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">./hello_world</span><br><span class="line"><span class="comment"># CPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br></pre></td></tr></table></figure>
<h2 id="2-dimension-测试"><a class="markdownIt-Anchor" href="#2-dimension-测试"></a> 2. dimension 测试</h2>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;threadIdx:(%d,%d,%d) blockIdx:(%d,%d,%d) blockDim:(%d,%d,%d)\</span></span><br><span class="line"><span class="string">  gridDim(%d,%d,%d)\n&quot;</span>,</span><br><span class="line">         threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y,</span><br><span class="line">         blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x, gridDim.y,</span><br><span class="line">         gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> nElem = <span class="number">6</span>;  <span class="comment">// number of elements</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>;  <span class="comment">// block size</span></span><br><span class="line">  <span class="keyword">int</span> nBlock = (nElem + block.x - <span class="number">1</span>) / block.x; <span class="comment">// number of blocks</span></span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nBlock)</span></span>;  <span class="comment">// grid size</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;grid.x %d grid.y %d grid.z %d\n&quot;</span>, grid.x, grid.y, grid.z);  </span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;block.x %d block.y %d block.z %d\n&quot;</span>, block.x, block.y, block.z);</span><br><span class="line">  checkIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>执行结果：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./check_dimension</span><br><span class="line"><span class="comment"># grid.x 2 grid.y 1 grid.z 1</span></span><br><span class="line"><span class="comment"># block.x 3 block.y 1 block.z 1</span></span><br><span class="line"><span class="comment"># threadIdx:(0,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(1,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(2,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(0,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(1,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(2,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br></pre></td></tr></table></figure>
<h2 id="3-cuda-向量加法"><a class="markdownIt-Anchor" href="#3-cuda-向量加法"></a> 3. CUDA 向量加法</h2>
<h3 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;freshman.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__host__ <span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">float</span> *res, <span class="keyword">const</span> <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">    res[i] = a[i] + b[i];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">float</span> *res, <span class="keyword">const</span> <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (i &lt; size) <span class="comment">// 线程索引越界检查</span></span><br><span class="line">    res[i] = a[i] + b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// set up device</span></span><br><span class="line">  <span class="built_in">initDevice</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// allocate host memory</span></span><br><span class="line">  <span class="keyword">int</span> nElem = <span class="number">1</span> &lt;&lt; <span class="number">24</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>, nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte = <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>) * nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="comment">// allocate device memory</span></span><br><span class="line">  <span class="keyword">float</span> *a_d, *b_d, *res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;a_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;b_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;res_d, nByte));</span><br><span class="line">  <span class="comment">// randomly initialize the input data</span></span><br><span class="line">  <span class="built_in">initialData</span>(a_h, nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h, nElem);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d, a_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d, b_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="comment">// set up execution configuration</span></span><br><span class="line">  <span class="comment">// 1. 计算最佳块大小</span></span><br><span class="line">  <span class="keyword">int</span> minGridSize, bestBlockSize;</span><br><span class="line">  <span class="built_in">cudaOccupancyMaxPotentialBlockSize</span>(&amp;minGridSize, &amp;bestBlockSize,</span><br><span class="line">                                     (<span class="keyword">void</span> *)sumArraysGPU,</span><br><span class="line">                                     <span class="number">0</span>, <span class="comment">// 动态共享内存大小</span></span><br><span class="line">                                     <span class="number">0</span>  <span class="comment">// 无块大小限制</span></span><br><span class="line">  );</span><br><span class="line">  <span class="comment">// 2. 设置网格和块维度</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(bestBlockSize)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nElem + bestBlockSize - <span class="number">1</span>) / bestBlockSize)</span></span>;</span><br><span class="line">  <span class="comment">// 3. 设备执行并统计耗时</span></span><br><span class="line">  <span class="keyword">double</span> iStart, iElaps;</span><br><span class="line">  iStart = <span class="built_in">cpuSecond</span>();</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(a_d, b_d, res_d, nElem);</span><br><span class="line">  iElaps = <span class="built_in">cpuSecond</span>() - iStart;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h, res_d, nByte, cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed %f sec\n&quot;</span>, grid.x,</span><br><span class="line">         block.x, iElaps);</span><br><span class="line">  <span class="comment">// 4. CPU执行并统计耗时</span></span><br><span class="line">  iStart = <span class="built_in">cpuSecond</span>();</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h, b_h, res_h, nElem);</span><br><span class="line">  iElaps = <span class="built_in">cpuSecond</span>() - iStart;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;CPU Time elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line">  <span class="comment">// 5. 检查结果</span></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h, res_from_gpu_h, nElem);</span><br><span class="line">  <span class="comment">// 6. 释放内存</span></span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaOccupancyMaxPotentialBlockSize</code> 函数用于计算最佳块大小</li>
<li><code>max thread per block</code> 是 <code>cuda</code> 中的一个限制，表示每个块中最多可以有多少个线程，一般为 <code>1024</code>，当超过这个限制时，<code>CHECK(cudaGetLastError());</code> 会报错</li>
</ul>
<h3 id="运行结果"><a class="markdownIt-Anchor" href="#运行结果"></a> 运行结果</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./sum_arrays</span><br><span class="line"><span class="comment"># Using device 0: NVIDIA GeForce RTX 4060 Laptop GPU</span></span><br><span class="line"><span class="comment"># Vector size:16777216</span></span><br><span class="line"><span class="comment"># Execution configuration&lt;&lt;&lt;21846,768&gt;&gt;&gt; Time elapsed 0.000030 sec</span></span><br><span class="line"><span class="comment"># CPU Time elapsed 0.076604 sec</span></span><br><span class="line"><span class="comment"># Check result success!</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可以看出，<code>cuda</code> 的执行时间远远小于 <code>cpu</code> 的执行时间，相差了 <code>2553</code> 倍</li>
</ul>
<h3 id="总体流程"><a class="markdownIt-Anchor" href="#总体流程"></a> 总体流程</h3>
<pre class="mermaid">graph TD
    A[Host 端申请内存] --> B[Host 端初始化输入数据]
    B --> C[Device 端申请内存]
    C --> D[拷贝 Host 输入数据到 Device 端]
    D --> E[Device 端执行核函数]
    E --> F[拷贝 Device 端输出数据到 Host 端]
    F --> G[Host 端检查结果]
    G --> H[释放 Host 端和 Device 端全部内存]
    B --> J[Host 端执行普通函数] --> G</pre>
<h3 id="细节分析"><a class="markdownIt-Anchor" href="#细节分析"></a> 细节分析</h3>
<h4 id="1-cuda-内存分配是怎么做的"><a class="markdownIt-Anchor" href="#1-cuda-内存分配是怎么做的"></a> 1. <code>cuda</code> 内存分配是怎么做的？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *a_d; <span class="comment">// 空指针</span></span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;a_d, nByte);  <span class="comment">// 将指针的地址转成二级指针（指针的指针）传入内存分配函数</span></span><br></pre></td></tr></table></figure>
<ul>
<li>这里的二级指针应用很巧妙，由于 <code>c++</code> 中的指针是值传递，所以如果是一级指针传入 <code>cudaMalloc</code> 函数时，指针 <code>a_d</code> 的值不会改变，因此只能将指针的地址转成二级指针传入内存分配函数</li>
</ul>
<h4 id="2-cuda-内存拷贝是怎么做的"><a class="markdownIt-Anchor" href="#2-cuda-内存拷贝是怎么做的"></a> 2. <code>cuda</code> 内存拷贝是怎么做的？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMemcpy</span>(a_d, a_h, nByte, cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaMemcpy</code> 函数的四个参数分别是：
<ul>
<li><code>dst</code>：目标地址</li>
<li><code>src</code>：源地址</li>
<li><code>size</code>：拷贝的字节数</li>
<li><code>kind</code>：拷贝的类型，属于 <code>cudaMemcpyKind</code> 枚举类型
<ul>
<li><code>cudaMemcpyHostToHost          =   0,      /**&lt; Host   -&gt; Host */</code></li>
<li><code>cudaMemcpyHostToDevice        =   1,      /**&lt; Host   -&gt; Device */</code></li>
<li><code>cudaMemcpyDeviceToHost        =   2,      /**&lt; Device -&gt; Host */</code></li>
<li><code>cudaMemcpyDeviceToDevice      =   3,      /**&lt; Device -&gt; Device */</code></li>
<li><code>cudaMemcpyDefault             =   4       /**&lt; Direction of the transfer is inferred from the pointer values. Requires unified virtual addressing */</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-cuda-核函数如何解析线程索引"><a class="markdownIt-Anchor" href="#3-cuda-核函数如何解析线程索引"></a> 3. <code>cuda</code> 核函数如何解析线程索引？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"><span class="keyword">if</span> (i &lt; size) <span class="comment">// 线程索引越界检查</span></span><br><span class="line">  res[i] = a[i] + b[i];</span><br></pre></td></tr></table></figure>
<ul>
<li><code>blockIdx</code>：表示当前块的索引</li>
<li><code>blockDim</code>：表示当前块的维度（每个块中的线程数）</li>
<li><code>threadIdx</code>：表示当前线程的索引</li>
<li>每个线程中计算两个标量的和</li>
<li>由于 <code>gridDim * blockDim</code> 可能大于 <code>size</code>，所以需要判断线程索引是否越界</li>
</ul>
<h4 id="4-如何计算最佳块大小"><a class="markdownIt-Anchor" href="#4-如何计算最佳块大小"></a> 4. 如何计算最佳块大小？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> minGridSize, bestBlockSize;</span><br><span class="line"><span class="built_in">cudaOccupancyMaxPotentialBlockSize</span>(&amp;minGridSize, &amp;bestBlockSize,</span><br><span class="line">                                     (<span class="keyword">void</span> *)sumArraysGPU,</span><br><span class="line">                                     <span class="number">0</span>, <span class="comment">// 动态共享内存大小</span></span><br><span class="line">                                     <span class="number">0</span>  <span class="comment">// 无块大小限制</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaOccupancyMaxPotentialBlockSize</code> 函数用于计算最佳块大小，五个参数分别是：
<ul>
<li><code>minGridSize</code>：最小网格大小变量地址</li>
<li><code>bestBlockSize</code>：最佳块大小变量地址</li>
<li><code>kernel</code>：核函数指针</li>
<li><code>dynamicSMemSize</code>：动态共享内存大小</li>
<li><code>blockSizeLimit</code>：块大小限制</li>
</ul>
</li>
<li>函数名就是函数地址，可强转为 <code>void *</code> 函数指针（也可以写成：<code>(void *)&amp;sumArraysGPU</code>）</li>
</ul>
<h2 id="4-cuda-编程模型"><a class="markdownIt-Anchor" href="#4-cuda-编程模型"></a> 4. CUDA 编程模型</h2>
<h3 id="线程块"><a class="markdownIt-Anchor" href="#线程块"></a> 线程块</h3>
<ul>
<li>线程块 <code>block</code> 是 <code>CUDA</code> 中的逻辑执行单元，是一个三维逻辑结构：
<ul>
<li><code>block.x</code>：表示块的 x 维度大小</li>
<li><code>block.y</code>：表示块的 y 维度大小</li>
<li><code>block.z</code>：表示块的 z 维度大小</li>
<li>其中 <code>block.x</code> 是最内层的循环，<code>block.y</code> 是第二层循环，<code>block.z</code> 是最外层的循环</li>
<li>用三维数组可以表示为：<code>tread[z][y][x]</code>，即 <code>tid = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y</code></li>
</ul>
</li>
</ul>
<h3 id="线程束"><a class="markdownIt-Anchor" href="#线程束"></a> 线程束</h3>
<ul>
<li>线程束 <code>warp</code> 是 <code>CUDA</code> 基本调度执行单元，一个 <code>warp</code> 由 <code>32</code> 个线程组成
<ul>
<li>一个 <code>warp</code> 中的线程在一个时钟周期内执行同一条指令（单指令多线程，<code>SIMT</code>）</li>
<li>一个 <code>warp</code> 中的线程可以共享指令指针和执行资源（如寄存器、缓存等）</li>
<li><code>Warp</code> 调度器（<code>warp scheduler</code>）负责将 <code>warp</code> 分配到物理执行单元上执行</li>
</ul>
</li>
<li>线程块会被划分为多个 <code>warp</code>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>a</mi><mi>r</mi><mi>p</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>B</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi><mo>=</mo><mi>c</mi><mi>e</mi><mi>i</mi><mi>l</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>T</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>B</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi></mrow><mn>32</mn></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">WarpsPerBlock=ceil(\frac{ThreadsPerBlock}{32})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight">h</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<h3 id="cuda-core"><a class="markdownIt-Anchor" href="#cuda-core"></a> CUDA core</h3>
<ul>
<li><code>CUDA core</code> 是 <code>CUDA</code> 物理执行单元，负责实际的计算任务</li>
<li>一个 <code>CUDA core</code> 一个时钟周期只能计算一个线程的指令</li>
</ul>
<h3 id="streammultiprocessor"><a class="markdownIt-Anchor" href="#streammultiprocessor"></a> StreamMultiprocessor</h3>
<ul>
<li><code>StreamMultiprocessor</code> 流式多处理器（简称 <code>SM</code>），负责执行 <code>CUDA</code> 线程块中的并行计算任务</li>
<li>每个 <code>GPU</code> 包含多个 <code>SM</code>，每个 <code>SM</code> 包含多个 <code>CUDA core</code>，例如：<code>RTX 4060</code> 有 <code>24</code> 个 <code>SM</code>，每个 <code>SM</code> 有 <code>128</code> 个 <code>CUDA core</code></li>
</ul>
<h2 id="5-reduce"><a class="markdownIt-Anchor" href="#5-reduce"></a> 5. Reduce</h2>
<ul>
<li>规约（<code>Reduce</code>）是 <code>CUDA</code> 编程中常见的操作，主要用于将多个数据元素规约为一个数据元素</li>
<li>规约操作通常是一个二元操作，例如：<code>sum</code>、<code>mul</code>、<code>max</code>、<code>min</code> 等，简单的规约可以合并成强大的算子，甚至可以说规约算子是神经网络的基础</li>
</ul>
<h3 id="规约求和"><a class="markdownIt-Anchor" href="#规约求和"></a> 规约求和</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CPU 规约求和</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">recursiveReduce</span><span class="params">(<span class="keyword">int</span> *data, <span class="keyword">int</span> <span class="keyword">const</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// terminate check</span></span><br><span class="line">  <span class="keyword">if</span> (size == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">  <span class="comment">// renew the stride</span></span><br><span class="line">  <span class="keyword">int</span> <span class="keyword">const</span> stride = size / <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">if</span> (size % <span class="number">2</span> == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stride; i++) &#123;</span><br><span class="line">      data[i] += data[i + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    data[<span class="number">0</span>] += data[size - <span class="number">1</span>];</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stride; i++) &#123;</span><br><span class="line">      data[i] += data[i + stride];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// call</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">recursiveReduce</span>(data, stride);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GPU 规约相邻求和</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceNeighbored</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// set thread ID</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="comment">// boundary check</span></span><br><span class="line">  <span class="keyword">if</span> (tid &gt;= n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// convert global data pointer to local point of this block</span></span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="comment">// in-place reduction in global memory</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((tid % (<span class="number">2</span> * stride)) == <span class="number">0</span>) &#123;</span><br><span class="line">      idata[tid] += idata[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// synchronize within block</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global mem</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GPU 规约相邻求和（简化版）</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceNeighboredLess</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="keyword">unsigned</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="comment">// convert global data pointer to the local point of this block</span></span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span> (idx &gt; n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// in-place reduction in global memory</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="comment">// convert tid into local array index</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">2</span> * stride * tid;</span><br><span class="line">    <span class="keyword">if</span> (index &lt; blockDim.x) &#123;</span><br><span class="line">      idata[index] += idata[index + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global men</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GPU 规约交错求和，主要是 stride 的计算方式不同</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceInterleaved</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="keyword">unsigned</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="comment">// convert global data pointer to the local point of this block</span></span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span> (idx &gt;= n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// in-place reduction in global memory</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = blockDim.x / <span class="number">2</span>; stride &gt; <span class="number">0</span>; stride &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; stride) &#123;</span><br><span class="line">      idata[tid] += idata[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global men</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="效率分析"><a class="markdownIt-Anchor" href="#效率分析"></a> 效率分析</h3>
<ul>
<li>本代码中使用了三种核函数实现方式做同一个规约操作，分别是：
<ul>
<li><code>reduceNeighbored</code>：相邻线程规约</li>
<li><code>reduceNeighboredLess</code>：相邻线程规约（简化版）</li>
<li><code>reduceInterleaved</code>：交错线程规约</li>
</ul>
</li>
<li>三者效率从高到低依次是：
<ul>
<li><code>reduceInterleaved</code> &gt; <code>reduceNeighboredLess</code> &gt; <code>reduceNeighbored</code></li>
</ul>
</li>
<li>三者的示意图分别如下：
<ul>
<li><code>reduceNeighbored</code>：相邻线程规约的实现<br />
<image src="https://s2.loli.net/2025/04/14/ThePxqG52r3YKCd.png" width=60%/></li>
<li><code>reduceNeighboredLess</code>：相邻线程规约的简化版实现（注意圆圈中的符号已和上图不一致）<br />
<image src="https://s2.loli.net/2025/04/14/9JTkPeCd5u1a7gF.png" width=60%/></li>
<li><code>reduceInterleaved</code>：交错线程规约的实现<br />
<image src="https://s2.loli.net/2025/04/14/dUcei9EZGsbgzRB.png" width=60%/></li>
</ul>
</li>
<li>三者效率差异主要来自于 <strong>线程分支分化</strong>，后续会详细介绍</li>
</ul>
<h2 id="6-循环展开"><a class="markdownIt-Anchor" href="#6-循环展开"></a> 6. 循环展开</h2>
<ul>
<li>循环展开（<code>Loop Unrolling</code>）是 <code>CUDA</code> 中常用的优化手段，主要用于减少循环控制开销和提高指令级并行度</li>
<li>简单说就是一个线程不再只计算一个数据，而是计算多个数据，而且是直接在代码中展开，而不是在编译器中展开</li>
<li>可以简单理解成：启动线程是需要花时间的，启动一个线程只算一个数据，太浪费了，所以我们可以让一个线程计算多个数据，这样就能减少启动线程的时间开销，所以就省时间了</li>
</ul>
<h3 id="代码-2"><a class="markdownIt-Anchor" href="#代码-2"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 数据总长是 8 * blockDim.x * gridDim.x</span></span><br><span class="line"><span class="comment">// 线程数是 blockDim.x * gridDim.x</span></span><br><span class="line"><span class="comment">// 每个线程计算 8 个数据</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceUnroll8</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> idx = blockDim.x * blockIdx.x * <span class="number">8</span> + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (tid &gt;= n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x * <span class="number">8</span>;</span><br><span class="line">  <span class="comment">// 循环展开，每个线程计算 8 个数据</span></span><br><span class="line">  <span class="comment">// 直接把 8 * blockDim.x * gridDim.x 的数据总长</span></span><br><span class="line">  <span class="comment">// 聚合到了 blockDim.x * gridDim.x 的线程数上</span></span><br><span class="line">  <span class="keyword">if</span> (idx + <span class="number">7</span> * blockDim.x &lt; n) &#123;</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">2</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">3</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">4</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">5</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">6</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">7</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 这里需要同步，也就是线程阻塞直到所有线程都执行完</span></span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="comment">// 然后就是一个最简单的规约操作了，和上面一样</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = blockDim.x / <span class="number">2</span>; stride &gt; <span class="number">0</span>; stride &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; stride) &#123;</span><br><span class="line">      idata[tid] += idata[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global mem</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-核函数递归调用"><a class="markdownIt-Anchor" href="#7-核函数递归调用"></a> 7. 核函数递归调用</h2>
<ul>
<li>和 <code>CPU</code> 一样，<code>CUDA</code> 也支持核函数递归调用，调用方式和普通递归函数一样</li>
<li>需要注意的是在编译的时候需要加上 <code>-rdc=true</code> 选项</li>
</ul>
<h3 id="代码-3"><a class="markdownIt-Anchor" href="#代码-3"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">nesthelloworld</span><span class="params">(<span class="keyword">int</span> iSize, <span class="keyword">int</span> iDepth)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;depth : %d blockIdx: %d,threadIdx: %d\n&quot;</span>, iDepth, blockIdx.x,</span><br><span class="line">         threadIdx.x);</span><br><span class="line">  <span class="keyword">if</span> (iSize == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">int</span> nthread = (iSize &gt;&gt; <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span> &amp;&amp; nthread &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 递归调用核函数</span></span><br><span class="line">    nesthelloworld&lt;&lt;&lt;<span class="number">1</span>, nthread&gt;&gt;&gt;(nthread, ++iDepth);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;-----------&gt; nested execution depth: %d\n&quot;</span>, iDepth);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="8-固定内存"><a class="markdownIt-Anchor" href="#8-固定内存"></a> 8. 固定内存</h2>
<ul>
<li><code>Pinned Memory</code> 是 <code>CUDA</code> 中的一种特殊内存类型（不是显存，是内存），主要用于提高数据传输效率</li>
<li>普通内存是分页管理，分页管理存在两个问题：
<ol>
<li>一页内存逻辑上连续，但物理上不连续</li>
<li>操作系统可能会将内存页交换到磁盘上，导致数据不在物理内存中</li>
</ol>
</li>
<li><code>Pinned Memory</code> 就是解决了这两个问题，分配了一块连续物理地址且固定的主机内存（<code>host</code> 内存），方便整块拷贝数据到显存（<code>DMA</code>）</li>
</ul>
<h3 id="代码-4"><a class="markdownIt-Anchor" href="#代码-4"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;freshman.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line">  <span class="keyword">int</span> nElem = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line">  <span class="keyword">int</span> nByte = <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>) * nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h, *b_h, *res_h, *res_from_gpu_h;</span><br><span class="line">  <span class="comment">// 注意这里的 cudaMallocHost 和 cudaMalloc 是不同的</span></span><br><span class="line">  <span class="comment">// 前者申请的是 host 固定内存，后者申请的是 device 显存</span></span><br><span class="line">  <span class="comment">// cudaMallocHost 是 malloc 的一个平替</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;a_h, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;b_h, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;res_h, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;res_from_gpu_h, nByte));</span><br><span class="line">  <span class="comment">// 初始化数据</span></span><br><span class="line">  <span class="built_in">memset</span>(res_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="built_in">initialData</span>(a_h, nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h, nElem);</span><br><span class="line">  <span class="comment">// 申请设备显存</span></span><br><span class="line">  <span class="keyword">float</span> *a_d, *b_d, *res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;a_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;b_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;res_d, nByte));</span><br><span class="line">  <span class="comment">// 拷贝数据到设备显存</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d, a_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d, b_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="comment">// 跑核函数</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem / block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(a_d, b_d, res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>, grid.x, block.x);</span><br><span class="line">  <span class="comment">// 结果拷贝回主机并检查两个设备计算结果是否一致</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h, res_d, nByte, cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h, b_h, res_h, nElem);</span><br><span class="line">  <span class="built_in">checkResult</span>(res_h, res_from_gpu_h, nElem);</span><br><span class="line">  <span class="comment">// 释放内存</span></span><br><span class="line">  <span class="comment">// 注意这里的 cudaFreeHost 和 cudaFree 是不同的</span></span><br><span class="line">  <span class="comment">// 前者释放的是 host 固定内存，后者释放的是 device 显存</span></span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(a_h);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(b_h);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(res_h);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(res_from_gpu_h);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="关键点"><a class="markdownIt-Anchor" href="#关键点"></a> 关键点</h3>
<ol>
<li><strong>主存</strong> <strong>普通内存</strong> 的分配和释放函数是 <code>malloc</code> 和 <code>free</code></li>
<li><strong>主存</strong> <strong>固定内存</strong> 的分配和释放函数是 <code>cudaMallocHost</code> 和 <code>cudaFreeHost</code></li>
<li><strong>显存</strong> 的分配和释放函数是 <code>cudaMalloc</code> 和 <code>cudaFree</code></li>
</ol>
<h2 id="9-零拷贝内存-和-统一虚拟地址"><a class="markdownIt-Anchor" href="#9-零拷贝内存-和-统一虚拟地址"></a> 9. 零拷贝内存 和 统一虚拟地址</h2>
<ul>
<li><code>Zero-Copy Memory</code> 是 <code>CUDA</code> 中一种允许 <code>GPU</code> 直接访问主机内存的技术，避免了显式的数据拷贝操作（不需要用 <code>cudaMemcpy</code> 函数）</li>
<li>实际上，<code>Zero-Copy Memory</code> 在很多时候并不快，因为 <code>GPU</code> 访问主机内存的速度远远低于访问显存的速度，因此，<code>Zero-Copy Memory</code> 只适用于一些特殊的场景，例如：
<ul>
<li>主机内存中的数据只需要被 <code>GPU</code> 使用一次</li>
<li>数据量太大，显存放不下</li>
<li>调试用途</li>
</ul>
</li>
<li><code>Zero-Copy Memory</code> 的实现方式是将主机内存映射到 <code>GPU</code> 的地址空间中，<code>GPU</code> 通过访问这个地址空间来访问主机内存，实际上走的是 <code>PCIe</code> 总线</li>
<li>由于不需要先完成所有数据的拷贝再开始执行核函数，因此 <code>Zero-Copy Memory</code> 使用异步拷贝的方式来实现，可将部分拷贝数据的时间和核函数执行的时间重叠，但并不多</li>
<li><code>Unified Virtual Addressing (UVA)</code> 是 <code>CUDA</code> 中的一种内存管理机制，允许 <code>CPU</code> 和 <code>GPU</code> 共享同一虚拟地址空间</li>
</ul>
<h3 id="代码-5"><a class="markdownIt-Anchor" href="#代码-5"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *a_host, *b_host, *res_d;</span><br><span class="line"><span class="comment">// 申请主机固定内存，添加特殊 flag cudaHostAllocMapped</span></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;a_host, nByte, cudaHostAllocMapped));</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;b_host, nByte, cudaHostAllocMapped));</span><br><span class="line"><span class="comment">// a_host 和 b_host 是可直接作为核函数的输入参数</span></span><br><span class="line"><span class="comment">// 也可以转成 Device 地址空间，如下：</span></span><br><span class="line"><span class="keyword">float</span> *a_dev, *b_dev;</span><br><span class="line"><span class="comment">// 映射主机内存到设备地址空间</span></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaHostGetDevicePointer</span>((<span class="keyword">void</span> **)&amp;a_dev, (<span class="keyword">void</span> *)a_host, <span class="number">0</span>));</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaHostGetDevicePointer</span>((<span class="keyword">void</span> **)&amp;b_dev, (<span class="keyword">void</span> *)b_host, <span class="number">0</span>));</span><br><span class="line"><span class="comment">// 用 a_dev 和 b_dev 作为核函数的输入参数计算</span></span><br></pre></td></tr></table></figure>
<ul>
<li>和 <code>Zero-Copy Memory</code> 一样，<code>UVA</code> 甚至不需要将 <code>a_host</code> 转成 <code>a_dev</code>，直接用 <code>a_host</code> 就可以调用核函数</li>
</ul>
<h2 id="10-aos-和-soa"><a class="markdownIt-Anchor" href="#10-aos-和-soa"></a> 10. Aos 和 SoA</h2>
<ul>
<li><code>CUDA</code> 不仅支持最简单的原生数据类型，还支持自定义数据类型（<code>struct</code>），例如 <code>Aos</code> 和 <code>SoA</code> 等</li>
<li><code>Aos</code>（Array of Structures）和 <code>SoA</code>（Structure of Arrays）是两种不同的数据存储方式，这两种方式由于变量的排布方式不同，导致了访问内存的效率差异</li>
</ul>
<h3 id="aos"><a class="markdownIt-Anchor" href="#aos"></a> Aos</h3>
<ul>
<li><code>Aos</code> 是将多个结构体存储在一个数组中，每个结构体的成员变量是连续存储的</li>
<li>例如：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">AoSStruct</span> &#123;</span></span><br><span class="line">  <span class="keyword">float</span> a;</span><br><span class="line">  <span class="keyword">float</span> b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, struct naiveStruct *res, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (i &lt; n)</span><br><span class="line">    res[i].a = a[i] + b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>这里的 <code>res</code> 是一个 <strong>结构体数组</strong>，<code>a</code> 和 <code>b</code> 是结构体的成员变量，每个变量都是一个 <code>float</code> 类型的<strong>标量</strong></li>
<li>每个结构体的成员变量是连续存储的，即：<code>a1 b1 a2 b2 a3 b3 ...</code></li>
</ul>
<h3 id="soa"><a class="markdownIt-Anchor" href="#soa"></a> SoA</h3>
<ul>
<li><code>SoA</code> 是将多个结构体的成员变量存储在一个数组中，每个成员变量是连续存储的</li>
<li>例如：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SoAStruct</span> &#123;</span></span><br><span class="line">  <span class="keyword">float</span> a[SIZE];</span><br><span class="line">  <span class="keyword">float</span> b[SIZE];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, struct SoAStruct *res, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (i &lt; n)</span><br><span class="line">    res-&gt;a[i] = a[i] + b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>这里的 <code>res</code> 是一个 <strong>结构体</strong>，<code>a</code> 和 <code>b</code> 是结构体的成员变量，每个变量都是一个 <code>float</code> 类型的<strong>数组</strong></li>
<li>每个成员变量的数组是连续存储的，即：<code>a1 a2 a3 ... b1 b2 b3 ...</code></li>
</ul>
<h2 id="11-行主序和列主序"><a class="markdownIt-Anchor" href="#11-行主序和列主序"></a> 11. 行主序和列主序</h2>
<ul>
<li><code>行主序</code>（Row Major Order）和 <code>列主序</code>（Column Major Order）是两种不同的数组存储方式</li>
<li><code>行主序</code> 是将数组的每一行存储在连续的内存中，<code>列主序</code> 是将数组的每一列存储在连续的内存中</li>
<li>例如：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">3</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">    &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;,</span><br><span class="line">    &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;,</span><br><span class="line">    &#123;<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>&#125;&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>行主序</code> 存储方式是：<code>1 2 3 4 5 6 7 8 9 10 11 12</code></li>
<li><code>列主序</code> 存储方式是：<code>1 5 9 2 6 10 3 7 11 4 8 12</code></li>
<li>默认情况下，<code>C / C++ / CUDA</code> 语言是 <code>行主序</code> 存储方式</li>
<li>在行主序存储下，如果按行序访问数组元素，访问效率会更高，因为连续的内存访问会提高缓存命中率，反之如果按列序访问数组元素，访问效率会更低</li>
</ul>
<h3 id="速度对比"><a class="markdownIt-Anchor" href="#速度对比"></a> 速度对比</h3>
<p>WIP</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/17/Transformers-without-Normalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/17/Transformers-without-Normalization/" class="post-title-link" itemprop="url">Transformers without Normalization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-17 13:16:18" itemprop="dateCreated datePublished" datetime="2025-03-17T13:16:18+08:00">2025-03-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/17/Transformers-without-Normalization/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/17/Transformers-without-Normalization/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.10622">https://arxiv.org/pdf/2503.10622</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/jiachenzhu/DyT">https://github.com/jiachenzhu/DyT</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>这是由恺明和杨立昆提出的一篇关于 <code>transformer</code> 算子优化的论文，主要观点是去掉 <code>transformer</code> 结构中的 <code>normalization</code> 层，改成 <code>tanh</code> 层</li>
<li>改用 <code>tanh</code> 算子的 <code>transformer</code> 模型，在大多数任务上可达到使用归一化层的模型相同的性能，甚至更好</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<p><img src="https://s2.loli.net/2025/03/18/hYMbytL1A3ZU8nO.png" alt="dyt.png" /></p>
<ul>
<li>简单来说，这篇论文的核心思想是将 <code>transformer</code> 中的 <code>normalization</code> 层（可以是 <code>LayerNorm</code> 或 <code>RMSNorm</code>）替换成 <code>dynamic tanh</code> 层（简称 <code>DyT</code>）</li>
<li><code>normalization</code> 计算公式：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>normalization</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mo>×</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\text{normalization}(x) = \gamma \times \frac{x - \mu}{\sqrt{\sigma^2+\epsilon}} + \beta
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">normalization</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.1903300000000003em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.196611em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.913389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.873389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.12661100000000003em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span></p>
<blockquote>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">μ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 分别是 <code>mean</code> 和 <code>std</code>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是 <code>scale</code> 和 <code>shift</code> 参数</p>
</blockquote>
<ul>
<li><code>DyT</code> 计算公式：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>DyT</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mo>×</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\text{DyT}(x) = \gamma \times \tanh(\alpha x) + \beta
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">DyT</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span></p>
<blockquote>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 是个可学习参数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是 <code>scale</code> 和 <code>shift</code> 参数（和 <code>normalization</code> 一样）</p>
</blockquote>
<ul>
<li><code>DyT</code> 实现伪代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input x has the shape of [B, T, C]</span></span><br><span class="line"><span class="comment"># B: batch size, T: tokens, C: dimension</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DyT</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, C, init_α</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.α = Parameter(ones(<span class="number">1</span>) * init_α)</span><br><span class="line">        self.γ = Parameter(ones(C))</span><br><span class="line">        self.β = Parameter(zeros(C))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = tanh(self.alpha * x)</span><br><span class="line">        <span class="keyword">return</span> self.γ * x + self.β</span><br></pre></td></tr></table></figure>
<blockquote>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 默认初始化值为 <code>0.5</code></p>
</blockquote>
<h2 id="results"><a class="markdownIt-Anchor" href="#results"></a> Results</h2>
<ul>
<li>作者在多个领域的知名模型上都对比了修改前后训练精度，<code>DyT</code> 的性能和 <code>normalization</code> 的性能基本一致，打的有来有回<br />
<img src="https://s2.loli.net/2025/03/18/DGAE6nMyocKstev.png" alt="dyt2.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Ul9eYLW6zrM43qv.png" alt="dyt3.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Bf1q5NszOU96Pbv.png" alt="dyt4.png" /><br />
<img src="https://s2.loli.net/2025/03/18/lQyRxsAoWcdIbHt.png" alt="dyt5.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Xn1SF2praD5HwRe.png" alt="dyt6.png" /><br />
<img src="https://s2.loli.net/2025/03/18/OleUF7P9XQJKxy4.png" alt="dyt1.png" /></li>
<li>作者还对比了 <code>DyT</code> 和 <code>normalization</code> 的训练/推理速度，<code>DyT</code> 的训练/推理速度要快很多<br />
<img src="https://s2.loli.net/2025/03/18/lVu9axTNIhgjpKB.png" alt="dyt7.png" /></li>
<li>作者同时做 <code>tanh</code> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 做了消融实验，发现 <code>tanh</code> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 都是必要的<br />
<img src="https://s2.loli.net/2025/03/18/v3zIDnFKW6OwV4Z.png" alt="dyt8.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Y6dWnqjsD3PZANt.png" alt="dyt9.png" /></li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>属于是恺明和立昆的梦幻联动了…，这种对最火的结构的优化，非大佬不能为也，想象下如果这篇论文是大学实验室发表的，大家第一反应恐怕是：Who think you are? 😂</li>
<li>之前算是稍微接触过硬件，<code>DyT</code> 这种 <code>element-wise op</code> 比 <code>normalization</code> 这种 <code>reduce op</code> 一定快多了，想怎么 <code>tiling</code> 都行…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第五弹 —— 3FS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-14 11:09:25" itemprop="dateCreated datePublished" datetime="2025-03-14T11:09:25+08:00">2025-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/file-system/" itemprop="url" rel="index"><span itemprop="name">file-system</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/3FS">https://github.com/deepseek-ai/3FS</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>3FS (Fire-Flyer File System)</code> 是一个高性能的分布式文件系统，旨在提供低延迟和高吞吐量的存储解决方案，利用现代 <code>SSD</code> 和 <code>RDMA</code> 网络带全宽的并行文件系统，解决 <code>AI</code> 训练和推理存储问题</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="效果"><a class="markdownIt-Anchor" href="#效果"></a> 效果</h3>
<ul>
<li><strong>集群高吞吐</strong>：在 <code>180</code> 节点集群中，<code>3FS</code> 实现了高达 <code>6.6 TiB/s</code> 的聚合读取吞吐量</li>
<li><strong>基准测试优异</strong>：在 <code>25</code> 节点集群的 <code>GraySort</code> 基准测试中，<code>3FS</code> 达到了 <code>3.66 TiB /min</code> 的吞吐量</li>
<li><strong>单节点高性能</strong>：每个客户端节点的 <code>KVCache</code> 查找峰值吞吐量超过 <code>40 GiB/s</code></li>
<li><strong>架构先进</strong>： <code>3FS</code> 采用去中心化架构，并具备强一致性语义</li>
</ul>
<h3 id="系统介绍"><a class="markdownIt-Anchor" href="#系统介绍"></a> 系统介绍</h3>
<ul>
<li>还是看大佬讲吧，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27317616324">传送门</a></li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>这种底层架构一般只有大厂可以做，<code>deepseek</code> 有点东西</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第四弹 —— DualPipe ＆ EPLB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-12 11:35:53" itemprop="dateCreated datePublished" datetime="2025-03-12T11:35:53+08:00">2025-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>DualPipe code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DualPipe">https://github.com/deepseek-ai/DualPipe</a></li>
<li>EPLB code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/EPLB">https://github.com/deepseek-ai/EPLB</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>DualPipe</code> 是 <code>deepseek</code> 提出的一种 <strong>流水线并行算法</strong>，和之间读过的 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/">GPipe</a> 和 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/">PipeDream</a> 类似，但 <code>DualPipe</code> 的硬件利用率更高，空泡更少</li>
<li><code>DPLB (Expert Parallelism Load Balancer)</code> 是一种 <strong>专家并行负载均衡算法</strong></li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="dualpipe"><a class="markdownIt-Anchor" href="#dualpipe"></a> DualPipe</h3>
<ul>
<li><code>DualPipe</code> 的计算过程<br />
<img src="https://s2.loli.net/2025/05/29/8kwaE7BPzYmGJiK.png" alt="dualpipe.png" /></li>
<li><code>DualPipe-v</code> 的计算过程<br />
<img src="https://s2.loli.net/2025/05/29/lK83UM7YyWsZbDO.png" alt="dualpipev.png" /></li>
</ul>
<blockquote>
<p>上图是民间自己二创的 dualpipe-v 的计算过程，将 dualpipe 对半切开后效果更好，<a target="_blank" rel="noopener" href="https://hackmd.io/@ufotalent/r1lVXsa9Jg">博客地址</a></p>
</blockquote>
<h4 id="dualpipe-核心特点"><a class="markdownIt-Anchor" href="#dualpipe-核心特点"></a> <code>DualPipe</code> 核心特点</h4>
<ol>
<li><strong>计算与通信重叠</strong>：<code>DualPipe</code> 的设计目标是最大化集群设备的计算性能，通过在<code>Forward</code> 和 <code>Backward</code> 阶段实现计算与通信的完全重叠，显著减少传统流水线并行中的 “空泡”（<code>Pipeline Bubble</code>，即空闲等待时间）。这对于需要跨节点协作的专家并行（<code>Expert Parallelism</code>）场景尤为重要。</li>
<li><strong>双向调度</strong>：与传统的单向流水线并行不同，<code>DualPipe</code> 采用双向调度策略，从流水线的两端同时输入微批次（<code>Micro-batches</code>），充分利用硬件资源。这种方法在保持计算通信比例恒定的情况下，即使模型规模进一步扩大，也能维持接近零的通信开销。</li>
<li><strong>高效扩展性</strong>：<code>DualPipe</code> 针对跨节点的混合专家模型（<code>MoE</code>）进行了优化，通过减少通信瓶颈，使得大规模分布式训练能够在相对有限的硬件资源（如 <code>H800 GPU</code>）上高效运行。</li>
<li><strong>显存优化</strong>：<code>DualPipe</code> 将模型的最浅层（包括嵌入层）和最深层（包括输出层）部署在同一流水线级别（<code>PP Rank</code>），实现参数和梯度的物理共享，进一步提升内存效率。这种设计减少了高代价的张量并行（<code>Tensor Parallelism</code>）需求。</li>
</ol>
<h3 id="eplb"><a class="markdownIt-Anchor" href="#eplb"></a> EPLB</h3>
<ul>
<li><code>EPLB (Expert Parallelism Load Balancer)</code> 是一种专家并行负载均衡算法，旨在解决专家并行中的负载不均问题</li>
<li>很简单，仅有 <code>160</code> 行 <code>python</code> 代码</li>
<li>核心思想是预估每个专家的负载，并根据负载设置专家拷贝和放置计划</li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li><code>DualPipe</code> 实际上是 <code>deepseek</code> 根据 <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/profile-data"><code>profile-data</code></a> 分析空泡后做的一种流水线并行算法，而且用其强大的工程能力实现了 <code>SMs</code> 通信耗时降低（实际上就是用 <code>PTX</code> 编程把一部分 <code>SM</code> 当做是全职的数据搬运工），这太 <s>crazy</s> 了</li>
<li><code>Transformer</code> 是一种重 <code>IO</code> 轻计算的架构，在 <code>Hopper</code> 硬件架构上，不改变 <code>SM</code> 是不可能做到通信和计算完全重叠的，所以 <code>deepseek</code> 做了非常底层的优化</li>
<li><code>EPLB</code> 作为一种专家并行负载均衡算法，虽然简单，但在实际应用中可以显著提升专家并行的效率</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第三弹 —— DeepGEMM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-10 11:51:21" itemprop="dateCreated datePublished" datetime="2025-03-10T11:51:21+08:00">2025-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepGEMM">https://github.com/deepseek-ai/DeepGEMM</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>DeepGEMM</code> 是一个简单但功能强大的 <code>Hopper GPU （H100/H800）</code> 矩阵计算加速库</li>
<li>包含大约 <code>300</code> 行核心代码，可以做到在绝大多数大小的矩阵乘法均优于专家调优的内核，<code>hopper GPU</code> 上最高可达 <code>1350+ FP8 TFLOPS</code></li>
<li>完全即时编译，没有过多依赖，就像教程一样简洁，支持 <code>dense</code> 和 <code>moe</code> 架构</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<ul>
<li>这 <code>HPC</code> 相关的内容对于我确实超纲了，<code>CPU</code> 快给我干烧了</li>
<li>还是看大佬的讲解吧，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26437292382">传送门走你</a></li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li><code>deepseek</code> 牛逼，为 <code>LLM</code> 平权做了不可磨灭的贡献</li>
<li>而且如此技术信仰，是算法工程师应有的样子，打 call</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhangzhe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">175</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">106</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhangzhe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js', () => {
    // 初始化 Mermaid 配置
    mermaid.initialize({
      theme    : 'dark',  // 设置主题
      logLevel : 3,  // 设置日志等级
      flowchart: { curve: 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 },
      themeVariables: {
        'fontFamily': 'Microsoft YaHei, Arial, sans-serif',  // 设置中文字体
      }
    });

    // 初始化 Mermaid 图表
    mermaid.init(undefined, document.querySelectorAll('pre.mermaid'));
  }, window.mermaid);
}
</script>


  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">


  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'eK3W25jybCO5jVUrYBBpAPqM-gzGzoHsz',
      appKey     : 'F4KVyUj9wHI5c80Bhz7O2uhq',
      placeholder: "说点什么再走吧...",
      avatar     : 'hide',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
