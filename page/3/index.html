<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"real-zhangzhe.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhangzhe&#39;s Blog">
<meta property="og:url" content="https://real-zhangzhe.github.io/page/3/index.html">
<meta property="og:site_name" content="Zhangzhe&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhangzhe">
<meta property="article:tag" content="No mistakes in the tango, not like life.">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://real-zhangzhe.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Zhangzhe's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhangzhe's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The projection of my life.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/archives/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/05/10/Denoising-Diffusion-Implicit-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/10/Denoising-Diffusion-Implicit-Models/" class="post-title-link" itemprop="url">Denoising Diffusion Implicit Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-10 10:17:21" itemprop="dateCreated datePublished" datetime="2025-05-10T10:17:21+08:00">2025-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Diffusion/" itemprop="url" rel="index"><span itemprop="name">Diffusion</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/05/10/Denoising-Diffusion-Implicit-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/05/10/Denoising-Diffusion-Implicit-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.02502">https://arxiv.org/pdf/2010.02502</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>这篇论文提出了去噪扩散隐式模型 <code>Denoising Diffusion Implicit Models (DDIM)</code> 模型，可以看作是对 <code>Denoising Diffusion Probabilistic Models (DDPM)</code> 模型的改进。</li>
<li><code>DDPM</code> 的采样过程是一个 <code>Markov</code> 过程，<code>Markov</code> 过程只有知道 <code>t</code> 时刻的状态才能计算第 <code>t-1</code> 时刻，而 <code>DDIM</code> 的采样过程是一个非 <code>Markov</code> 过程。</li>
<li><code>DDIM</code> 的优势是：
<ol>
<li>去噪速度更快。可以在采样（去噪）时使用更少的时间步数，从而加快采样速度，并且用 <code>DDPM</code> 训练的模型可以直接用于 <code>DDIM</code> 的采样，二者可以无缝衔接。</li>
<li>确定性。在 <code>DDIM</code> 中，给定一个模型和一个噪声图像和时间步数，可以确定性地生成一个图像（运行再多次也是同一个图）。在 <code>DDPM</code> 中，给定一个模型和一个噪声图像和时间步数，生成的图像是随机的（每次跑都不一样）。</li>
</ol>
</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<ul>
<li><code>DDIM</code> 的公式是从 <code>DDPM</code> 公式通过复杂推导得到的，推导过程比较复杂，这里不做详细介绍，重点讲二者逆向过程（去噪）公式区别和使用上的区别。</li>
</ul>
<h3 id="ddpm-逆向过程公式"><a class="markdownIt-Anchor" href="#ddpm-逆向过程公式"></a> DDPM 逆向过程公式</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>1</mn><msqrt><msub><mi>α</mi><mi>t</mi></msub></msqrt></mfrac><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><mfrac><msub><mi>β</mi><mi>t</mi></msub><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub></mrow></msqrt></mfrac><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{t-1}=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\cdot \epsilon_\theta(x_t,t))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3221600000000002em;vertical-align:-1.00072em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.72528em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.68528em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.31472em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.00072em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.30144em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.27778em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322200000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.79222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20777999999999996em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>其中：
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\epsilon_\theta(x_t,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 是模型预测的噪声，即 <code>model(x_t, t)</code></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t=1-\beta_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msub><mi>α</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\bar\alpha_t=\prod_{s=1}^t\alpha_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.71778em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.233166em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.933456em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><code>t</code> 取值是 <code>999, 998,..., 1, 0</code>，即从 <code>T-1</code> 到 <code>0</code> 逐步去噪</li>
</ul>
</li>
</ul>
<h3 id="ddim-逆向过程公式"><a class="markdownIt-Anchor" href="#ddim-逆向过程公式"></a> DDIM 逆向过程公式</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo stretchy="false">(</mo><mfrac><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><msqrt><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mo>+</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{t-1}=\sqrt {\bar\alpha_t-1}(\frac{x_t-\sqrt {\bar\alpha_t-1}\cdot\epsilon_\theta(x_t,t)}{\sqrt {\bar\alpha_t}})+\sqrt{1-\bar\alpha_{t-1}}\cdot\epsilon_\theta(x_t,t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.4413300000000002em;vertical-align:-0.9321100000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8810950000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.841095em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15890499999999996em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.50922em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.79389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.75389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24611000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322200000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.79222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20777999999999996em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9321100000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.2880705000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9519294999999999em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9119295000000003em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2880705000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>其中：
<ul>
<li>大多数符号含义都和 <code>DDPM</code> 一样</li>
<li>只有 <code>t</code> 取值是 <code>980, 960,..., 20, 0</code> 这种非连续的取值</li>
</ul>
</li>
</ul>
<h3 id="ddim-公式拆解"><a class="markdownIt-Anchor" href="#ddim-公式拆解"></a> <code>DDIM</code> 公式拆解：</h3>
<h4 id="1-预测原输入"><a class="markdownIt-Anchor" href="#1-预测原输入"></a> 1. 预测原输入</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><msub><mi>d</mi><mrow><mi>x</mi><mn>0</mn></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><msqrt><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">pred_{x0}=\frac{x_t-\sqrt {\bar\alpha_t-1}\cdot\epsilon_\theta(x_t,t)}{\sqrt{\bar\alpha_t}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.4413300000000002em;vertical-align:-0.9321100000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.50922em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.79389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.75389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24611000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322200000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.79222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20777999999999996em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9321100000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>通过当前噪声隐变量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和模型预测的噪声 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\epsilon_\theta(x_t,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 估计原始输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，即去噪</li>
</ul>
<h4 id="2-计算调整方向"><a class="markdownIt-Anchor" href="#2-计算调整方向"></a> 2. 计算调整方向</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>d</mi><mi>i</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mo>=</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msqrt><mo>⋅</mo><msub><mi>ϵ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">direction\_point=\sqrt{1-\bar\alpha_{t-1}}\cdot\epsilon_\theta(x_t,t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.2880705000000001em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9519294999999999em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9119295000000003em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2880705000000001em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>根据噪声预测结果，计算从当前时间步 <code>t</code> 到前一个时间步 <code>t-1</code> 的调整方向，这一方向结合了噪声预测和噪声调度参数，用于引导隐变量的更新。</li>
</ul>
<h4 id="3-更新隐变量"><a class="markdownIt-Anchor" href="#3-更新隐变量"></a> 3. 更新隐变量</h4>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msqrt><mrow><msub><mover accent="true"><mi>α</mi><mo>ˉ</mo></mover><mi>t</mi></msub><mo>−</mo><mn>1</mn></mrow></msqrt><mo>⋅</mo><mi>p</mi><mi>r</mi><mi>e</mi><msub><mi>d</mi><mrow><mi>x</mi><mn>0</mn></mrow></msub><mo>+</mo><mi>d</mi><mi>i</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">x_{t-1}=\sqrt {\bar\alpha_t-1}\cdot pred_{x0}+direction\_point
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.15890499999999996em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8810950000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-2.841095em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15890499999999996em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span></span></p>
<ul>
<li>将预测的原始输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>r</mi><mi>e</mi><msub><mi>d</mi><mrow><mi>x</mi><mn>0</mn></mrow></msub></mrow><annotation encoding="application/x-tex">pred_{x0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 与调整方向结合，生成前一时刻的隐变量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>。此步骤通过线性组合逐步去噪，最终逼近目标数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li>
</ul>
<h3 id="ddpm-和-ddim-使用上的区别"><a class="markdownIt-Anchor" href="#ddpm-和-ddim-使用上的区别"></a> DDPM 和 DDIM 使用上的区别</h3>
<ul>
<li><code>DDPM</code> 去噪过程</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">model = UNet2DModel.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>).to(device)</span><br><span class="line">scheduler = DDPMScheduler.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>)</span><br><span class="line"><span class="comment"># Get precalculated alphas and alpha bars from the scheduler</span></span><br><span class="line">alphas = scheduler.alphas</span><br><span class="line">alphas_cumprod = scheduler.alphas_cumprod</span><br><span class="line"><span class="comment"># Initialize sample with static random noise</span></span><br><span class="line">sample = torch.load(<span class="string">&quot;random_noise.pt&quot;</span>).to(device)</span><br><span class="line"><span class="comment"># DDPM denoising loop</span></span><br><span class="line"><span class="comment"># scheduler.timesteps = [999, 998, ..., 1, 0]</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm.tqdm(scheduler.timesteps):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># Model prediction (noise residual)</span></span><br><span class="line">        residual = model(sample, t).sample</span><br><span class="line">        <span class="comment"># DDPM denoising formula</span></span><br><span class="line">        sample = (</span><br><span class="line">            sample - (<span class="number">1</span> - alphas[t]) / torch.sqrt(<span class="number">1</span> - alphas_cumprod[t]) * residual</span><br><span class="line">        ) / torch.sqrt(alphas[t])</span><br><span class="line">        <span class="comment"># Add random noise only for t &gt; 1</span></span><br><span class="line">        <span class="keyword">if</span> t &gt; <span class="number">1</span>:</span><br><span class="line">            noise = torch.randn_like(sample).to(device)</span><br><span class="line">            sample += torch.sqrt(<span class="number">1</span> - alphas[t]) * noise</span><br></pre></td></tr></table></figure>
<ul>
<li><code>DDIM</code> 去噪过程</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">model = UNet2DModel.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>).to(device)</span><br><span class="line">scheduler = DDIMScheduler.from_pretrained(<span class="string">&quot;google/ddpm-celebahq-256&quot;</span>)</span><br><span class="line"><span class="comment"># set inference steps</span></span><br><span class="line">scheduler.set_timesteps(num_inference_steps=<span class="number">50</span>)</span><br><span class="line"><span class="comment"># Initialize sample with static random noise</span></span><br><span class="line">sample = torch.load(<span class="string">&quot;random_noise.pt&quot;</span>).to(device)</span><br><span class="line"><span class="comment"># DDIM denoising loop</span></span><br><span class="line"><span class="comment"># scheduler.timesteps = [980, 960,..., 20, 0]</span></span><br><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm.tqdm(scheduler.timesteps)):</span><br><span class="line">    <span class="comment"># 将时间步转换为LongTensor并确保在正确设备上</span></span><br><span class="line">    t = t.to(device).long()</span><br><span class="line">    <span class="comment"># 获取当前和上一步的alpha累积乘积</span></span><br><span class="line">    alpha_cumprod_t = scheduler.alphas_cumprod[t]</span><br><span class="line">    alpha_cumprod_prev = (</span><br><span class="line">        scheduler.alphas_cumprod[scheduler.timesteps[i + <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">if</span> i + <span class="number">1</span> &lt; <span class="built_in">len</span>(scheduler.timesteps)</span><br><span class="line">        <span class="keyword">else</span> torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 将alpha值转换到相同设备</span></span><br><span class="line">    alpha_cumprod_t = alpha_cumprod_t.to(device)</span><br><span class="line">    alpha_cumprod_prev = alpha_cumprod_prev.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 1. 预测噪声残差</span></span><br><span class="line">        residual = model(sample, t).sample</span><br><span class="line">        <span class="comment"># 2. 计算预测的原始图像x0（去噪后的图像）</span></span><br><span class="line">        pred_x0 = (sample - torch.sqrt(<span class="number">1.0</span> - alpha_cumprod_t) * residual) / torch.sqrt(</span><br><span class="line">            alpha_cumprod_t</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 3. 计算下一步的样本方向</span></span><br><span class="line">        direction_xt = torch.sqrt(<span class="number">1.0</span> - alpha_cumprod_prev) * residual</span><br><span class="line">        <span class="comment"># 4. 组合得到新的样本</span></span><br><span class="line">        sample = torch.sqrt(alpha_cumprod_prev) * pred_x0 + direction_xt</span><br></pre></td></tr></table></figure>
<h3 id="二者对比分析"><a class="markdownIt-Anchor" href="#二者对比分析"></a> 二者对比分析</h3>
<ol>
<li><code>DDIM</code> 只需要 <code>50</code> 次迭代就能生成高质量的图像，而 <code>DDPM</code> 需要 <code>1000</code> 次迭代。</li>
<li>生成的图像质量相似，<code>DDIM</code> 生成的图像质量略高。</li>
<li>上面的代码中加载的噪声图是静态的，<code>DDIM</code> 跑多次生成的图像是一样的，而 <code>DDPM</code> 跑多次生成的图像是不一样的。</li>
<li>二者去噪结果对比，左侧是 <code>DDIM</code>，右侧是 <code>DDPM</code>：<br />
<img src="https://s2.loli.net/2025/05/15/SCknWUVJcxiHYr1.png" alt="concat.png" /></li>
</ol>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li><code>DDIM</code> 解决了 <code>DDPM</code> 的两大痛点，算是一个很好的改进。</li>
<li>为后续的 <code>LDM</code> 等模型打下了基础。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/" class="post-title-link" itemprop="url">Qwen3 技术报告先导篇</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-05 21:40:44" itemprop="dateCreated datePublished" datetime="2025-05-05T21:40:44+08:00">2025-05-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/05/05/Qwen3-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%85%88%E5%AF%BC%E7%AF%87/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>blog: <a target="_blank" rel="noopener" href="https://qwenlm.github.io/zh/blog/qwen3/">https://qwenlm.github.io/zh/blog/qwen3/</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>Qwen3</code> 系列模型四月二十九日正式发布，但到目前为止，还没发布技术报告，只有一篇官方博客，介绍了 <code>Qwen3</code> 的一些基本信息</li>
<li>本文围绕着官方博客介绍，结合实际使用情况，给出一些个人的理解</li>
</ul>
<h2 id="qwen3-系列模型"><a class="markdownIt-Anchor" href="#qwen3-系列模型"></a> Qwen3 系列模型</h2>
<ul>
<li>本次 <code>Qwen3</code> 主要发布了八个版本的模型，其中包含两个 <code>MOE</code> 模型和六个 <code>dense</code> 模型：
<ul>
<li><code>Qwen3-235B-A22B</code>：<code>MOE</code> 模型，<code>235B</code> 参数量，<code>22B</code> 激活参数量</li>
<li><code>Qwen3-30B-A3B</code>：<code>MOE</code> 模型，<code>30B</code> 参数量，<code>3B</code> 激活参数量</li>
<li><code>Qwen3-32B</code>：<code>dense</code> 模型，<code>32B</code> 参数量</li>
<li><code>Qwen3-14B</code>：<code>dense</code> 模型，<code>14B</code> 参数量</li>
<li><code>Qwen3-8B</code>：<code>dense</code> 模型，<code>8B</code> 参数量</li>
<li><code>Qwen3-4B</code>：<code>dense</code> 模型，<code>4B</code> 参数量</li>
<li><code>Qwen3-1.7B</code>：<code>dense</code> 模型，<code>1.7B</code> 参数量</li>
<li><code>Qwen3-0.6B</code>：<code>dense</code> 模型，<code>0.6B</code> 参数量</li>
</ul>
</li>
<li>还有对应的 <code>Base</code> 模型（只经过预训练）和 <code>fp8 Quantized</code> 模型（量化模型）</li>
<li><code>Qwen3</code> 相较于上一代 <code>Qwen2.5</code>，一个较大的技术进步是：<strong>统一了 Reasoning 和非 Reasoning 模式</strong>，即不再区分<strong>推理模型和非推理模型（或者叫思考模型和非思考模型）</strong>，而是一个模型可以通过 <code>prompt</code> 来选择推理模式和非推理模式</li>
</ul>
<h2 id="qwen3-模型的主要特性"><a class="markdownIt-Anchor" href="#qwen3-模型的主要特性"></a> Qwen3 模型的主要特性</h2>
<h3 id="预训练"><a class="markdownIt-Anchor" href="#预训练"></a> 预训练</h3>
<ul>
<li>相较于上一代 <code>Qwen2.5</code> 使用了 <code>18</code> 万亿个 <code>token</code> 做预训练，<code>Qwen3</code> 使用了 <code>36</code> 万亿个 <code>token</code> 做预训练（整整翻了一倍，这是多大的数据团队才能搞出来的，<s>太壕了</s>）</li>
<li>包含了 <code>119</code> 种语言和方言，有大量数据是合成数据和在 <code>PDF</code> 上识别的文本数据</li>
<li>预训练分成了三个阶段：
<ul>
<li>第一阶段：模型在超过 <code>30</code> 万亿个 <code>token</code> 上进行了预训练，上下文长度为 <code>4K token</code></li>
<li>第二阶段：通过增加知识密集型数据（如 <code>STEM</code>、编程和推理任务）的比例来改进数据集，在 <code>5</code> 万亿个 <code>token</code> 上进行了预训练</li>
<li>第三阶段：使用高质量的长上下文数据将上下文长度扩展到 <code>32K token</code></li>
</ul>
</li>
<li>预训练实际效果：官方的说法是 <code>Qwen3-1.7B/4B/8B/14B/32B-Base</code> 分别与 <code>Qwen2.5-3B/7B/14B/32B/72B-Base</code> 表现相当</li>
</ul>
<h3 id="后训练"><a class="markdownIt-Anchor" href="#后训练"></a> 后训练</h3>
<p><img src="https://s2.loli.net/2025/05/05/eEUjxX8bvydgNqt.png" alt="post-training.png" /></p>
<ul>
<li>简单概括：用 <code>SFT → RL → SFT+RL 混合 → RL</code> 四个阶段后训练 <code>Qwen3-235B-A22B</code> 和 <code>Qwen3-32B</code> 模型，然后蒸馏得到其他小尺寸模型（大 <code>MOE</code> 蒸小 <code>MOE</code>，大 <code>dense</code> 蒸小 <code>dense</code>）</li>
<li>四个阶段分别是：
<ul>
<li>长思维链冷启动 ：使用监督微调（<code>SFT</code>）训练模型生成初步的长思维链推理能力，作为初始阶段的基础。</li>
<li>长思维链强化学习 ：通过强化学习（<code>RL</code>）进一步提升模型在复杂推理任务中的表现，优化生成思维链的质量和连贯性。</li>
<li>思维模式融合 ：结合 <code>SFT</code> 和 <code>RL</code> 的混合策略，将不同思维模式（如逻辑推理、知识检索等）整合到统一框架中，增强模型的灵活性。</li>
<li>通用强化学习 ：以 <code>RL</code> 为主导，对模型进行全局优化，强化其在多样化任务中的通用性和鲁棒性。</li>
</ul>
</li>
</ul>
<h2 id="qwen3-使用"><a class="markdownIt-Anchor" href="#qwen3-使用"></a> Qwen3 使用</h2>
<ul>
<li>使用上和 <code>Qwen2.5</code> 没有太大区别，主要是增加了 <code>Reasoning</code> 模式的选择开关</li>
<li>这个开关也非常简便，大概有两种方式可以选择：
<ol>
<li>在最新版 <code>Transformer</code> 库中 <code>tokenizer.apply_chat_template</code> 中可以设置 <code>enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.</code></li>
<li>另外一种方式就是始终保持上面的开关常开，然后在 <code>prompt</code> 中添加 <code>/no_think</code> 来关闭推理模式。<code>/no_think</code> 可加到 <code>system content</code> 或 <code>user content</code> 结尾。</li>
</ol>
</li>
<li>在一些下游任务上做了 <code>SFT</code> 微调测试，发现 <code>Qwen3-1.7B</code> 经过 <code>SFT</code> 之后和 <code>Qwen2.5-3B-Instruct</code> 经过 <code>SFT</code> 之后的效果差不多，这一点和官方对预训练的说法一致</li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>在当今这个大模型结构同质化时代，预训练数据越多，模型的能力越强（同样效果下数据翻一倍模型尺寸可缩减一半！！），所以小厂没有这么多数据工程师，搞不到大量的高质量的数据，是很难在大模型上追赶大厂</li>
<li>到今天为止，<code>Llama</code> 系列官方模型都不能做到原生支持中文，和 <code>Qwen</code> 系列模型原生支持 <code>119</code> 种语言和方言相比，感觉非常小家子气，注定会被扫进历史的垃圾堆里</li>
<li><code>DeepSeek</code> 对 <code>Qwen</code> 的影响很大，比如：
<ul>
<li>从 <code>DeepSeek v3</code> 之后，<code>Qwen</code> 系列模型预训练和最终版的命名从：<code>Qwen2.5-7B/Qwen2.5-7B-Instruct</code> 变成了 <code>Qwen3-8B-Base/Qwen3-8B</code></li>
<li>广泛的使用了<strong>模型蒸馏</strong>，确实极大的提高了小尺寸模型的能力（之前 <code>Qwen2.5-0.5B-Instruct</code> 基本就是个答非所问的傻子，现如今的 <code>Qwen3-0.6B</code> 在不开推理模式的情况下也可以解决很多数学问题，非常强）</li>
<li>四段式后训练也和 <code>DeepSeek</code> 使用的后训练非常相似</li>
</ul>
</li>
<li>合并推理和非推理模型的做法，是今后大模型的趋势，有不少模型都在朝着这个方向发展</li>
<li>官方表示 <code>Qwen</code> 系列后续会向着 <code>Agent</code> 方向发展，铺垫了很久的 <code>MCP</code> 可能会产生新的 <code>Agent</code> 应用变革</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">CUDA 学习笔记 01 —— CUDA 基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-24 09:57:45" itemprop="dateCreated datePublished" datetime="2025-03-24T09:57:45+08:00">2025-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/24/CUDA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E2%80%94%E2%80%94-CUDA-%E5%9F%BA%E7%A1%80/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="0-学习-cuda-的目的"><a class="markdownIt-Anchor" href="#0-学习-cuda-的目的"></a> 0. 学习 CUDA 的目的</h2>
<ul>
<li>作为一个算法工程师，平时接触 <code>HPC (High Performance Computing)</code> 的机会并不多，那为什么还要学习 <code>CUDA</code> 呢？</li>
<li>学习 <code>CUDA</code> 的目的不是为了用 <code>CUDA</code> 做模型加速，而是从 <code>CUDA</code> 角度理解目前较新的大模型设计理念，这些高性能模型是如何从原理上做到又快又好的。</li>
<li>例如火出圈的 <code>DeepSeek</code> 系列模型，在模型设计角度做了较多创新，并开源了部分 <code>CUDA</code> 代码，对于不了解 <code>CUDA</code> 的工程师，很难 get 到算法设计的优雅之处。</li>
<li>反观某家大模型基座公司，曾开源某个模型结构，论文中一通自夸，分析理论计算量有多低。但很多人实测发现速度并没有很快，究其原因，实际上是这家公司还用的小模型时代的旧思维，即：一个模型理论计算量低，那就是快。</li>
<li>大模型时代不了解硬件，不尊重硬件，在算法创新上不太可能走的远。</li>
</ul>
<h2 id="1-hello-world"><a class="markdownIt-Anchor" href="#1-hello-world"></a> 1. Hello World</h2>
<h3 id="cuda-代码"><a class="markdownIt-Anchor" href="#cuda-代码"></a> cuda 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime_api.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="comment">// cuda 中 host 表示 cpu 端，device 表示 gpu 端</span></span><br><span class="line"><span class="comment">// __device__ 是设备函数的声明符号，表明该函数在 device 执行，且只能在 device</span></span><br><span class="line"><span class="comment">// 中调用</span></span><br><span class="line"><span class="function">__device__ <span class="keyword">const</span> <span class="keyword">char</span> *<span class="title">device_hello_world</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">&quot;GPU: Hello world!\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// __host__ 是主机函数的声明符号，表明该函数在 host 执行，且只能在 host 中调用</span></span><br><span class="line"><span class="function">__host__ <span class="keyword">const</span> <span class="keyword">char</span> *<span class="title">host_hello_world</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123; <span class="keyword">return</span> <span class="string">&quot;CPU: Hello world!\n&quot;</span>; &#125;</span><br><span class="line"><span class="comment">// __global__ 是核函数的声明符号，表明该函数在 device 执行，且只能在 host 中调用</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_world</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *str = <span class="built_in">device_hello_world</span>();</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, str);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, <span class="built_in">host_hello_world</span>());</span><br><span class="line">  <span class="comment">// &lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt; 是核函数的调用符号，表示启动 grid_dim 个 block，</span></span><br><span class="line">  <span class="comment">// 每个 block 有 block_dim 个线程</span></span><br><span class="line">  hello_world&lt;&lt;&lt;<span class="number">1</span>, <span class="number">10</span>&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cuda</code> 的三个函数声明符号：
<ul>
<li><code>__host__</code>：主机函数，表示该函数在 CPU 上执行，且只能在 CPU 中调用</li>
<li><code>__device__</code>：设备函数，表示该函数在 GPU 上执行，且只能在 GPU 中调用</li>
<li><code>__global__</code>：核函数，表示该函数在 GPU 上执行，且只能在 CPU 中调用</li>
</ul>
</li>
<li>其中 <code>__global__</code> 声明的函数类型被称为 <strong>核函数</strong>，是 <code>CUDA</code> 中最重要的函数类型
<ul>
<li>核函数通过 <code>&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;</code> 的方式调用，其中 <code>&lt;&lt;&lt;&gt;&gt;&gt;</code> 是 <code>cuda</code> 扩展关键字</li>
<li><code>grid_dim</code> 表示启动的 <code>block</code> 数量，<code>block_dim</code> 表示每个 <code>block</code> 中的线程数量</li>
<li><code>grid_dim</code> 和 <code>block_dim</code> 都是 <code>dim3</code> 类型的变量，表示三维数组，如果使用整形则模型 <code>y</code> 和 <code>z</code> 维度都为 1</li>
</ul>
</li>
</ul>
<h3 id="编译"><a class="markdownIt-Anchor" href="#编译"></a> 编译</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc hello_world.cu -g -o hello_world</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a class="markdownIt-Anchor" href="#运行"></a> 运行</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">./hello_world</span><br><span class="line"><span class="comment"># CPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br><span class="line"><span class="comment"># GPU: Hello world!</span></span><br></pre></td></tr></table></figure>
<h2 id="2-dimension-测试"><a class="markdownIt-Anchor" href="#2-dimension-测试"></a> 2. dimension 测试</h2>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;threadIdx:(%d,%d,%d) blockIdx:(%d,%d,%d) blockDim:(%d,%d,%d)\</span></span><br><span class="line"><span class="string">  gridDim(%d,%d,%d)\n&quot;</span>,</span><br><span class="line">         threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y,</span><br><span class="line">         blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x, gridDim.y,</span><br><span class="line">         gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> nElem = <span class="number">6</span>;  <span class="comment">// number of elements</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>;  <span class="comment">// block size</span></span><br><span class="line">  <span class="keyword">int</span> nBlock = (nElem + block.x - <span class="number">1</span>) / block.x; <span class="comment">// number of blocks</span></span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nBlock)</span></span>;  <span class="comment">// grid size</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;grid.x %d grid.y %d grid.z %d\n&quot;</span>, grid.x, grid.y, grid.z);  </span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;block.x %d block.y %d block.z %d\n&quot;</span>, block.x, block.y, block.z);</span><br><span class="line">  checkIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>执行结果：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./check_dimension</span><br><span class="line"><span class="comment"># grid.x 2 grid.y 1 grid.z 1</span></span><br><span class="line"><span class="comment"># block.x 3 block.y 1 block.z 1</span></span><br><span class="line"><span class="comment"># threadIdx:(0,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(1,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(2,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(0,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(1,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br><span class="line"><span class="comment"># threadIdx:(2,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span></span><br></pre></td></tr></table></figure>
<h2 id="3-cuda-向量加法"><a class="markdownIt-Anchor" href="#3-cuda-向量加法"></a> 3. CUDA 向量加法</h2>
<h3 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;freshman.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__host__ <span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">float</span> *res, <span class="keyword">const</span> <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">    res[i] = a[i] + b[i];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">float</span> *res, <span class="keyword">const</span> <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (i &lt; size) <span class="comment">// 线程索引越界检查</span></span><br><span class="line">    res[i] = a[i] + b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// set up device</span></span><br><span class="line">  <span class="built_in">initDevice</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// allocate host memory</span></span><br><span class="line">  <span class="keyword">int</span> nElem = <span class="number">1</span> &lt;&lt; <span class="number">24</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>, nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte = <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>) * nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="comment">// allocate device memory</span></span><br><span class="line">  <span class="keyword">float</span> *a_d, *b_d, *res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;a_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;b_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;res_d, nByte));</span><br><span class="line">  <span class="comment">// randomly initialize the input data</span></span><br><span class="line">  <span class="built_in">initialData</span>(a_h, nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h, nElem);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d, a_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d, b_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="comment">// set up execution configuration</span></span><br><span class="line">  <span class="comment">// 1. 计算最佳块大小</span></span><br><span class="line">  <span class="keyword">int</span> minGridSize, bestBlockSize;</span><br><span class="line">  <span class="built_in">cudaOccupancyMaxPotentialBlockSize</span>(&amp;minGridSize, &amp;bestBlockSize,</span><br><span class="line">                                     (<span class="keyword">void</span> *)sumArraysGPU,</span><br><span class="line">                                     <span class="number">0</span>, <span class="comment">// 动态共享内存大小</span></span><br><span class="line">                                     <span class="number">0</span>  <span class="comment">// 无块大小限制</span></span><br><span class="line">  );</span><br><span class="line">  <span class="comment">// 2. 设置网格和块维度</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(bestBlockSize)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nElem + bestBlockSize - <span class="number">1</span>) / bestBlockSize)</span></span>;</span><br><span class="line">  <span class="comment">// 3. 设备执行并统计耗时</span></span><br><span class="line">  <span class="keyword">double</span> iStart, iElaps;</span><br><span class="line">  iStart = <span class="built_in">cpuSecond</span>();</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(a_d, b_d, res_d, nElem);</span><br><span class="line">  iElaps = <span class="built_in">cpuSecond</span>() - iStart;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h, res_d, nByte, cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed %f sec\n&quot;</span>, grid.x,</span><br><span class="line">         block.x, iElaps);</span><br><span class="line">  <span class="comment">// 4. CPU执行并统计耗时</span></span><br><span class="line">  iStart = <span class="built_in">cpuSecond</span>();</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h, b_h, res_h, nElem);</span><br><span class="line">  iElaps = <span class="built_in">cpuSecond</span>() - iStart;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;CPU Time elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line">  <span class="comment">// 5. 检查结果</span></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h, res_from_gpu_h, nElem);</span><br><span class="line">  <span class="comment">// 6. 释放内存</span></span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaOccupancyMaxPotentialBlockSize</code> 函数用于计算最佳块大小</li>
<li><code>max thread per block</code> 是 <code>cuda</code> 中的一个限制，表示每个块中最多可以有多少个线程，一般为 <code>1024</code>，当超过这个限制时，<code>CHECK(cudaGetLastError());</code> 会报错</li>
</ul>
<h3 id="运行结果"><a class="markdownIt-Anchor" href="#运行结果"></a> 运行结果</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./sum_arrays</span><br><span class="line"><span class="comment"># Using device 0: NVIDIA GeForce RTX 4060 Laptop GPU</span></span><br><span class="line"><span class="comment"># Vector size:16777216</span></span><br><span class="line"><span class="comment"># Execution configuration&lt;&lt;&lt;21846,768&gt;&gt;&gt; Time elapsed 0.000030 sec</span></span><br><span class="line"><span class="comment"># CPU Time elapsed 0.076604 sec</span></span><br><span class="line"><span class="comment"># Check result success!</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可以看出，<code>cuda</code> 的执行时间远远小于 <code>cpu</code> 的执行时间，相差了 <code>2553</code> 倍</li>
</ul>
<h3 id="总体流程"><a class="markdownIt-Anchor" href="#总体流程"></a> 总体流程</h3>
<pre class="mermaid">graph TD
    A[Host 端申请内存] --> B[Host 端初始化输入数据]
    B --> C[Device 端申请内存]
    C --> D[拷贝 Host 输入数据到 Device 端]
    D --> E[Device 端执行核函数]
    E --> F[拷贝 Device 端输出数据到 Host 端]
    F --> G[Host 端检查结果]
    G --> H[释放 Host 端和 Device 端全部内存]
    B --> J[Host 端执行普通函数] --> G</pre>
<h3 id="细节分析"><a class="markdownIt-Anchor" href="#细节分析"></a> 细节分析</h3>
<h4 id="1-cuda-内存分配是怎么做的"><a class="markdownIt-Anchor" href="#1-cuda-内存分配是怎么做的"></a> 1. <code>cuda</code> 内存分配是怎么做的？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *a_d; <span class="comment">// 空指针</span></span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;a_d, nByte);  <span class="comment">// 将指针的地址转成二级指针（指针的指针）传入内存分配函数</span></span><br></pre></td></tr></table></figure>
<ul>
<li>这里的二级指针应用很巧妙，由于 <code>c++</code> 中的指针是值传递，所以如果是一级指针传入 <code>cudaMalloc</code> 函数时，指针 <code>a_d</code> 的值不会改变，因此只能将指针的地址转成二级指针传入内存分配函数</li>
</ul>
<h4 id="2-cuda-内存拷贝是怎么做的"><a class="markdownIt-Anchor" href="#2-cuda-内存拷贝是怎么做的"></a> 2. <code>cuda</code> 内存拷贝是怎么做的？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMemcpy</span>(a_d, a_h, nByte, cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaMemcpy</code> 函数的四个参数分别是：
<ul>
<li><code>dst</code>：目标地址</li>
<li><code>src</code>：源地址</li>
<li><code>size</code>：拷贝的字节数</li>
<li><code>kind</code>：拷贝的类型，属于 <code>cudaMemcpyKind</code> 枚举类型
<ul>
<li><code>cudaMemcpyHostToHost          =   0,      /**&lt; Host   -&gt; Host */</code></li>
<li><code>cudaMemcpyHostToDevice        =   1,      /**&lt; Host   -&gt; Device */</code></li>
<li><code>cudaMemcpyDeviceToHost        =   2,      /**&lt; Device -&gt; Host */</code></li>
<li><code>cudaMemcpyDeviceToDevice      =   3,      /**&lt; Device -&gt; Device */</code></li>
<li><code>cudaMemcpyDefault             =   4       /**&lt; Direction of the transfer is inferred from the pointer values. Requires unified virtual addressing */</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-cuda-核函数如何解析线程索引"><a class="markdownIt-Anchor" href="#3-cuda-核函数如何解析线程索引"></a> 3. <code>cuda</code> 核函数如何解析线程索引？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"><span class="keyword">if</span> (i &lt; size) <span class="comment">// 线程索引越界检查</span></span><br><span class="line">  res[i] = a[i] + b[i];</span><br></pre></td></tr></table></figure>
<ul>
<li><code>blockIdx</code>：表示当前块的索引</li>
<li><code>blockDim</code>：表示当前块的维度（每个块中的线程数）</li>
<li><code>threadIdx</code>：表示当前线程的索引</li>
<li>每个线程中计算两个标量的和</li>
<li>由于 <code>gridDim * blockDim</code> 可能大于 <code>size</code>，所以需要判断线程索引是否越界</li>
</ul>
<h4 id="4-如何计算最佳块大小"><a class="markdownIt-Anchor" href="#4-如何计算最佳块大小"></a> 4. 如何计算最佳块大小？</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> minGridSize, bestBlockSize;</span><br><span class="line"><span class="built_in">cudaOccupancyMaxPotentialBlockSize</span>(&amp;minGridSize, &amp;bestBlockSize,</span><br><span class="line">                                     (<span class="keyword">void</span> *)sumArraysGPU,</span><br><span class="line">                                     <span class="number">0</span>, <span class="comment">// 动态共享内存大小</span></span><br><span class="line">                                     <span class="number">0</span>  <span class="comment">// 无块大小限制</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaOccupancyMaxPotentialBlockSize</code> 函数用于计算最佳块大小，五个参数分别是：
<ul>
<li><code>minGridSize</code>：最小网格大小变量地址</li>
<li><code>bestBlockSize</code>：最佳块大小变量地址</li>
<li><code>kernel</code>：核函数指针</li>
<li><code>dynamicSMemSize</code>：动态共享内存大小</li>
<li><code>blockSizeLimit</code>：块大小限制</li>
</ul>
</li>
<li>函数名就是函数地址，可强转为 <code>void *</code> 函数指针（也可以写成：<code>(void *)&amp;sumArraysGPU</code>）</li>
</ul>
<h2 id="4-cuda-编程模型"><a class="markdownIt-Anchor" href="#4-cuda-编程模型"></a> 4. CUDA 编程模型</h2>
<h3 id="线程块"><a class="markdownIt-Anchor" href="#线程块"></a> 线程块</h3>
<ul>
<li>线程块 <code>block</code> 是 <code>CUDA</code> 中的逻辑执行单元，是一个三维逻辑结构：
<ul>
<li><code>block.x</code>：表示块的 x 维度大小</li>
<li><code>block.y</code>：表示块的 y 维度大小</li>
<li><code>block.z</code>：表示块的 z 维度大小</li>
<li>其中 <code>block.x</code> 是最内层的循环，<code>block.y</code> 是第二层循环，<code>block.z</code> 是最外层的循环</li>
<li>用三维数组可以表示为：<code>tread[z][y][x]</code>，即 <code>tid = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y</code></li>
</ul>
</li>
</ul>
<h3 id="线程束"><a class="markdownIt-Anchor" href="#线程束"></a> 线程束</h3>
<ul>
<li>线程束 <code>warp</code> 是 <code>CUDA</code> 基本调度执行单元，一个 <code>warp</code> 由 <code>32</code> 个线程组成
<ul>
<li>一个 <code>warp</code> 中的线程在一个时钟周期内执行同一条指令（单指令多线程，<code>SIMT</code>）</li>
<li>一个 <code>warp</code> 中的线程可以共享指令指针和执行资源（如寄存器、缓存等）</li>
<li><code>Warp</code> 调度器（<code>warp scheduler</code>）负责将 <code>warp</code> 分配到物理执行单元上执行</li>
</ul>
</li>
<li>线程块会被划分为多个 <code>warp</code>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>a</mi><mi>r</mi><mi>p</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>B</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi><mo>=</mo><mi>c</mi><mi>e</mi><mi>i</mi><mi>l</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>T</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mi>P</mi><mi>e</mi><mi>r</mi><mi>B</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>k</mi></mrow><mn>32</mn></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">WarpsPerBlock=ceil(\frac{ThreadsPerBlock}{32})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight">h</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<h3 id="cuda-core"><a class="markdownIt-Anchor" href="#cuda-core"></a> CUDA core</h3>
<ul>
<li><code>CUDA core</code> 是 <code>CUDA</code> 物理执行单元，负责实际的计算任务</li>
<li>一个 <code>CUDA core</code> 一个时钟周期只能计算一个线程的指令</li>
</ul>
<h3 id="streammultiprocessor"><a class="markdownIt-Anchor" href="#streammultiprocessor"></a> StreamMultiprocessor</h3>
<ul>
<li><code>StreamMultiprocessor</code> 流式多处理器（简称 <code>SM</code>），负责执行 <code>CUDA</code> 线程块中的并行计算任务</li>
<li>每个 <code>GPU</code> 包含多个 <code>SM</code>，每个 <code>SM</code> 包含多个 <code>CUDA core</code>，例如：<code>RTX 4060</code> 有 <code>24</code> 个 <code>SM</code>，每个 <code>SM</code> 有 <code>128</code> 个 <code>CUDA core</code></li>
</ul>
<h2 id="5-reduce"><a class="markdownIt-Anchor" href="#5-reduce"></a> 5. Reduce</h2>
<ul>
<li>规约（<code>Reduce</code>）是 <code>CUDA</code> 编程中常见的操作，主要用于将多个数据元素规约为一个数据元素</li>
<li>规约操作通常是一个二元操作，例如：<code>sum</code>、<code>mul</code>、<code>max</code>、<code>min</code> 等，简单的规约可以合并成强大的算子，甚至可以说规约算子是神经网络的基础</li>
</ul>
<h3 id="规约求和"><a class="markdownIt-Anchor" href="#规约求和"></a> 规约求和</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CPU 规约求和</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">recursiveReduce</span><span class="params">(<span class="keyword">int</span> *data, <span class="keyword">int</span> <span class="keyword">const</span> size)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// terminate check</span></span><br><span class="line">  <span class="keyword">if</span> (size == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">  <span class="comment">// renew the stride</span></span><br><span class="line">  <span class="keyword">int</span> <span class="keyword">const</span> stride = size / <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">if</span> (size % <span class="number">2</span> == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stride; i++) &#123;</span><br><span class="line">      data[i] += data[i + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    data[<span class="number">0</span>] += data[size - <span class="number">1</span>];</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stride; i++) &#123;</span><br><span class="line">      data[i] += data[i + stride];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// call</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">recursiveReduce</span>(data, stride);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GPU 规约相邻求和</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceNeighbored</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// set thread ID</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="comment">// boundary check</span></span><br><span class="line">  <span class="keyword">if</span> (tid &gt;= n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// convert global data pointer to local point of this block</span></span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="comment">// in-place reduction in global memory</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((tid % (<span class="number">2</span> * stride)) == <span class="number">0</span>) &#123;</span><br><span class="line">      idata[tid] += idata[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// synchronize within block</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global mem</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GPU 规约相邻求和（简化版）</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceNeighboredLess</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="keyword">unsigned</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="comment">// convert global data pointer to the local point of this block</span></span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span> (idx &gt; n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// in-place reduction in global memory</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="comment">// convert tid into local array index</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">2</span> * stride * tid;</span><br><span class="line">    <span class="keyword">if</span> (index &lt; blockDim.x) &#123;</span><br><span class="line">      idata[index] += idata[index + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global men</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GPU 规约交错求和，主要是 stride 的计算方式不同</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceInterleaved</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="keyword">unsigned</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="comment">// convert global data pointer to the local point of this block</span></span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span> (idx &gt;= n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// in-place reduction in global memory</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = blockDim.x / <span class="number">2</span>; stride &gt; <span class="number">0</span>; stride &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; stride) &#123;</span><br><span class="line">      idata[tid] += idata[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global men</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="效率分析"><a class="markdownIt-Anchor" href="#效率分析"></a> 效率分析</h3>
<ul>
<li>本代码中使用了三种核函数实现方式做同一个规约操作，分别是：
<ul>
<li><code>reduceNeighbored</code>：相邻线程规约</li>
<li><code>reduceNeighboredLess</code>：相邻线程规约（简化版）</li>
<li><code>reduceInterleaved</code>：交错线程规约</li>
</ul>
</li>
<li>三者效率从高到低依次是：
<ul>
<li><code>reduceInterleaved</code> &gt; <code>reduceNeighboredLess</code> &gt; <code>reduceNeighbored</code></li>
</ul>
</li>
<li>三者的示意图分别如下：
<ul>
<li><code>reduceNeighbored</code>：相邻线程规约的实现<br />
<image src="https://s2.loli.net/2025/04/14/ThePxqG52r3YKCd.png" width=60%/></li>
<li><code>reduceNeighboredLess</code>：相邻线程规约的简化版实现（注意圆圈中的符号已和上图不一致）<br />
<image src="https://s2.loli.net/2025/04/14/9JTkPeCd5u1a7gF.png" width=60%/></li>
<li><code>reduceInterleaved</code>：交错线程规约的实现<br />
<image src="https://s2.loli.net/2025/04/14/dUcei9EZGsbgzRB.png" width=60%/></li>
</ul>
</li>
<li>三者效率差异主要来自于 <strong>线程分支分化</strong>，后续会详细介绍</li>
</ul>
<h2 id="6-循环展开"><a class="markdownIt-Anchor" href="#6-循环展开"></a> 6. 循环展开</h2>
<ul>
<li>循环展开（<code>Loop Unrolling</code>）是 <code>CUDA</code> 中常用的优化手段，主要用于减少循环控制开销和提高指令级并行度</li>
<li>简单说就是一个线程不再只计算一个数据，而是计算多个数据，而且是直接在代码中展开，而不是在编译器中展开</li>
<li>可以简单理解成：启动线程是需要花时间的，启动一个线程只算一个数据，太浪费了，所以我们可以让一个线程计算多个数据，这样就能减少启动线程的时间开销，所以就省时间了</li>
</ul>
<h3 id="代码-2"><a class="markdownIt-Anchor" href="#代码-2"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 数据总长是 8 * blockDim.x * gridDim.x</span></span><br><span class="line"><span class="comment">// 线程数是 blockDim.x * gridDim.x</span></span><br><span class="line"><span class="comment">// 每个线程计算 8 个数据</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">reduceUnroll8</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> idx = blockDim.x * blockIdx.x * <span class="number">8</span> + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (tid &gt;= n)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">int</span> *idata = g_idata + blockIdx.x * blockDim.x * <span class="number">8</span>;</span><br><span class="line">  <span class="comment">// 循环展开，每个线程计算 8 个数据</span></span><br><span class="line">  <span class="comment">// 直接把 8 * blockDim.x * gridDim.x 的数据总长</span></span><br><span class="line">  <span class="comment">// 聚合到了 blockDim.x * gridDim.x 的线程数上</span></span><br><span class="line">  <span class="keyword">if</span> (idx + <span class="number">7</span> * blockDim.x &lt; n) &#123;</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">2</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">3</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">4</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">5</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">6</span>];</span><br><span class="line">    g_idata[idx] += g_idata[idx + blockDim.x * <span class="number">7</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 这里需要同步，也就是线程阻塞直到所有线程都执行完</span></span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="comment">// 然后就是一个最简单的规约操作了，和上面一样</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> stride = blockDim.x / <span class="number">2</span>; stride &gt; <span class="number">0</span>; stride &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; stride) &#123;</span><br><span class="line">      idata[tid] += idata[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// write result for this block to global mem</span></span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-核函数递归调用"><a class="markdownIt-Anchor" href="#7-核函数递归调用"></a> 7. 核函数递归调用</h2>
<ul>
<li>和 <code>CPU</code> 一样，<code>CUDA</code> 也支持核函数递归调用，调用方式和普通递归函数一样</li>
<li>需要注意的是在编译的时候需要加上 <code>-rdc=true</code> 选项</li>
</ul>
<h3 id="代码-3"><a class="markdownIt-Anchor" href="#代码-3"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">nesthelloworld</span><span class="params">(<span class="keyword">int</span> iSize, <span class="keyword">int</span> iDepth)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;depth : %d blockIdx: %d,threadIdx: %d\n&quot;</span>, iDepth, blockIdx.x,</span><br><span class="line">         threadIdx.x);</span><br><span class="line">  <span class="keyword">if</span> (iSize == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">int</span> nthread = (iSize &gt;&gt; <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span> &amp;&amp; nthread &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 递归调用核函数</span></span><br><span class="line">    nesthelloworld&lt;&lt;&lt;<span class="number">1</span>, nthread&gt;&gt;&gt;(nthread, ++iDepth);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;-----------&gt; nested execution depth: %d\n&quot;</span>, iDepth);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="8-固定内存"><a class="markdownIt-Anchor" href="#8-固定内存"></a> 8. 固定内存</h2>
<ul>
<li><code>Pinned Memory</code> 是 <code>CUDA</code> 中的一种特殊内存类型（不是显存，是内存），主要用于提高数据传输效率</li>
<li>普通内存是分页管理，分页管理存在两个问题：
<ol>
<li>一页内存逻辑上连续，但物理上不连续</li>
<li>操作系统可能会将内存页交换到磁盘上，导致数据不在物理内存中</li>
</ol>
</li>
<li><code>Pinned Memory</code> 就是解决了这两个问题，分配了一块连续物理地址且固定的主机内存（<code>host</code> 内存），方便整块拷贝数据到显存（<code>DMA</code>）</li>
</ul>
<h3 id="代码-4"><a class="markdownIt-Anchor" href="#代码-4"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;freshman.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line">  <span class="keyword">int</span> nElem = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line">  <span class="keyword">int</span> nByte = <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>) * nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h, *b_h, *res_h, *res_from_gpu_h;</span><br><span class="line">  <span class="comment">// 注意这里的 cudaMallocHost 和 cudaMalloc 是不同的</span></span><br><span class="line">  <span class="comment">// 前者申请的是 host 固定内存，后者申请的是 device 显存</span></span><br><span class="line">  <span class="comment">// cudaMallocHost 是 malloc 的一个平替</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;a_h, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;b_h, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;res_h, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;res_from_gpu_h, nByte));</span><br><span class="line">  <span class="comment">// 初始化数据</span></span><br><span class="line">  <span class="built_in">memset</span>(res_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h, <span class="number">0</span>, nByte);</span><br><span class="line">  <span class="built_in">initialData</span>(a_h, nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h, nElem);</span><br><span class="line">  <span class="comment">// 申请设备显存</span></span><br><span class="line">  <span class="keyword">float</span> *a_d, *b_d, *res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;a_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;b_d, nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="keyword">float</span> **)&amp;res_d, nByte));</span><br><span class="line">  <span class="comment">// 拷贝数据到设备显存</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d, a_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d, b_h, nByte, cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="comment">// 跑核函数</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem / block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid, block&gt;&gt;&gt;(a_d, b_d, res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>, grid.x, block.x);</span><br><span class="line">  <span class="comment">// 结果拷贝回主机并检查两个设备计算结果是否一致</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h, res_d, nByte, cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h, b_h, res_h, nElem);</span><br><span class="line">  <span class="built_in">checkResult</span>(res_h, res_from_gpu_h, nElem);</span><br><span class="line">  <span class="comment">// 释放内存</span></span><br><span class="line">  <span class="comment">// 注意这里的 cudaFreeHost 和 cudaFree 是不同的</span></span><br><span class="line">  <span class="comment">// 前者释放的是 host 固定内存，后者释放的是 device 显存</span></span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(a_h);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(b_h);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(res_h);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(res_from_gpu_h);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="关键点"><a class="markdownIt-Anchor" href="#关键点"></a> 关键点</h3>
<ol>
<li><strong>主存</strong> <strong>普通内存</strong> 的分配和释放函数是 <code>malloc</code> 和 <code>free</code></li>
<li><strong>主存</strong> <strong>固定内存</strong> 的分配和释放函数是 <code>cudaMallocHost</code> 和 <code>cudaFreeHost</code></li>
<li><strong>显存</strong> 的分配和释放函数是 <code>cudaMalloc</code> 和 <code>cudaFree</code></li>
</ol>
<h2 id="9-零拷贝内存-和-统一虚拟地址"><a class="markdownIt-Anchor" href="#9-零拷贝内存-和-统一虚拟地址"></a> 9. 零拷贝内存 和 统一虚拟地址</h2>
<ul>
<li><code>Zero-Copy Memory</code> 是 <code>CUDA</code> 中一种允许 <code>GPU</code> 直接访问主机内存的技术，避免了显式的数据拷贝操作（不需要用 <code>cudaMemcpy</code> 函数）</li>
<li>实际上，<code>Zero-Copy Memory</code> 在很多时候并不快，因为 <code>GPU</code> 访问主机内存的速度远远低于访问显存的速度，因此，<code>Zero-Copy Memory</code> 只适用于一些特殊的场景，例如：
<ul>
<li>主机内存中的数据只需要被 <code>GPU</code> 使用一次</li>
<li>数据量太大，显存放不下</li>
<li>调试用途</li>
</ul>
</li>
<li><code>Zero-Copy Memory</code> 的实现方式是将主机内存映射到 <code>GPU</code> 的地址空间中，<code>GPU</code> 通过访问这个地址空间来访问主机内存，实际上走的是 <code>PCIe</code> 总线</li>
<li>由于不需要先完成所有数据的拷贝再开始执行核函数，因此 <code>Zero-Copy Memory</code> 使用异步拷贝的方式来实现，可将部分拷贝数据的时间和核函数执行的时间重叠，但并不多</li>
<li><code>Unified Virtual Addressing (UVA)</code> 是 <code>CUDA</code> 中的一种内存管理机制，允许 <code>CPU</code> 和 <code>GPU</code> 共享同一虚拟地址空间</li>
</ul>
<h3 id="代码-5"><a class="markdownIt-Anchor" href="#代码-5"></a> 代码</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *a_host, *b_host, *res_d;</span><br><span class="line"><span class="comment">// 申请主机固定内存，添加特殊 flag cudaHostAllocMapped</span></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;a_host, nByte, cudaHostAllocMapped));</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="keyword">float</span> **)&amp;b_host, nByte, cudaHostAllocMapped));</span><br><span class="line"><span class="comment">// a_host 和 b_host 是可直接作为核函数的输入参数</span></span><br><span class="line"><span class="comment">// 也可以转成 Device 地址空间，如下：</span></span><br><span class="line"><span class="keyword">float</span> *a_dev, *b_dev;</span><br><span class="line"><span class="comment">// 映射主机内存到设备地址空间</span></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaHostGetDevicePointer</span>((<span class="keyword">void</span> **)&amp;a_dev, (<span class="keyword">void</span> *)a_host, <span class="number">0</span>));</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaHostGetDevicePointer</span>((<span class="keyword">void</span> **)&amp;b_dev, (<span class="keyword">void</span> *)b_host, <span class="number">0</span>));</span><br><span class="line"><span class="comment">// 用 a_dev 和 b_dev 作为核函数的输入参数计算</span></span><br></pre></td></tr></table></figure>
<ul>
<li>和 <code>Zero-Copy Memory</code> 一样，<code>UVA</code> 甚至不需要将 <code>a_host</code> 转成 <code>a_dev</code>，直接用 <code>a_host</code> 就可以调用核函数</li>
</ul>
<h2 id="10-aos-和-soa"><a class="markdownIt-Anchor" href="#10-aos-和-soa"></a> 10. Aos 和 SoA</h2>
<ul>
<li><code>CUDA</code> 不仅支持最简单的原生数据类型，还支持自定义数据类型（<code>struct</code>），例如 <code>Aos</code> 和 <code>SoA</code> 等</li>
<li><code>Aos</code>（Array of Structures）和 <code>SoA</code>（Structure of Arrays）是两种不同的数据存储方式，这两种方式由于变量的排布方式不同，导致了访问内存的效率差异</li>
</ul>
<h3 id="aos"><a class="markdownIt-Anchor" href="#aos"></a> Aos</h3>
<ul>
<li><code>Aos</code> 是将多个结构体存储在一个数组中，每个结构体的成员变量是连续存储的</li>
<li>例如：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">AoSStruct</span> &#123;</span></span><br><span class="line">  <span class="keyword">float</span> a;</span><br><span class="line">  <span class="keyword">float</span> b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, struct naiveStruct *res, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (i &lt; n)</span><br><span class="line">    res[i].a = a[i] + b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>这里的 <code>res</code> 是一个 <strong>结构体数组</strong>，<code>a</code> 和 <code>b</code> 是结构体的成员变量，每个变量都是一个 <code>float</code> 类型的<strong>标量</strong></li>
<li>每个结构体的成员变量是连续存储的，即：<code>a1 b1 a2 b2 a3 b3 ...</code></li>
</ul>
<h3 id="soa"><a class="markdownIt-Anchor" href="#soa"></a> SoA</h3>
<ul>
<li><code>SoA</code> 是将多个结构体的成员变量存储在一个数组中，每个成员变量是连续存储的</li>
<li>例如：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SoAStruct</span> &#123;</span></span><br><span class="line">  <span class="keyword">float</span> a[SIZE];</span><br><span class="line">  <span class="keyword">float</span> b[SIZE];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, struct SoAStruct *res, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (i &lt; n)</span><br><span class="line">    res-&gt;a[i] = a[i] + b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>这里的 <code>res</code> 是一个 <strong>结构体</strong>，<code>a</code> 和 <code>b</code> 是结构体的成员变量，每个变量都是一个 <code>float</code> 类型的<strong>数组</strong></li>
<li>每个成员变量的数组是连续存储的，即：<code>a1 a2 a3 ... b1 b2 b3 ...</code></li>
</ul>
<h2 id="11-行主序和列主序"><a class="markdownIt-Anchor" href="#11-行主序和列主序"></a> 11. 行主序和列主序</h2>
<ul>
<li><code>行主序</code>（Row Major Order）和 <code>列主序</code>（Column Major Order）是两种不同的数组存储方式</li>
<li><code>行主序</code> 是将数组的每一行存储在连续的内存中，<code>列主序</code> 是将数组的每一列存储在连续的内存中</li>
<li>例如：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">3</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">    &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;,</span><br><span class="line">    &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;,</span><br><span class="line">    &#123;<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>&#125;&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>行主序</code> 存储方式是：<code>1 2 3 4 5 6 7 8 9 10 11 12</code></li>
<li><code>列主序</code> 存储方式是：<code>1 5 9 2 6 10 3 7 11 4 8 12</code></li>
<li>默认情况下，<code>C / C++ / CUDA</code> 语言是 <code>行主序</code> 存储方式</li>
<li>在行主序存储下，如果按行序访问数组元素，访问效率会更高，因为连续的内存访问会提高缓存命中率，反之如果按列序访问数组元素，访问效率会更低</li>
</ul>
<h3 id="速度对比"><a class="markdownIt-Anchor" href="#速度对比"></a> 速度对比</h3>
<p>WIP</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/17/Transformers-without-Normalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/17/Transformers-without-Normalization/" class="post-title-link" itemprop="url">Transformers without Normalization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-17 13:16:18" itemprop="dateCreated datePublished" datetime="2025-03-17T13:16:18+08:00">2025-03-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/17/Transformers-without-Normalization/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/17/Transformers-without-Normalization/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.10622">https://arxiv.org/pdf/2503.10622</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/jiachenzhu/DyT">https://github.com/jiachenzhu/DyT</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>这是由恺明和杨立昆提出的一篇关于 <code>transformer</code> 算子优化的论文，主要观点是去掉 <code>transformer</code> 结构中的 <code>normalization</code> 层，改成 <code>tanh</code> 层</li>
<li>改用 <code>tanh</code> 算子的 <code>transformer</code> 模型，在大多数任务上可达到使用归一化层的模型相同的性能，甚至更好</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<p><img src="https://s2.loli.net/2025/03/18/hYMbytL1A3ZU8nO.png" alt="dyt.png" /></p>
<ul>
<li>简单来说，这篇论文的核心思想是将 <code>transformer</code> 中的 <code>normalization</code> 层（可以是 <code>LayerNorm</code> 或 <code>RMSNorm</code>）替换成 <code>dynamic tanh</code> 层（简称 <code>DyT</code>）</li>
<li><code>normalization</code> 计算公式：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>normalization</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mo>×</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\text{normalization}(x) = \gamma \times \frac{x - \mu}{\sqrt{\sigma^2+\epsilon}} + \beta
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">normalization</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.1903300000000003em;vertical-align:-0.93em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.196611em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.913389em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.873389em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.12661100000000003em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span></p>
<blockquote>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">μ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 分别是 <code>mean</code> 和 <code>std</code>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是 <code>scale</code> 和 <code>shift</code> 参数</p>
</blockquote>
<ul>
<li><code>DyT</code> 计算公式：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>DyT</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mo>×</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\text{DyT}(x) = \gamma \times \tanh(\alpha x) + \beta
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">DyT</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span></p>
<blockquote>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 是个可学习参数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是 <code>scale</code> 和 <code>shift</code> 参数（和 <code>normalization</code> 一样）</p>
</blockquote>
<ul>
<li><code>DyT</code> 实现伪代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input x has the shape of [B, T, C]</span></span><br><span class="line"><span class="comment"># B: batch size, T: tokens, C: dimension</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DyT</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, C, init_α</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.α = Parameter(ones(<span class="number">1</span>) * init_α)</span><br><span class="line">        self.γ = Parameter(ones(C))</span><br><span class="line">        self.β = Parameter(zeros(C))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = tanh(self.alpha * x)</span><br><span class="line">        <span class="keyword">return</span> self.γ * x + self.β</span><br></pre></td></tr></table></figure>
<blockquote>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 默认初始化值为 <code>0.5</code></p>
</blockquote>
<h2 id="results"><a class="markdownIt-Anchor" href="#results"></a> Results</h2>
<ul>
<li>作者在多个领域的知名模型上都对比了修改前后训练精度，<code>DyT</code> 的性能和 <code>normalization</code> 的性能基本一致，打的有来有回<br />
<img src="https://s2.loli.net/2025/03/18/DGAE6nMyocKstev.png" alt="dyt2.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Ul9eYLW6zrM43qv.png" alt="dyt3.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Bf1q5NszOU96Pbv.png" alt="dyt4.png" /><br />
<img src="https://s2.loli.net/2025/03/18/lQyRxsAoWcdIbHt.png" alt="dyt5.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Xn1SF2praD5HwRe.png" alt="dyt6.png" /><br />
<img src="https://s2.loli.net/2025/03/18/OleUF7P9XQJKxy4.png" alt="dyt1.png" /></li>
<li>作者还对比了 <code>DyT</code> 和 <code>normalization</code> 的训练/推理速度，<code>DyT</code> 的训练/推理速度要快很多<br />
<img src="https://s2.loli.net/2025/03/18/lVu9axTNIhgjpKB.png" alt="dyt7.png" /></li>
<li>作者同时做 <code>tanh</code> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 做了消融实验，发现 <code>tanh</code> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 都是必要的<br />
<img src="https://s2.loli.net/2025/03/18/v3zIDnFKW6OwV4Z.png" alt="dyt8.png" /><br />
<img src="https://s2.loli.net/2025/03/18/Y6dWnqjsD3PZANt.png" alt="dyt9.png" /></li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>属于是恺明和立昆的梦幻联动了…，这种对最火的结构的优化，非大佬不能为也，想象下如果这篇论文是大学实验室发表的，大家第一反应恐怕是：Who think you are? 😂</li>
<li>之前算是稍微接触过硬件，<code>DyT</code> 这种 <code>element-wise op</code> 比 <code>normalization</code> 这种 <code>reduce op</code> 一定快多了，想怎么 <code>tiling</code> 都行…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第五弹 —— 3FS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-14 11:09:25" itemprop="dateCreated datePublished" datetime="2025-03-14T11:09:25+08:00">2025-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/file-system/" itemprop="url" rel="index"><span itemprop="name">file-system</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/14/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%94%E5%BC%B9-%E2%80%94%E2%80%94-3FS/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/3FS">https://github.com/deepseek-ai/3FS</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>3FS (Fire-Flyer File System)</code> 是一个高性能的分布式文件系统，旨在提供低延迟和高吞吐量的存储解决方案，利用现代 <code>SSD</code> 和 <code>RDMA</code> 网络带全宽的并行文件系统，解决 <code>AI</code> 训练和推理存储问题</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="效果"><a class="markdownIt-Anchor" href="#效果"></a> 效果</h3>
<ul>
<li><strong>集群高吞吐</strong>：在 <code>180</code> 节点集群中，<code>3FS</code> 实现了高达 <code>6.6 TiB/s</code> 的聚合读取吞吐量</li>
<li><strong>基准测试优异</strong>：在 <code>25</code> 节点集群的 <code>GraySort</code> 基准测试中，<code>3FS</code> 达到了 <code>3.66 TiB /min</code> 的吞吐量</li>
<li><strong>单节点高性能</strong>：每个客户端节点的 <code>KVCache</code> 查找峰值吞吐量超过 <code>40 GiB/s</code></li>
<li><strong>架构先进</strong>： <code>3FS</code> 采用去中心化架构，并具备强一致性语义</li>
</ul>
<h3 id="系统介绍"><a class="markdownIt-Anchor" href="#系统介绍"></a> 系统介绍</h3>
<ul>
<li>还是看大佬讲吧，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27317616324">传送门</a></li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>这种底层架构一般只有大厂可以做，<code>deepseek</code> 有点东西</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第四弹 —— DualPipe ＆ EPLB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-12 11:35:53" itemprop="dateCreated datePublished" datetime="2025-03-12T11:35:53+08:00">2025-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/12/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E5%9B%9B%E5%BC%B9-%E2%80%94%E2%80%94-DualPipe-%EF%BC%86-EPLB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>DualPipe code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DualPipe">https://github.com/deepseek-ai/DualPipe</a></li>
<li>EPLB code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/EPLB">https://github.com/deepseek-ai/EPLB</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>DualPipe</code> 是 <code>deepseek</code> 提出的一种 <strong>流水线并行算法</strong>，和之间读过的 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/">GPipe</a> 和 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/">PipeDream</a> 类似，但 <code>DualPipe</code> 的硬件利用率更高，空泡更少</li>
<li><code>DPLB (Expert Parallelism Load Balancer)</code> 是一种 <strong>专家并行负载均衡算法</strong></li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="dualpipe"><a class="markdownIt-Anchor" href="#dualpipe"></a> DualPipe</h3>
<ul>
<li><code>DualPipe</code> 的计算过程<br />
<img src="https://s2.loli.net/2025/05/29/8kwaE7BPzYmGJiK.png" alt="dualpipe.png" /></li>
<li><code>DualPipe-v</code> 的计算过程<br />
<img src="https://s2.loli.net/2025/05/29/lK83UM7YyWsZbDO.png" alt="dualpipev.png" /></li>
</ul>
<blockquote>
<p>上图是民间自己二创的 dualpipe-v 的计算过程，将 dualpipe 对半切开后效果更好，<a target="_blank" rel="noopener" href="https://hackmd.io/@ufotalent/r1lVXsa9Jg">博客地址</a></p>
</blockquote>
<h4 id="dualpipe-核心特点"><a class="markdownIt-Anchor" href="#dualpipe-核心特点"></a> <code>DualPipe</code> 核心特点</h4>
<ol>
<li><strong>计算与通信重叠</strong>：<code>DualPipe</code> 的设计目标是最大化集群设备的计算性能，通过在<code>Forward</code> 和 <code>Backward</code> 阶段实现计算与通信的完全重叠，显著减少传统流水线并行中的 “空泡”（<code>Pipeline Bubble</code>，即空闲等待时间）。这对于需要跨节点协作的专家并行（<code>Expert Parallelism</code>）场景尤为重要。</li>
<li><strong>双向调度</strong>：与传统的单向流水线并行不同，<code>DualPipe</code> 采用双向调度策略，从流水线的两端同时输入微批次（<code>Micro-batches</code>），充分利用硬件资源。这种方法在保持计算通信比例恒定的情况下，即使模型规模进一步扩大，也能维持接近零的通信开销。</li>
<li><strong>高效扩展性</strong>：<code>DualPipe</code> 针对跨节点的混合专家模型（<code>MoE</code>）进行了优化，通过减少通信瓶颈，使得大规模分布式训练能够在相对有限的硬件资源（如 <code>H800 GPU</code>）上高效运行。</li>
<li><strong>显存优化</strong>：<code>DualPipe</code> 将模型的最浅层（包括嵌入层）和最深层（包括输出层）部署在同一流水线级别（<code>PP Rank</code>），实现参数和梯度的物理共享，进一步提升内存效率。这种设计减少了高代价的张量并行（<code>Tensor Parallelism</code>）需求。</li>
</ol>
<h3 id="eplb"><a class="markdownIt-Anchor" href="#eplb"></a> EPLB</h3>
<ul>
<li><code>EPLB (Expert Parallelism Load Balancer)</code> 是一种专家并行负载均衡算法，旨在解决专家并行中的负载不均问题</li>
<li>很简单，仅有 <code>160</code> 行 <code>python</code> 代码</li>
<li>核心思想是预估每个专家的负载，并根据负载设置专家拷贝和放置计划</li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li><code>DualPipe</code> 实际上是 <code>deepseek</code> 根据 <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/profile-data"><code>profile-data</code></a> 分析空泡后做的一种流水线并行算法，而且用其强大的工程能力实现了 <code>SMs</code> 通信耗时降低（实际上就是用 <code>PTX</code> 编程把一部分 <code>SM</code> 当做是全职的数据搬运工），这太 <s>crazy</s> 了</li>
<li><code>Transformer</code> 是一种重 <code>IO</code> 轻计算的架构，在 <code>Hopper</code> 硬件架构上，不改变 <code>SM</code> 是不可能做到通信和计算完全重叠的，所以 <code>deepseek</code> 做了非常底层的优化</li>
<li><code>EPLB</code> 作为一种专家并行负载均衡算法，虽然简单，但在实际应用中可以显著提升专家并行的效率</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第三弹 —— DeepGEMM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-10 11:51:21" itemprop="dateCreated datePublished" datetime="2025-03-10T11:51:21+08:00">2025-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/10/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%89%E5%BC%B9-%E2%80%94%E2%80%94-DeepGEMM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepGEMM">https://github.com/deepseek-ai/DeepGEMM</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>DeepGEMM</code> 是一个简单但功能强大的 <code>Hopper GPU （H100/H800）</code> 矩阵计算加速库</li>
<li>包含大约 <code>300</code> 行核心代码，可以做到在绝大多数大小的矩阵乘法均优于专家调优的内核，<code>hopper GPU</code> 上最高可达 <code>1350+ FP8 TFLOPS</code></li>
<li>完全即时编译，没有过多依赖，就像教程一样简洁，支持 <code>dense</code> 和 <code>moe</code> 架构</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<ul>
<li>这 <code>HPC</code> 相关的内容对于我确实超纲了，<code>CPU</code> 快给我干烧了</li>
<li>还是看大佬的讲解吧，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26437292382">传送门走你</a></li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li><code>deepseek</code> 牛逼，为 <code>LLM</code> 平权做了不可磨灭的贡献</li>
<li>而且如此技术信仰，是算法工程师应有的样子，打 call</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%8C%E5%BC%B9-%E2%80%94%E2%80%94-DeepEP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%8C%E5%BC%B9-%E2%80%94%E2%80%94-DeepEP/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第二弹 —— DeepEP</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-06 16:27:24" itemprop="dateCreated datePublished" datetime="2025-03-06T16:27:24+08:00">2025-03-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%8C%E5%BC%B9-%E2%80%94%E2%80%94-DeepEP/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%BA%8C%E5%BC%B9-%E2%80%94%E2%80%94-DeepEP/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepEP">https://github.com/deepseek-ai/DeepEP</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>目前大模型常用的三种并行：
<ul>
<li><code>DP</code>: <code>Data Parallelism</code>，数据并行，将数据分成多份，每个 <code>GPU</code> 处理一份数据</li>
<li><code>PP</code>: <code>Pipeline Parallelism</code>，管道并行，将模型分成多个阶段（连续一层或多层为一个阶段），每个 <code>GPU</code> 处理一个阶段</li>
<li><code>TP</code>: <code>Tensor Parallelism</code>，张量并行，将模型分成多份（通常是一层/一个算子/一个张量分成多份，主要解决超长序列引起的超大张量问题），每个 <code>GPU</code> 处理一部分张量</li>
</ul>
</li>
<li>对于 <code>MoE</code> 结构，<code>EP (Expert Parallelism)</code> 是一种新的并行策略，将 <code>MoE</code> 中的 <code>Expert</code> 分配到不同的 <code>GPU</code> 上</li>
<li><code>DeepEP</code> 是一个 用 <code>cuda</code> 实现的 <code>MoE</code> 模型的并行库，重点在于对 <code>All-to-All</code> 通信的优化</li>
</ul>
<h2 id="背景知识"><a class="markdownIt-Anchor" href="#背景知识"></a> 背景知识</h2>
<h3 id="节点内通信"><a class="markdownIt-Anchor" href="#节点内通信"></a> 节点内通信</h3>
<ul>
<li>通俗讲：一台服务器被称为一个节点，一个节点上的多个 <code>GPU</code> 之间的通信被称为节点内通信</li>
<li>通信协议：
<ul>
<li><code>PCIe</code>：比较通用的通信协议，目前最新的 <code>PCIe 6.0</code> 的带宽为 <code>32GB/s</code> 的双向带宽</li>
<li><code>NVLink</code>：<code>NVIDIA</code> 自家的通信协议，目前最新的 <code>NVLink 5.0</code> 的带宽约为 <code>800GB/s</code> 的双向带宽</li>
</ul>
</li>
</ul>
<h3 id="节点间通信"><a class="markdownIt-Anchor" href="#节点间通信"></a> 节点间通信</h3>
<ul>
<li>多台服务器组成一个集群，集群中的服务器之间的通信被称为节点间通信</li>
<li>通信协议：
<ul>
<li><code>InfiniBand</code>：<code>HPC</code> 领域常用的通信协议，目前最新的 <code>InfiniBand NDR</code> 可达 <code>400Gbps</code> 的双向带宽</li>
<li><code>Ethernet</code>：通用的通信协议，速度低于 <code>InfiniBand</code></li>
</ul>
</li>
<li>通信技术：
<ul>
<li><code>RDMA</code>：<code>Remote Direct Memory Access</code>，远程直接内存访问，<code>RDMA</code> 通过 <code>DMA</code> 直接访问远程内存，减少了 <code>CPU</code> 的参与，提高了通信效率</li>
</ul>
</li>
</ul>
<h3 id="通信视角下-moe-结构的特殊性"><a class="markdownIt-Anchor" href="#通信视角下-moe-结构的特殊性"></a> 通信视角下 MoE 结构的特殊性</h3>
<ul>
<li><code>MoE</code> 结构的本质是超大规模参数量 + 小规模激活参数量（实际计算量）来让模型更强大同时推理效率高</li>
<li>由于 <code>MoE</code> 结构在实际推理过程中，每个 <code>token</code> 激活的专家 <code>id</code> 是无法提前预测的，而是一个纯 <code>runtime</code> 的行为</li>
<li>对此，为了降低 <code>EP</code> 通信压力，<code>MoE</code> 结构通常会限制每个 <code>token</code> 实际激活的节点数量。例如，<code>DeepSeek V3</code> 有 <code>1</code> 个共享专家和 <code>256</code> 个路由专家，每个 <code>token</code> 会激活 <code>1</code> 个共享专家和 <code>8</code> 个路由专家，但同时限制最多只能激活 <code>4</code> 个节点，假如得分最高的 <code>8</code> 个路由专家来自超过 <code>4</code> 个节点，那么会牺牲部分高分专家，在节点数不超过 <code>4</code> 的情况下，用贪心算法选择得分最高的 <code>4</code> 个专家</li>
<li>虽然 <code>DP / TP / PP</code> 都存在通信问题，但都是可提前规划好的通信数据量和通信模式，只要调度得当，即可重叠计算和通信耗时，而 <code>MoE</code> 结构的通信是无法提前规划的，因此 <code>MoE</code> 结构的通信是最难优化的</li>
</ul>
<h2 id="关键特性和能力"><a class="markdownIt-Anchor" href="#关键特性和能力"></a> 关键特性和能力</h2>
<p><code>DeepEP</code> 的关键特性和能力包括：</p>
<ul>
<li><strong>高吞吐量节点内通信</strong>：使用 <code>NVLink</code> 优化节点内所有到所有通信的内核，实现高达 <code>155 GB/s</code> 的带宽。</li>
<li><strong>高吞吐量节点间通信</strong>：使用 <code>RDMA</code> 实现高效的跨节点所有到所有通信，在不同的 <code>EP</code> 配置中保持大约 <code>45 GB/s</code> 的带宽。</li>
<li><strong>低延迟内核</strong>：专用推理解码内核，分发操作延迟低至 <code>163</code> 微秒，组合操作延迟低至 <code>318</code> 微秒。</li>
<li><strong>FP8 支持</strong>：原生支持低精度操作，包括 <code>FP8</code> 分发，与大型模型中量化趋势一致。</li>
<li><strong>灵活的 GPU 资源控制</strong>：可配置的 <code>SM</code> 使用，用于计算 - 通信重叠，允许精细调整性能优化。</li>
<li><strong>自适应路由支持</strong>：在低延迟内核中支持自适应路由，使复杂拓扑中的网络利用更高效。</li>
</ul>
<h2 id="技术实现"><a class="markdownIt-Anchor" href="#技术实现"></a> 技术实现</h2>
<p><code>DeepEP</code> 是用 <code>C++</code> 和 <code>CUDA</code> 组件实现的，并带有 <code>Python</code> 接口。实现包括几个关键组件：</p>
<ul>
<li><strong>缓冲管理</strong>：核心 <code>Buffer</code> 类管理 <code>NVLink</code> 和 <code>RDMA</code> 的通信缓冲区，处理内存分配和同步。</li>
<li><strong>通信内核</strong>：
<ul>
<li>训练和推理预填充的高吞吐量内核</li>
<li>推理解码的低延迟内核</li>
<li>支持节点内（<code>NVLink</code>）和节点间（<code>RDMA</code>）通信</li>
</ul>
</li>
<li><strong>事件管理</strong>：<code>EventOverlap</code> 类提供 <code>CUDA</code> 事件处理和计算 - 通信重叠的工具。</li>
<li><strong>分发和组合操作</strong>：
<ul>
<li><code>dispatch</code>：将令牌特征发送到跨 <code>GPU</code> 的对应专家</li>
<li><code>combine</code>：从专家收集处理后的特征并返回到原始位置</li>
</ul>
</li>
</ul>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>大模型，尤其是基座大模型，拼的是基建</li>
<li>大模型时代不会再出现小模型时代经常出现的 <strong>理论计算量低但实际很慢的算法</strong> 了，<code>GPU</code> 上快才是真的快，不光要考虑计算，存储 / 通信也同时需要认真考虑</li>
<li>软硬件 <code>co-design</code> 是未来趋势，<code>People who're serious about software should make their own hardware.</code> 这句名言的含金量还在上升</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%80%E5%BC%B9-%E2%80%94%E2%80%94-FlashMLA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%80%E5%BC%B9-%E2%80%94%E2%80%94-FlashMLA/" class="post-title-link" itemprop="url">2025.02 DeepSeek 开源周第一弹 —— FlashMLA</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-06 13:05:31" itemprop="dateCreated datePublished" datetime="2025-03-06T13:05:31+08:00">2025-03-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%80%E5%BC%B9-%E2%80%94%E2%80%94-FlashMLA/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/06/2025-02-DeepSeek-%E5%BC%80%E6%BA%90%E5%91%A8%E7%AC%AC%E4%B8%80%E5%BC%B9-%E2%80%94%E2%80%94-FlashMLA/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/FlashMLA">https://github.com/deepseek-ai/FlashMLA</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li><code>FlashMLA</code> 是针对 <code>DeepSeek</code> 提出的 <code>MLA (Multi-head Latent Attention)</code> 模块，在 <code>Nvidia Hopper</code> 架构 <code>GPU</code> 上的加速算法，尤其对边长序列推理做了优化</li>
<li><code>FlashMLA</code> 和 <code>MLA</code> 的关系大概可以类比到 <code>FlashAttention</code> 和 <code>Attention</code> 的关系</li>
</ul>
<h2 id="关键特性和能力"><a class="markdownIt-Anchor" href="#关键特性和能力"></a> 关键特性和能力</h2>
<ol>
<li><strong>仅对 <code>Hopper (sm_90)</code> 架构做优化</strong>：充分挖掘了硬件 (<code>sm</code>) 的计算能力和 <code>Memory Hierachy</code> 的 存储 / <code>IO</code> 能力</li>
<li><strong>支持可变长度序列</strong>：和现实世界中推理场景更贴合</li>
<li><strong>分页 KV cache</strong>：使用 <code>block size = 64</code> 的分页存储（这里的 <code>64</code> 的含义是：一个 <code>block</code> 存储某一层某个头的连续 <code>64</code> 个 <code>token</code> 对应的 <code>kv cache</code>）</li>
<li><strong>高性能带宽和算力</strong>：在 <code>H800 SXM5</code> 设备上，在内存带宽受限配置环境下，内存带宽可达 <code>3000 GB/s</code>，在算力受限配置下，算力可达 <code>580 TFLOPS</code></li>
<li><strong>支持多种精度</strong>：支持 <code>BF16</code> 和 <code>FP16</code></li>
</ol>
<h2 id="技术实现"><a class="markdownIt-Anchor" href="#技术实现"></a> 技术实现</h2>
<ol>
<li>基于 <code>Nvidia cutlass</code> 库 + <code>cuda</code> 实现主要计算</li>
<li>用干净的 <code>python API</code> 包装，方便集成到 <code>PyTorch-base</code> 框架中</li>
<li>用元数据管理的方式支持变长序列</li>
</ol>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>做大模型不懂 <code>cuda</code> 是不行了，<code>cutlass</code> 要开始学起来了…</li>
<li><code>FlashMLA</code> 只在 <code>sm_90</code> 上实现了，其他显卡编译不通，<s><code>DeepSeek</code> 只打高端局</s></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2025/02/20/Native-Sparse-Attention-Hardware-Aligned-and-Natively-Trainable-Sparse-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/20/Native-Sparse-Attention-Hardware-Aligned-and-Natively-Trainable-Sparse-Attention/" class="post-title-link" itemprop="url">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-20 15:44:48" itemprop="dateCreated datePublished" datetime="2025-02-20T15:44:48+08:00">2025-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-10-23 13:32:17" itemprop="dateModified" datetime="2025-10-23T13:32:17+08:00">2025-10-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2025/02/20/Native-Sparse-Attention-Hardware-Aligned-and-Natively-Trainable-Sparse-Attention/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/02/20/Native-Sparse-Attention-Hardware-Aligned-and-Natively-Trainable-Sparse-Attention/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2502.11089">https://arxiv.org/pdf/2502.11089</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出了一种新的稀疏注意力机制，称为 <code>Native Sparse Attention</code>，该机制在硬件上对齐，并且可以直接训练，而无需额外的稀疏化技术</li>
<li>目前没有开源代码，只能通过论文中的公式和描述来实现</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="总体流程"><a class="markdownIt-Anchor" href="#总体流程"></a> 总体流程</h3>
<p><img src="https://s2.loli.net/2025/02/21/o5uIEHCaKlJA2ft.png" alt="nsa.png" /></p>
<ul>
<li>本质上是用不同的 <code>pattern</code> 组合来替代 <code>full attention</code>，以减少计算量</li>
</ul>
<h3 id="代码实现"><a class="markdownIt-Anchor" href="#代码实现"></a> 代码实现</h3>
<ul>
<li>由 <code>DeepSeek-R1</code> 根据论文中的公式和描述实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NSAModule</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        d_model,</span></span></span><br><span class="line"><span class="params"><span class="function">        n_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">        compress_block=<span class="number">32</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        select_block=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        select_topk=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        window_size=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.head_dim = d_model // n_heads</span><br><span class="line">        <span class="comment"># 参数设置</span></span><br><span class="line">        self.compress_block = compress_block</span><br><span class="line">        self.select_block = select_block</span><br><span class="line">        self.select_topk = select_topk</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        <span class="comment"># 压缩用MLP</span></span><br><span class="line">        self.compress_mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(self.head_dim * compress_block, <span class="number">256</span>),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, self.head_dim),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 门控机制</span></span><br><span class="line">        self.gate_mlp = nn.Sequential(nn.Linear(d_model, <span class="number">3</span> * n_heads), nn.Sigmoid())</span><br><span class="line">        <span class="comment"># 投影层</span></span><br><span class="line">        self.q_proj = nn.Linear(d_model, d_model)</span><br><span class="line">        self.k_proj = nn.Linear(d_model, d_model)</span><br><span class="line">        self.v_proj = nn.Linear(d_model, d_model)</span><br><span class="line">        self.out_proj = nn.Linear(d_model, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compress_tokens</span>(<span class="params">self, k, v</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;压缩KV序列到块级别&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调整输入维度 (batch, seq_len, n_heads, head_dim)</span></span><br><span class="line">        b, t, nh, hd = k.shape  <span class="comment"># 修改为四维解包</span></span><br><span class="line">        block_size = self.compress_block</span><br><span class="line">        num_blocks = (t + block_size - <span class="number">1</span>) // block_size</span><br><span class="line">        pad_len = num_blocks * block_size - t</span><br><span class="line">        <span class="comment"># 填充并分块</span></span><br><span class="line">        k = F.pad(k, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, pad_len))  <span class="comment"># 添加头部维度的填充</span></span><br><span class="line">        v = F.pad(v, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, pad_len))</span><br><span class="line">        <span class="comment"># 调整维度: [batch, num_blocks, block_size, n_heads, head_dim]</span></span><br><span class="line">        k_blocks = k.view(b, num_blocks, block_size, nh, hd)</span><br><span class="line">        v_blocks = v.view(b, num_blocks, block_size, nh, hd)</span><br><span class="line">        <span class="comment"># 压缩处理 (保持头部分离)</span></span><br><span class="line">        k_compressed = self.compress_mlp(</span><br><span class="line">            k_blocks.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">        )  <span class="comment"># [b, num_blocks, nh, hd]</span></span><br><span class="line">        v_compressed = self.compress_mlp(v_blocks.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>).flatten(<span class="number">3</span>))</span><br><span class="line">        <span class="keyword">return</span> k_compressed, v_compressed</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_select_blocks</span>(<span class="params">self, q, k_compressed, v_compressed</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;基于注意力分数选择关键块&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 计算压缩注意力分数</span></span><br><span class="line">        scores = torch.einsum(<span class="string">&quot;bthd,bkhd-&gt;bthk&quot;</span>, q, k_compressed) / (</span><br><span class="line">            self.head_dim ** <span class="number">0.5</span></span><br><span class="line">        )</span><br><span class="line">        probs = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 选择topk块</span></span><br><span class="line">        topk_scores, topk_indices = torch.topk(</span><br><span class="line">            probs.mean(dim=<span class="number">2</span>), self.select_topk, dim=-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 收集选中的块</span></span><br><span class="line">        k_selected = torch.gather(</span><br><span class="line">            k_compressed,</span><br><span class="line">            <span class="number">1</span>,</span><br><span class="line">            topk_indices.unsqueeze(-<span class="number">1</span>).expand(-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, self.head_dim),</span><br><span class="line">        )</span><br><span class="line">        v_selected = torch.gather(</span><br><span class="line">            v_compressed,</span><br><span class="line">            <span class="number">1</span>,</span><br><span class="line">            topk_indices.unsqueeze(-<span class="number">1</span>).expand(-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, self.head_dim),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> k_selected, v_selected</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        b, t, d = x.shape</span><br><span class="line">        <span class="comment"># 投影QKV并保持四维结构</span></span><br><span class="line">        q = self.q_proj(x).view(b, t, self.n_heads, self.head_dim)</span><br><span class="line">        k = self.k_proj(x).view(b, t, self.n_heads, self.head_dim)</span><br><span class="line">        v = self.v_proj(x).view(b, t, self.n_heads, self.head_dim)</span><br><span class="line">        <span class="comment"># 压缩路径</span></span><br><span class="line">        k_compressed, v_compressed = self._compress_tokens(k, v)</span><br><span class="line">        <span class="comment"># 选择路径</span></span><br><span class="line">        k_selected, v_selected = self._select_blocks(q, k_compressed, v_compressed)</span><br><span class="line">        <span class="comment"># 滑动窗口</span></span><br><span class="line">        k_window = k[:, <span class="built_in">max</span>(<span class="number">0</span>, t - self.window_size) :]</span><br><span class="line">        v_window = v[:, <span class="built_in">max</span>(<span class="number">0</span>, t - self.window_size) :]</span><br><span class="line">        <span class="comment"># 门控权重</span></span><br><span class="line">        gate = self.gate_mlp(x).view(b, t, <span class="number">3</span>, self.n_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 三路注意力计算</span></span><br><span class="line">        attn_outputs = []</span><br><span class="line">        <span class="keyword">for</span> branch_k, branch_v <span class="keyword">in</span> [</span><br><span class="line">            (k_compressed, v_compressed),</span><br><span class="line">            (k_selected, v_selected),</span><br><span class="line">            (k_window, v_window),</span><br><span class="line">        ]:</span><br><span class="line">            scores = torch.einsum(<span class="string">&quot;bthd,bkhd-&gt;bthk&quot;</span>, q, branch_k) / (</span><br><span class="line">                self.head_dim ** <span class="number">0.5</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                scores = scores.masked_fill(attn_mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">            probs = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">            probs = self.dropout(probs)</span><br><span class="line">            output = torch.einsum(<span class="string">&quot;bthk,bkhd-&gt;bthd&quot;</span>, probs, branch_v)</span><br><span class="line">            attn_outputs.append(output)</span><br><span class="line">        <span class="comment"># 门控融合</span></span><br><span class="line">        weighted = <span class="built_in">sum</span>(g * o <span class="keyword">for</span> g, o <span class="keyword">in</span> <span class="built_in">zip</span>(gate, attn_outputs))</span><br><span class="line">        output = weighted.contiguous().view(b, t, d)</span><br><span class="line">        <span class="keyword">return</span> self.out_proj(output)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_nsa_attention_shapes</span>():</span></span><br><span class="line">    <span class="comment"># 测试参数</span></span><br><span class="line">    batch_size = <span class="number">2</span></span><br><span class="line">    seq_len = <span class="number">128</span></span><br><span class="line">    d_model = <span class="number">256</span></span><br><span class="line">    n_heads = <span class="number">8</span></span><br><span class="line">    nsa_attn = NSAModule(d_model, n_heads)</span><br><span class="line">    x = torch.randn(batch_size, seq_len, d_model)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;输入形状:&quot;</span>, x.shape)</span><br><span class="line">    <span class="comment"># 打印中间形状</span></span><br><span class="line">    q = nsa_attn.q_proj(x).view(batch_size, seq_len, n_heads, -<span class="number">1</span>)</span><br><span class="line">    k = nsa_attn.k_proj(x).view(batch_size, seq_len, n_heads, -<span class="number">1</span>)</span><br><span class="line">    v = nsa_attn.v_proj(x).view(batch_size, seq_len, n_heads, -<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nQ/K/V形状:&quot;</span>, q.shape)  <span class="comment"># 应该输出 [2, 128, 8, 32]</span></span><br><span class="line">    k_comp, v_comp = nsa_attn._compress_tokens(k, v)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;压缩后KV形状:&quot;</span>, k_comp.shape)  <span class="comment"># [2, 4, 8, 32]</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    test_nsa_attention_shapes()</span><br></pre></td></tr></table></figure>
<h2 id="thoughts"><a class="markdownIt-Anchor" href="#thoughts"></a> Thoughts</h2>
<ul>
<li>速度快并不惊讶，但效果竟然比 <code>Full Attention</code> 还好，如果实验比较 <code>solid</code>，那么 <code>NSA</code> 确实比较有前途</li>
<li>由于没有开源代码，所以只能通过论文中的公式和描述来实现，这对于一些实验复现和工程应用来说是一个挑战</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhangzhe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">172</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">107</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhangzhe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js', () => {
    // 初始化 Mermaid 配置
    mermaid.initialize({
      theme    : 'dark',  // 设置主题
      logLevel : 3,  // 设置日志等级
      flowchart: { curve: 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 },
      themeVariables: {
        'fontFamily': 'Microsoft YaHei, Arial, sans-serif',  // 设置中文字体
      }
    });

    // 初始化 Mermaid 图表
    mermaid.init(undefined, document.querySelectorAll('pre.mermaid'));
  }, window.mermaid);
}
</script>


  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">


  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'eK3W25jybCO5jVUrYBBpAPqM-gzGzoHsz',
      appKey     : 'F4KVyUj9wHI5c80Bhz7O2uhq',
      placeholder: "说点什么再走吧...",
      avatar     : 'hide',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
