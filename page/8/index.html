<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"real-zhangzhe.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhangzhe&#39;s Blog">
<meta property="og:url" content="https://real-zhangzhe.github.io/page/8/index.html">
<meta property="og:site_name" content="Zhangzhe&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhangzhe">
<meta property="article:tag" content="No mistakes in the tango, not like life.">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://real-zhangzhe.github.io/page/8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Zhangzhe's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhangzhe's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The projection of my life.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/archives/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/21/ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/21/ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/" class="post-title-link" itemprop="url">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-21 17:01:35" itemprop="dateCreated datePublished" datetime="2024-08-21T17:01:35+08:00">2024-08-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Parallelism/" itemprop="url" rel="index"><span itemprop="name">Data Parallelism</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/21/ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/21/ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.02054">https://arxiv.org/pdf/1910.02054</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>传统的 <code>DDP</code> 训练大模型有个很大的问题是：由于 <code>DDP</code> 训练中每个 <code>GPU</code> 上都需要存储模型状态信息和其他状态信息：
<ul>
<li>模型状态信息包括：<code>Prameter</code> + <code>Gradient</code> + <code>Optimizer states</code></li>
<li>其他状态信息包括：激活值（<code>Activation</code>）、各种临时缓冲区（<code>buffer</code>）以及无法使用的显存碎片（<code>fragmentation</code>）<br />
所以显存占用较大，使得不能通过 <code>DDP</code> 训练较大的模型。</li>
</ul>
</li>
<li>本文提出来 <code>Zero-DP</code> 和 <code>Zero-R</code> 两种大模型训练优化策略，分别解决模型状态信息过大和其他状态信息过大的问题。</li>
<li><code>Zero-DP</code> 全称是 <code>Zero Redundancy Optimizer Data Parallelism</code>，主要优化点是将 <code>GPU</code> 间冗余存储的模型状态信息（<code>Parmeter + Gradient + Optimizer states</code>）删除，每个 <code>GPU</code> 只保留一小部分信息，所有 <code>GPU</code> 上的信息汇聚之后才是完整模型。</li>
<li><code>Zero-R</code> 的目的是解决 <code>Tensor Parallelism</code> 训练时，<code>Activation</code> 信息占用过大的问题。</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="zero-dp"><a class="markdownIt-Anchor" href="#zero-dp"></a> <code>ZeRO-DP</code></h3>
<h4 id="zero-dp-想要解决什么问题"><a class="markdownIt-Anchor" href="#zero-dp-想要解决什么问题"></a> <code>ZeRO-DP</code> 想要解决什么问题？</h4>
<ul>
<li>解决 <code>DDP</code> 训练过程中，<code>Parmeter + Gradient + Optimizer states</code> 对显存的占用过大，导致无法训练很大的模型的问题。</li>
</ul>
<h4 id="没有其他方法可以降低显存占用吗"><a class="markdownIt-Anchor" href="#没有其他方法可以降低显存占用吗"></a> 没有其他方法可以降低显存占用吗？</h4>
<ul>
<li>有的，比如混合精度训练 (<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.03740">https://arxiv.org/pdf/1710.03740</a>) 就是一个常用的有效的手段<br />
<img src="https://s2.loli.net/2024/08/21/ibqQzKWk4UvAPru.png" alt="mixed_precision_1.png" /></li>
<li>已知 <code>LLM</code> 通常使用 <code>Adam / AdamW</code> 优化器进行优化，假设模型参数量为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Φ</span></span></span></span>，那么用混合精度训练过程中，模型状态信息实际的显存占用是:
<ul>
<li><code>Parameter</code> 占用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">2\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">Φ</span></span></span></span> 字节（<code>fp16</code> 类型存储）</li>
<li><code>Gradient</code> 占用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">2\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">Φ</span></span></span></span> 字节（<code>fp16</code> 类型存储）</li>
<li><code>Adam Optimizer state</code> 占用：
<ul>
<li><code>fp32 parameter</code> 备份占用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">4\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">Φ</span></span></span></span> 字节</li>
<li><code>fp32 momentum</code> 一阶动量占用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">4\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">Φ</span></span></span></span> 字节</li>
<li><code>fp32 variance</code> 二阶动量占用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">4\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">Φ</span></span></span></span> 字节</li>
</ul>
</li>
<li>共计 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">16\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">6</span><span class="mord">Φ</span></span></span></span> 字节，其中 <code>Optimizer states</code> 占用 <code>75%</code>，因此 <code>Optimizer states</code> 是本文主要想解决的问题</li>
</ul>
</li>
</ul>
<h4 id="zero-dp-是如何解决模型状态信息占用过大问题的"><a class="markdownIt-Anchor" href="#zero-dp-是如何解决模型状态信息占用过大问题的"></a> <code>ZeRO-DP</code> 是如何解决模型状态信息占用过大问题的？</h4>
<p><img src="https://s2.loli.net/2024/08/26/QPTxZX7gnLeG8iF.png" alt="ZeRO-DP_1.png" /></p>
<ul>
<li><code>ZeRO-DP</code> 的核心思想是 <code>Data Parallelism</code> 时，每个模型只保留 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的模型状态信息，<code>N</code> 表示 <code>GPU</code> 数量，在需要信息聚合时使用 <code>Ring Reduce</code> 来聚合。</li>
<li>有三种程度的优化：
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 只删除冗余的优化器状态，所有优化器的信息加起来才是完整优化器状态。</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 删除冗余的优化器状态和梯度。</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi><mo>+</mo><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g+p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 删除冗余优化器状态、梯度和权重。</li>
</ul>
</li>
</ul>
<h4 id="分块存储-ring-reduce-会带来额外的通信代价吗"><a class="markdownIt-Anchor" href="#分块存储-ring-reduce-会带来额外的通信代价吗"></a> 分块存储 + <code>Ring Reduce</code> 会带来额外的通信代价吗？</h4>
<ul>
<li><code>All Reduce</code> 运算来计算设备间梯度均值（求和）<br />
<img src="https://s2.loli.net/2024/08/27/k3m7wPG82yEzgD1.png" alt="ZeRO-DP_2.png" /></li>
<li><code>Reduce Scatter</code> 来求和，<code>All Gather</code> 做 <code>broadcast</code> 同步到所有设备上<br />
<img src="https://s2.loli.net/2024/08/27/oRKD4MYkLa9z1UX.png" alt="ZeRO-DP_3.png" /></li>
<li>使用 <code>Ring Reduce</code> 实现 <code>Reduce Scatter</code><br />
<img src="https://s2.loli.net/2024/08/27/PrQFLkZAc7zRbOn.png" alt="ZeRO-DP_4.png" /></li>
</ul>
<blockquote>
<p>颜色逐渐累积代表梯度累加</p>
</blockquote>
<ul>
<li>使用 <code>Ring Reduce</code> 实现 <code>All Gather</code> 做 <code>broadcast</code><br />
<img src="https://s2.loli.net/2024/08/27/4wOgZiHpSTVeWPJ.png" alt="ZeRO-DP_5.png" /></li>
</ul>
<blockquote>
<p>深色逐渐蔓延到所有设备，代表 <code>broadcast</code></p>
</blockquote>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 不会带来额外的通信代价，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi><mo>+</mo><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g+p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 会</li>
<li>证明：
<ul>
<li>传统 <code>Data Parallelism</code> 在计算梯度后，需要进行一次 <code>All Reduce</code> 来计算设备间的梯度均值，<code>All Reduce</code> 可分为 <code>Reduce Scatter</code> 和 <code>All Gather</code> 两步，每一步实际上都是一次 <code>Ring Reduce</code>
<ul>
<li><code>Reduce Scatter</code> 如图三所示，把所有设备梯度累加，分散到各个设备上</li>
<li><code>All Gather</code> 如图四所示，把分散到各个设备上的梯度累加值同步到所有设备上</li>
<li><code>Ring Reduce</code> 是一种理论最优通信算法，每个周期内每个设备的发送和接收都被占用，因此梯度总字节数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">2\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">Φ</span></span></span></span> (<code>fp16</code>)，由于做了两次 <code>Ring Reduce</code>，所以每个设备的通信总字节数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">4\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">Φ</span></span></span></span>（发送和接收算一次）</li>
</ul>
</li>
<li><code>ZeRO-DP</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 也需要通过 <code>All Reduce</code> 计算设备间梯度均值，也需要 <code>Reduce Scatter</code> 和 <code>All Gather</code> 两个步骤，也是通过两次 <code>Ring Reduce</code> 实现，具体做法和传统 <code>Data Parallelism</code> 没有什么区别，因此每个设备的通信总字节数也是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">4\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">Φ</span></span></span></span></li>
<li><code>ZeRO-DP</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi><mo>+</mo><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g+p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.25833100000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 在前向传播时，也需要用 <code>All Reduce</code> 计算得到全量 <code>Parameter</code>，这一步在传统 <code>Data Parallelism</code> 中并不需要，因此会有额外通信量</li>
</ul>
</li>
</ul>
<h3 id="zero-r"><a class="markdownIt-Anchor" href="#zero-r"></a> <code>ZeRO-R</code></h3>
<h4 id="zero-r-想要解决哪些问题"><a class="markdownIt-Anchor" href="#zero-r-想要解决哪些问题"></a> <code>ZeRO-R</code> 想要解决哪些问题？</h4>
<ol>
<li><code>Tensor Parallelism</code> 中 <code>Activation</code> 冗余存储问题</li>
<li>临时缓冲区占用问题</li>
<li>显存碎片导致无法使用的问题</li>
</ol>
<h4 id="zero-r-如何解决-tensor-parallelism-中-activation-冗余存储问题"><a class="markdownIt-Anchor" href="#zero-r-如何解决-tensor-parallelism-中-activation-冗余存储问题"></a> <code>ZeRO-R</code> 如何解决 <code>Tensor Parallelism</code> 中 <code>Activation</code> 冗余存储问题？</h4>
<ul>
<li>是通过激活值检查点分片存储（<code>Partitioned Activation Checkpointing</code>，简称 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">P_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）来解决 <code>Tensor Parallelism</code> 中 <code>Activation</code> 冗余存储问题。</li>
<li>其中的 <strong>分片</strong> 思想和 <code>ZeRO-DP</code> 是一样的，<code>N</code> 个设备每个设备只存储 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的信息，通过 <code>All Reduce</code> 进行信息聚合</li>
<li><strong>激活值检查点</strong> 思想是来自 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.06174">https://arxiv.org/pdf/1604.06174</a> 这篇论文，实际上是传统前向反向传播过程和 <code>ReCompute</code> 前向反向传播过程的折衷，也是空间和时间的折衷。</li>
<li>传统前向反向传播过程：存储每个中间算子的前向传播计算结果，用于反向传播计算梯度<br />
<img src="https://s2.loli.net/2024/08/28/4LqChjgEwIAfuRS.webp" alt="ZeRO-R_1.webp" /></li>
<li><code>ReCompute</code> 前向反向传播过程：不存储任何中间算子的前向计算结果，反向传播需要用时重新计算<br />
<img src="https://s2.loli.net/2024/08/28/ZasfEMvulenz5JF.webp" alt="ZeRO-R_2.webp" /></li>
<li><code>Activation Checkpointing</code> 前向反向传播过程：存储部分中间算子前向计算结果，反向传播需要用时，从拓扑上游最近的检查点开始重新计算<br />
<img src="https://s2.loli.net/2024/08/28/4Fvq1eHSyhnwMKr.webp" alt="ZeRO-R_3.webp" /></li>
</ul>
<h4 id="zero-r-如何解决临时缓存区空间占用问题"><a class="markdownIt-Anchor" href="#zero-r-如何解决临时缓存区空间占用问题"></a> <code>ZeRO-R</code> 如何解决临时缓存区空间占用问题？</h4>
<ul>
<li>通常情况下，临时缓存区是用来做数据聚合的，例如 <code>All Reduce</code> 操作，模型越大，需要的临时缓存区也越大。</li>
<li><code>ZeRO-R</code> 用了一个非常简单的策略：在模型大到一定程度后，使用固定尺寸的融合缓存区，即常数尺寸缓存区（<code>Constant Size Buffers</code>，简称 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">C_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）</li>
</ul>
<h4 id="zero-r-如何解决显存碎片化问题"><a class="markdownIt-Anchor" href="#zero-r-如何解决显存碎片化问题"></a> <code>ZeRO-R</code> 如何解决显存碎片化问题？</h4>
<ul>
<li>问：为什么会产生显存碎片化问题？</li>
<li>答：因为 <code>Activation Checkpointing</code> 的存在，不同的 <code>Activation</code> 有不同的生命周期，非 <code>checkpoint</code> 的 <code>Activation</code> 用完即弃，<code>checkpoint</code> 的 <code>Activation</code> 需要反向传播之后才可以丢弃，不同生命周期的存储空间交织就会出现显存碎片</li>
<li>问：显存碎片化会带来什么问题？</li>
<li>答：显存碎片化通常会有两个问题：
<ol>
<li>大量的碎片无法使用，使得实际空间占用不多，但出现 <code>OOM</code></li>
<li>大量存储碎片会导致系统在分配内存时需要大量的搜索来找到合适的空间，会带来额外耗时</li>
</ol>
</li>
<li>问：那 <code>ZeRO-R</code> 如何解决显存碎片化问题？</li>
<li>答：预先给不同生命周期的 <code>Activation</code> 划分不同的存储空间，避免产生交织。例如：把显存划分成两段连续的存储空间，其中一段存储 <code>Checkpoint Activation</code> 另外一段存储 <code>Temporary Activation</code>。</li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>当大模型时代到来，算力和存储不够用了，大家也开始审视之前做的东西是不是时间/空间低效的，是不是还能榨点油水。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/21/Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/21/Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism/" class="post-title-link" itemprop="url">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-21 14:00:47" itemprop="dateCreated datePublished" datetime="2024-08-21T14:00:47+08:00">2024-08-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tensor-Parallelism/" itemprop="url" rel="index"><span itemprop="name">Tensor Parallelism</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/21/Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/21/Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">https://arxiv.org/pdf/1909.08053</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文是英伟达 <code>Megatron-LM</code> 大模型分布式并行训练框架的一篇简介，大概阐述了 <code>Megatron-LM</code> 的机制和效果，对于实现细节着墨不多。</li>
<li>从代码实现角度看，<code>Megatron-LM</code> 是对 <code>PyTorch</code> 进行了二次封装。</li>
<li>基本思想是 <code>Tensor Parallelism</code>，拆分一层的权重到不同 <code>GPU</code> 上并行运算。</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="tensor-parallelism"><a class="markdownIt-Anchor" href="#tensor-parallelism"></a> <code>Tensor Parallelism</code></h3>
<p><img src="https://s2.loli.net/2024/08/21/BVFJTzMgYqiQNsX.png" alt="megatron_1.png" /></p>
<ul>
<li>重点是这里的 <code>f</code> 和 <code>g</code> 函数，下面是其实现：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">f</span>(<span class="params">torch.autograd.Function</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, gradient</span>):</span></span><br><span class="line">    all_reduce(gradient)</span><br><span class="line">    <span class="keyword">return</span> gradient</span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">g</span>(<span class="params">torch.autograd.Function</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, x</span>):</span></span><br><span class="line">    all_reduce(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, gradient</span>):</span></span><br><span class="line">    <span class="keyword">return</span> gradient</span><br></pre></td></tr></table></figure>
<ul>
<li>在本例子中 <code>All-reduce</code> 实际上是 <code>ReduceSum</code>，实际上聚合运算（包括 <code>forward and backward</code>）需要 <code>GPU</code> 间同步，存在通信代价</li>
<li><code>Self-Attention</code> 可以 <code>tensor parallelism</code> 的一个重要原因是 <code>softmax</code> 是逐行做的而不是全局做的<br />
<img src="https://s2.loli.net/2024/08/21/w8dOQ5JjAbiSPLG.png" alt="megatron-lm_2.png" /></li>
<li>由图 <code>3</code> 可以得出结论，一个 <code>transformer layer</code> 只有 <code>4</code> 个同步算子（两个 <code>LayerNorm</code> 和两个 <code>Dropout</code>），其他算子都可以被分解为并行算子</li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>给我的感觉有点类似 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2022/08/21/1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E6%A6%82%E8%BF%B0/"><code>MLC</code></a> 或者机器学习编译中的 <code>Tiling</code>（只是升级为训练及推理全流程 <code>tiling</code>）</li>
<li><code>tensor parallelism</code> 这种策略很考验对计算流程的抽象，除非有大量人力把框架搭建好且把所有算子实现写好，否则会很难用起来（当然对于 <code>Nvidia</code> 这些都不是事），突然想起了 <code>Neuwizard</code>，一声叹息…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/" class="post-title-link" itemprop="url">PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-20 13:02:53" itemprop="dateCreated datePublished" datetime="2024-08-20T13:02:53+08:00">2024-08-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Pipeline-Parallelism/" itemprop="url" rel="index"><span itemprop="name">Pipeline Parallelism</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/20/PipeDream-Fast-and-Efficient-Pipeline-Parallel-DNN-Training/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.03377">https://arxiv.org/pdf/1806.03377</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/msr-fiddle/pipedream%EF%BC%88%E6%B0%91%E9%97%B4%E5%AE%9E%E7%8E%B0%EF%BC%89">https://github.com/msr-fiddle/pipedream（民间实现）</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出一种和 <code>Gpipe</code> 类似的 <code>pipeline parallelism</code> 并行训练机制 <code>PipeDream</code>，和 <code>Gpipe</code> 可以任意划分模型不同，<code>PipeDream</code> 自动化划分网络，以实现负载均衡和最小通信开销。</li>
<li>与 <code>Gpipe</code> 的同步更新机制不同，<code>PipeDream</code> 采用异步参数更新机制。</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<p><img src="https://s2.loli.net/2024/08/20/1Dk5ewIaTBxpGKH.png" alt="PipeDream_1.png" /></p>
<blockquote>
<p><code>backward</code> 的过程实际同时包含了 <em>参数更新</em></p>
</blockquote>
<ul>
<li>与 <code>Gpipe</code> 使用的 <code>Micro-Batch</code> + 同步参数更新机制不同，<code>PipeDream</code> 使用异步更新机制，异步更新主要包含以下几点：</li>
</ul>
<h3 id="weigt-stashing"><a class="markdownIt-Anchor" href="#weigt-stashing"></a> <code>weigt stashing</code></h3>
<ol>
<li>从上图可以看出 <code>Machine 1</code> 在 <code>forward 5</code> 时，使用的是更新过 <code>1</code> 次的参数（<code>forward 5</code> 之前只做了 <code>backward 1</code>）；但当 <code>backward 5</code> 时，参数已经被更新了 <code>4</code> 次，如果直接使用会出现 <code>forward</code> 和 <code>backward</code> 参数不一致问题</li>
<li>因此，<code>PipeDream</code> 提出了 <strong><code>weight stashing</code> (权重存储)</strong>，即保存多组 <code>weights</code>，每次 <code>forward</code> 过程使用最新版本的 <code>weights</code>，当 <code>backward</code> 时，使用 <code>forward</code> 时刻对应的 <code>weights</code></li>
<li>使用 <code>weight stashing</code> 之前，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>γ</mi><mo>⋅</mo><mi mathvariant="normal">△</mi><mi>f</mi><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>w</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msubsup><mi>w</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w^{(t+1)} = w^{(t)}+\gamma \cdot \triangle f(w_1^{(t)},w_2^{(t)},...,w_n^{(t)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.63889em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.311108em;vertical-align:-0.26630799999999993em;"></span><span class="mord">△</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.5834080000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11659199999999997em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中：
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 表示 <code>learning rate</code></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span> 表示 <code>stage</code> 的数量，即 <code>GPU</code> 数量</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">w_i^{(t)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span> 表示 <code>weights of stage i after t mini-batch</code>，即当运行了 <code>t</code> 个 <code>mini-batch</code> 之后的 <code>stage i</code> 的权重</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">△</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\triangle f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">△</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> 表示梯度</li>
</ol>
</li>
<li>使用 <code>weight stashing</code> 之后，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>γ</mi><mo>⋅</mo><mi mathvariant="normal">△</mi><mi>f</mi><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>w</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>n</mi><mo>+</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msubsup><mi>w</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w^{(t+1)} = w^{(t)}+\gamma \cdot \triangle f(w_1^{(t-n+1)},w_2^{(t-n+2)},...,w_n^{(t)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.63889em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.311108em;vertical-align:-0.26630799999999993em;"></span><span class="mord">△</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.5834080000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11659199999999997em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
</ol>
<h3 id="vertical-sync"><a class="markdownIt-Anchor" href="#vertical-sync"></a> <code>vertical sync</code></h3>
<ol>
<li><code>weight stashing</code> 解决了 <code>forward</code> 和 <code>backward</code> 使用的 <code>weight</code> 版本不一致问题，但依然存在另外一个问题：<strong>同一个 <code>mini-batch</code> 在不同 <code>stage</code> 上 <code>forward</code> 使用的 <code>weight</code> 版本不一致</strong></li>
<li>例如，对于 <code>mini-batch 4</code>，在 <code>Machine 2</code> 上 <code>forward</code> 使用的参数版本是未更新版本，在 <code>Machine 3</code> 上 <code>forward</code> 使用的版本是更新过一次的版本</li>
<li>为解决这个问题，作者提出 <strong><code>vertical sync</code></strong> 概念，即：<strong>对于一个 <code>mini-batch</code> 在每个 <code>stage</code> 上 <code>forward</code> 都使用最旧版本的参数，<code>stage forward and backward</code> 结束后统一更新参数</strong></li>
<li><code>vertical sync</code> 应用之后，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>γ</mi><mi mathvariant="normal">△</mi><mi>f</mi><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>w</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msubsup><mi>w</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w^{(t+1)}=w^{(t)}+\gamma \triangle f(w_1^{(t-n+1)},w_2^{(t-n+1)},...,w_n^{(t-n+1)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.311108em;vertical-align:-0.26630799999999993em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord">△</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.5834080000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11659199999999997em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li>
<li>事实上实验证明，<code>weight stashing</code> 对训练精度提升明显，<code>vertical sync</code> 提升较小</li>
</ol>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>感觉 <code>PipeDream</code> 为了充分排流水，减小 <code>bubble</code>，没有等 <code>forward</code> 全部结束就开始 <code>backward</code>，然后发现和单 <code>GPU</code> 更新机制不等价，然后用了 <code>weight stashing</code> 和 <code>vertical sync</code> 去找补，做到等价</li>
<li>和 <code>Gpipe</code> 相比，灵活性更差，实现难度更高（需要缓存各个版本的 <code>weights</code>，听上去也是几倍的存储开销以及吞吐量）</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/" class="post-title-link" itemprop="url">GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-20 12:04:46" itemprop="dateCreated datePublished" datetime="2024-08-20T12:04:46+08:00">2024-08-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Pipeline-Parallelism/" itemprop="url" rel="index"><span itemprop="name">Pipeline Parallelism</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/20/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06965">https://arxiv.org/pdf/1811.06965</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py">https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>随着模型和数据量的增长，单 <code>GPU</code> 已经无法满足需求，所以需要多 <code>GPU</code> 并行</li>
<li><code>Data parallelism</code> 是最常用的，即将数据划分到多个 <code>GPU</code> 上，每个 <code>GPU</code> 单独前向反向传播，仅在梯度更新时聚合，例如 <code>pytorch ddp</code></li>
<li>本论文提出一种新的并行方式 <code>Pipeline parallelism</code>，这种方式是将模型划分为多段子图，每个设备上加载一段，各个子图直接串联</li>
<li>直接实现的 <code>Pipeline parallelism</code> 的 <code>GPU</code> 利用率较低，本文通过一些方法可大幅提高 <code>GPU</code> 利用率</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<p><img src="https://s2.loli.net/2024/08/20/XuV9cyDHBgK2nqf.png" alt="gpipe_1.png" /><br />
<code>Gpipe</code> 方案主要包含三个部分</p>
<h3 id="划分-stage"><a class="markdownIt-Anchor" href="#划分-stage"></a> 划分 <code>Stage</code></h3>
<ol>
<li>模型切成若干 <code>stage</code>，每个 <code>GPU</code> 只加载一段</li>
<li>每个 <code>stage</code> 串起来形成一个 <code>pipeline</code></li>
</ol>
<h3 id="划分-micro-batch"><a class="markdownIt-Anchor" href="#划分-micro-batch"></a> 划分 <code>Micro-Batch</code></h3>
<ol>
<li>传统模型训练过程中使用 <code>Mini-Batch</code>，<code>Mini-Batch</code> 在 <code>pipeline parallelism</code> 中会出现大量气泡，因此 <code>Gpipe</code> 提出 <code>Micro-Batch</code> 概念</li>
<li><code>Micro-Batch</code> 是将 <code>Mini-Batch</code> 再划分成多份，用于排计算流水，<code>Micro-Batch</code> 变成最小计算单元，有利于减小气泡，上图 <code>2.c</code> 中横向就是一个 <code>Mini-Batch</code> 划分得到的一组 <code>Micro-Batch</code></li>
<li>为了保证梯度更新和 <code>Mini-Batch</code> 的一致性，<code>Gpipe</code> 会将 <code>Micro-Batch</code> 梯度累积，在一个 <code>Mini-Batch</code> 的全部 <code>Micro-Batch</code> 计算结束后再更新（图 <code>2.c</code> 中一行只有一次 <code>update</code>）</li>
</ol>
<h3 id="重计算"><a class="markdownIt-Anchor" href="#重计算"></a> 重计算</h3>
<ol>
<li>传统模型训练过程中，计算的中间结果都需要保存，用于反向传播计算梯度，非常消耗显存，同时会导致 <code>pipeline parallelism</code> 的数据依赖问题，上图 <code>2.a</code> 中的横向连接</li>
<li>重计算是一种用计算换空间的操作，在前向传播过程中，丢弃 <code>stage</code> 内所有中间结果，只保留 <code>stage</code> 间中间结果；在反向传播时，<code>stage</code> 内中间结果会再做一次前向传播计算得到</li>
<li>重计算可大幅降低显存占用，使得可以放更大的 <code>batch size</code> 和更大的模型，且可以改善数据依赖问题</li>
</ol>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>这种方案前瞻性比较强，提出时还没有 <code>LLM</code>，且拥有和传统非并行训练在数学上一致的参数更新机制</li>
<li>给并行训练提供了一种新思路，非常灵活</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/15/SpecInfer-Accelerating-Large-Language-Model-Serving-with-Tree-based-Speculative-Inference-and-Verification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/15/SpecInfer-Accelerating-Large-Language-Model-Serving-with-Tree-based-Speculative-Inference-and-Verification/" class="post-title-link" itemprop="url">SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-15 12:25:11" itemprop="dateCreated datePublished" datetime="2024-08-15T12:25:11+08:00">2024-08-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/15/SpecInfer-Accelerating-Large-Language-Model-Serving-with-Tree-based-Speculative-Inference-and-Verification/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/15/SpecInfer-Accelerating-Large-Language-Model-Serving-with-Tree-based-Speculative-Inference-and-Verification/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.09781">https://arxiv.org/pdf/2305.09781</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/flexflow/FlexFlow/">https://github.com/flexflow/FlexFlow/</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文利用了 <code>Speculative decoding</code> 思想实现了一个 <code>SpecInfer</code> 大语言模型推理加速系统</li>
<li>和标准 <code>Speculative decoding</code> 使用 <code>seq</code> 组织预测的 <code>token</code> 不同，<code>SpecInfer</code> 使用 <code>tree</code> 数据结构组织预测的 <code>token</code></li>
<li><code>SpecInfer</code> 中的 <code>target</code> 模型可以并行验证整个 <code>token tree</code> 的正确性，而不是 <code>token seq</code>，且可以保证和原模型输出完全一致</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="增量式解码-序列式投机解码-树式投机解码三者对比"><a class="markdownIt-Anchor" href="#增量式解码-序列式投机解码-树式投机解码三者对比"></a> 增量式解码、序列式投机解码、树式投机解码三者对比</h3>
<p><img src="https://s2.loli.net/2024/08/15/98NPOk4gMia1dIy.png" alt="specinfer_1.png" /></p>
<blockquote>
<p>左图表示传统增量式推理 <code>LLM</code> 的解码方式<br />
右图从上到下依次表示：增量式解码、序列式投机解码、树式投机解码</p>
</blockquote>
<h3 id="候选树生成和验证流程"><a class="markdownIt-Anchor" href="#候选树生成和验证流程"></a> 候选树生成和验证流程</h3>
<p><img src="https://s2.loli.net/2024/08/15/eW9VJYnQv6bcuMA.png" alt="specinfer_2.png" /></p>
<blockquote>
<p><code>SSM (small speculative model)</code> 给出候选树<br />
<code>target model</code> 对候选树进行验证</p>
</blockquote>
<h3 id="如何对树式结构并行解码"><a class="markdownIt-Anchor" href="#如何对树式结构并行解码"></a> 如何对树式结构并行解码</h3>
<p><img src="https://s2.loli.net/2024/08/16/caHNM5xfVn2ZrPq.png" alt="specinfer_3.png" /></p>
<blockquote>
<p><strong>注意：这的 “并行” 是指可以一次解码一棵树而不需要将树深度优先搜索得到多组序列然后一一解码</strong></p>
</blockquote>
<ol>
<li>使用 <code>kv cache</code> 对解码过程进行加速是一个常见的方法，但树式结构无法直接使用 <code>kv cache</code>（因为要动态剪枝，如图左侧所示）</li>
<li>因此本文提出一种新颖的方法使得树式结构可以并行解码，这个方法包括两个改进：
<ol>
<li>对树进行 <strong>深度优先搜索</strong>，使得树变成一个序列</li>
<li>将传统的上三角 <code>attention mask</code> 变成一个 <code>topology-aware mask</code>，即通过 <code>mask</code> 来确定 <code>token</code> 两两之间的可见性（如图右侧所示），这种可见性包含：
<ol>
<li>每个 <code>token</code> 看不到之后的 <code>token</code>（矩阵下三角置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">∞</span></span></span></span>）</li>
<li>每个 <code>token</code> 看不到不在其拓扑路径上的 <code>token</code></li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="总体算法流程"><a class="markdownIt-Anchor" href="#总体算法流程"></a> 总体算法流程</h3>
<p><img src="https://s2.loli.net/2024/08/19/sOeLxDdgVC3QH4R.png" alt="specinfer_4.png" /></p>
<ol>
<li>先用 <code>small speculate model</code> 推理 <code>sequence</code> 得到候选树 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li>在使用 <code>LLM</code> 树式并行解码 <code>sequence</code> 得到大模型预测结果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi></mrow><annotation encoding="application/x-tex">O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span></span></span></span></li>
<li>用贪心算法或随机匹配算法对候选树 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 逐个节点验证（接受或拒绝）</li>
<li>重复直到 <code>&lt;EOS&gt;</code> 出现</li>
</ol>
<h3 id="贪心验证算法和随机验证算法"><a class="markdownIt-Anchor" href="#贪心验证算法和随机验证算法"></a> 贪心验证算法和随机验证算法</h3>
<p><img src="https://s2.loli.net/2024/08/19/xTEuNg3K6irh4Rz.png" alt="specinfer_5.png" /></p>
<ul>
<li>这里的两种算法和 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/">https://zhangzhe.space/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/</a> 里的流程基本一致，不一样的地方仅仅在于 <code>tree / sequence</code> 两种候选 <code>token</code> 的组织形式</li>
</ul>
<h4 id="贪心验证"><a class="markdownIt-Anchor" href="#贪心验证"></a> 贪心验证</h4>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span> 表示当前节点，从根节点开始广度优先遍历</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>v</mi></msub><mo>=</mo><mi>u</mi></mrow><annotation encoding="application/x-tex">p_v=u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span> 表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span> 的子节点</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>v</mi></msub><mo>=</mo><mi>O</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t_v=O(u)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mclose">)</span></span></span></span> 表示大模型和投机模型预测结果一致</li>
</ol>
<h4 id="随机验证"><a class="markdownIt-Anchor" href="#随机验证"></a> 随机验证</h4>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span> 表示当前节点，从根节点开始广度优先遍历</li>
<li>当在 <code>[0, 1]</code> 之间随机均匀采样得到的随机数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>&lt;</mo><mfrac><msub><mi>P</mi><mrow><mi>L</mi><mi>L</mi><mi>M</mi></mrow></msub><msub><mi>P</mi><mrow><mi>S</mi><mi>S</mi><mi>M</mi></mrow></msub></mfrac></mrow><annotation encoding="application/x-tex">r \lt \frac{P_{LLM}}{P_{SSM}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3339409999999998em;vertical-align:-0.44530499999999995em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8886359999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.13889em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.410305em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.13889em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44530499999999995em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 时，接受这个节点，否则拒绝</li>
</ol>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>这篇论文站在 <a target="_blank" rel="noopener" href="https://zhangzhe.space/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/"><code>speculate decoding</code></a> 的肩膀上，加入了树结构的候选列表，算是 <code>speculate decoding</code> 的超集</li>
<li><code>topology-aware mask</code> 和 <code>decoder attention mask</code> 结合的方法还是挺巧妙的</li>
<li>由于是树结构存储候选词，那么多个 <code>SSM</code> 是天然适配的，算是个工程角度很好用的 <code>trick</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/" class="post-title-link" itemprop="url">Fast Inference from Transformers via Speculative Decoding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-14 09:01:33" itemprop="dateCreated datePublished" datetime="2024-08-14T09:01:33+08:00">2024-08-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/14/Fast-Inference-from-Transformers-via-Speculative-Decoding/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.17192">https://arxiv.org/pdf/2211.17192</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/feifeibear/LLMSpeculativeSampling">https://github.com/feifeibear/LLMSpeculativeSampling</a> （民间实现）</li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出一种新颖的大模型推理加速的新范式，叫做投机解码，用一个参数量很小的近似模型预测得到一段序列，让大模型评判 <strong>接受</strong> 还是 <strong>拒绝</strong> 这段序列。</li>
<li>从数学上可以证明投机解码的结果和大模型逐个 <code>token</code> 预测的结果完全相同。</li>
<li>可以加速推理 <code>2 - 3</code> 倍，据说 <code>GPT-4</code> 使用了这种方法。</li>
<li>近似小模型的参数量一般要比大模型小 <code>2</code> 个数量级。<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mrow><mtext mathvariant="italic">generated</mtext><mtext> </mtext><mtext mathvariant="italic">tokens</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>1</mn><mo>−</mo><msup><mi>β</mi><mrow><mi>γ</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">E(\textit{generated tokens}) = \frac{1-\beta^{\gamma+1}}{1-\beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord text"><span class="mord textit">generated tokens</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.551136em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.070028em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05556em;">γ</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="算法流程"><a class="markdownIt-Anchor" href="#算法流程"></a> 算法流程</h3>
<ol>
<li>给定 <code>prefix_seq</code> 序列，用近似小模型（<code>approx model</code>， <code>M_q</code>）预测 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 长度的后续序列，记做 <code>predict_seq</code>，并记录 <code>predict_seq</code> 中每一个 <code>token</code> 的预测概率，记作 <code>q</code></li>
<li>大模型（<code>target model</code>，<code>M_p</code>）输入 <code>prefix_seq + predict_seq</code>，得到 <code>predict_seq</code> 每个 <code>token</code> 的概率（记作 <code>p</code>）和下一个 <code>token</code>（记作 <code>t</code>）</li>
<li>对于每一个 <code>token</code>，如果 <code>p &gt;= q</code>，则表示大模型接受小模型对这个 <code>token</code> 的提议；如果 <code>p &lt; q</code>，则表示 <strong>大模型以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>p</mi><mi>q</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{p}{q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.228608em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7475em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的概率接受这个 <code>token</code></strong>，用随机均匀采样来确定是否接受这个 <code>token</code></li>
<li>添加大模型连续接受的 <code>predict_seq</code> 到 <code>prefix_seq</code> 之后，如果 <code>predict_seq</code> 每个 <code>token</code> 都接受了，则 <code>prefix_seq = prefix_seq + predict_seq + t</code>。重复步骤 <code>1</code></li>
</ol>
<h3 id="代码实现"><a class="markdownIt-Anchor" href="#代码实现"></a> 代码实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">speculative_sampling</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    prefix: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    approx_model: torch.nn.Module,</span></span></span><br><span class="line"><span class="params"><span class="function">    target_model: torch.nn.Module,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_len: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    gamma: <span class="built_in">int</span> = <span class="number">4</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    temperature: <span class="built_in">float</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    top_k: <span class="built_in">int</span> = <span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    top_p: <span class="built_in">float</span> = <span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; torch.Tensor:</span></span><br><span class="line">    seq_len = prefix.shape[<span class="number">1</span>]</span><br><span class="line">    T = seq_len + max_len</span><br><span class="line">    approx_model_cache = KVCacheModel(approx_model, temperature, top_k, top_p)</span><br><span class="line">    target_model_cache = KVCacheModel(target_model, temperature, top_k, top_p)</span><br><span class="line">    resample_count = <span class="number">0</span></span><br><span class="line">    target_sample_count = <span class="number">0</span></span><br><span class="line">    accepted_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> prefix.shape[<span class="number">1</span>] &lt; T:</span><br><span class="line">        prefix_len = prefix.shape[<span class="number">1</span>]</span><br><span class="line">        x = approx_model_cache.generate(prefix, gamma)</span><br><span class="line">        _ = target_model_cache.generate(x, <span class="number">1</span>)</span><br><span class="line">        n = prefix_len + gamma - <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(gamma):</span><br><span class="line">            r = torch.rand(<span class="number">1</span>, device=target_model_cache.device)</span><br><span class="line">            j = x[:, prefix_len + i]</span><br><span class="line">            <span class="keyword">if</span> r &gt; (target_model_cache._prob_history[:, prefix_len + i - <span class="number">1</span>, j]) / (</span><br><span class="line">                approx_model_cache._prob_history[:, prefix_len + i - <span class="number">1</span>, j]</span><br><span class="line">            ):</span><br><span class="line">                <span class="comment"># reject</span></span><br><span class="line">                n = prefix_len + i - <span class="number">1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            accepted_count += <span class="number">1</span></span><br><span class="line">        prefix = x[:, : n + <span class="number">1</span>]</span><br><span class="line">        approx_model_cache.rollback(n + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> n &lt; prefix_len + gamma - <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># reject someone, sample from the pos n</span></span><br><span class="line">            t = sample(</span><br><span class="line">                max_fn(</span><br><span class="line">                    target_model_cache._prob_history[:, n, :]</span><br><span class="line">                    - approx_model_cache._prob_history[:, n, :]</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            resample_count += <span class="number">1</span></span><br><span class="line">            target_model_cache.rollback(n + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># all approx model decoding accepted</span></span><br><span class="line">            <span class="keyword">assert</span> n == target_model_cache._prob_history.shape[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">            t = sample(target_model_cache._prob_history[:, -<span class="number">1</span>, :])</span><br><span class="line">            target_sample_count += <span class="number">1</span></span><br><span class="line">        prefix = torch.cat((prefix, t), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> prefix</span><br></pre></td></tr></table></figure>
<h3 id="收益分析"><a class="markdownIt-Anchor" href="#收益分析"></a> 收益分析</h3>
<ul>
<li>大模型推理过程中，预填充阶段计算访存比较高，增量生成阶段计算访存比较低，因此增量生成阶段是主要优化方向</li>
<li>本论文的投机解码算法在某种程度上是将 <code>target model</code> 的增量生成阶段变成 <code>approx model</code> 的增量生成 + <code>target model</code> 的预填充，收益主要来自于这里</li>
<li>且投机解码算法和其他大模型推理算法（例如 <code>kv cache</code>、<code>flash attention</code>、<code>MQA / GQA</code>）并不冲突，可以合并使用达到最优加速效果</li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>从另外一种新颖且符合逻辑的方面优化了大模型推理</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/09/Locally-Typical-Sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/09/Locally-Typical-Sampling/" class="post-title-link" itemprop="url">Locally Typical Sampling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-09 12:26:39" itemprop="dateCreated datePublished" datetime="2024-08-09T12:26:39+08:00">2024-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/decoding-method/" itemprop="url" rel="index"><span itemprop="name">decoding method</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/09/Locally-Typical-Sampling/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/09/Locally-Typical-Sampling/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.00666">https://arxiv.org/pdf/2202.00666</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py#L608">https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py#L608</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出来一种 <code>Typical sampling</code> 的解码策略，是对 <code>top-p sampling</code> 的改进，启发了后续的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\eta-sampling</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span> 解码策略，旨在生成更自然、更符合人类语言使用习惯的文本。</li>
<li>论文使用信息论的观点，局部典型性（<code>local typicality</code>）采样的依据是条件熵和信息量的接近程度
<ul>
<li>条件熵：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>p</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(-log(p)*p).sum()</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mord">.</span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mclose">)</span></span></span></span></li>
<li>信息量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">-log(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></li>
</ul>
</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="公式角度理解"><a class="markdownIt-Anchor" href="#公式角度理解"></a> 公式角度理解</h3>
<ul>
<li>在时间步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span></span></span></span>，候选词集合为：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>L</mi><mi>ϵ</mi><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mo stretchy="false">{</mo><mi>y</mi><mo>=</mo><msub><mi>y</mi><mn>0</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>T</mi></msub><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∀</mi><mn>1</mn><mo>≤</mo><mi>t</mi><mo>≤</mo><mi>T</mi><mo separator="true">,</mo><mi mathvariant="normal">∣</mi><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mi>H</mi><mo stretchy="false">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>Y</mi><mrow><mo>&lt;</mo><mi>t</mi><mo>=</mo><mi>y</mi><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>&lt;</mo><mi>ϵ</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">L_{\epsilon}^{(T)}=\{y=y_0,...,y_T| \forall 1\le t \le T, | \log p(y_t|y_{\lt t}) + H(Y_t|Y_{\lt t=y \lt t}) | \lt \epsilon \}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.185em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϵ</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord">∀</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7719400000000001em;vertical-align:-0.13597em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ϵ</span><span class="mclose">}</span></span></span></span></span></p>
<ul>
<li>其中
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mi>H</mi><mo stretchy="false">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>Y</mi><mrow><mo>&lt;</mo><mi>t</mi><mo>=</mo><mi>y</mi><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>&lt;</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">| \log p(y_t|y_{\lt t}) + H(Y_t|Y_{\lt t=y \lt t}) | \lt \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 表示信息量和条件熵最大相差不大于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>，写成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>H</mi><mo stretchy="false">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>Y</mi><mrow><mo>&lt;</mo><mi>t</mi><mo>=</mo><mi>y</mi><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">(</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>&lt;</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">|H(Y_t|Y_{\lt t=y \lt t}) - (-\log p(y_t|y_{\lt t}) ) | \lt \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 更容易理解</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 是超参数，需要在 <code>(0, 1)</code> 之间</li>
</ul>
</li>
</ul>
<h3 id="代码角度理解"><a class="markdownIt-Anchor" href="#代码角度理解"></a> 代码角度理解</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TypicalLogitsWarper</span>(<span class="params">LogitsWarper</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mass: <span class="built_in">float</span> = <span class="number">0.9</span>, filter_value: <span class="built_in">float</span> = -<span class="built_in">float</span>(<span class="params"><span class="string">&quot;Inf&quot;</span></span>), min_tokens_to_keep: <span class="built_in">int</span> = <span class="number">1</span></span>):</span></span><br><span class="line">        mass = <span class="built_in">float</span>(mass)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (mass &gt; <span class="number">0</span> <span class="keyword">and</span> mass &lt; <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;`typical_p` has to be a float &gt; 0 and &lt; 1, but is <span class="subst">&#123;mass&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(min_tokens_to_keep, <span class="built_in">int</span>) <span class="keyword">or</span> (min_tokens_to_keep &lt; <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;`min_tokens_to_keep` has to be a positive integer, but is <span class="subst">&#123;min_tokens_to_keep&#125;</span>&quot;</span>)</span><br><span class="line">        self.filter_value = filter_value</span><br><span class="line">        self.mass = mass</span><br><span class="line">        self.min_tokens_to_keep = min_tokens_to_keep</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, input_ids: torch.LongTensor, scores: torch.FloatTensor</span>) -&gt; torch.FloatTensor:</span></span><br><span class="line">        <span class="comment"># calculate entropy</span></span><br><span class="line">        normalized = torch.nn.functional.log_softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        p = torch.exp(normalized)</span><br><span class="line">        ent = -(normalized * p).nansum(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># shift and sort</span></span><br><span class="line">        shifted_scores = torch.<span class="built_in">abs</span>((-normalized) - ent)</span><br><span class="line">        sorted_scores, sorted_indices = torch.sort(shifted_scores, descending=<span class="literal">False</span>)</span><br><span class="line">        sorted_logits = scores.gather(-<span class="number">1</span>, sorted_indices)</span><br><span class="line">        cumulative_probs = sorted_logits.softmax(dim=-<span class="number">1</span>).cumsum(dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Remove tokens with cumulative mass above the threshold</span></span><br><span class="line">        last_ind = (cumulative_probs &lt; self.mass).<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">        last_ind.clamp_(<span class="built_in">max</span>=sorted_scores.shape[-<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">        sorted_indices_to_remove = sorted_scores &gt; sorted_scores.gather(<span class="number">1</span>, last_ind.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        sorted_indices_to_remove[..., : self.min_tokens_to_keep] = <span class="number">0</span></span><br><span class="line">        indices_to_remove = sorted_indices_to_remove.scatter(<span class="number">1</span>, sorted_indices, sorted_indices_to_remove)</span><br><span class="line">        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)</span><br><span class="line">        <span class="keyword">return</span> scores_processed</span><br></pre></td></tr></table></figure>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>单看计算过程实际上比较符合直觉，用信息论的角度理解反而会云里雾里…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/09/Truncation-Sampling-as-Language-Model-Desmoothing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/09/Truncation-Sampling-as-Language-Model-Desmoothing/" class="post-title-link" itemprop="url">Truncation Sampling as Language Model Desmoothing</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-09 09:37:04" itemprop="dateCreated datePublished" datetime="2024-08-09T09:37:04+08:00">2024-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/decoding-method/" itemprop="url" rel="index"><span itemprop="name">decoding method</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/09/Truncation-Sampling-as-Language-Model-Desmoothing/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/09/Truncation-Sampling-as-Language-Model-Desmoothing/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.15191">https://arxiv.org/pdf/2210.15191</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/john-hewitt/truncation-sampling/blob/main/src/TruncationVisualization.ipynb">https://github.com/john-hewitt/truncation-sampling/blob/main/src/TruncationVisualization.ipynb</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>常用的 <code>top-p random sampling decoding method</code> 可能会导致输出的长文本质量较差</li>
<li>本文设计了一种基于熵的动态概率阈值优化 <code>top-p</code> 随机采样算法，叫做 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\eta-sampling</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span></li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="公式表示"><a class="markdownIt-Anchor" href="#公式表示"></a> 公式表示</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>A</mi><mrow><mi>x</mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi>x</mi><mo>∈</mo><mi>V</mi><mi mathvariant="normal">∣</mi><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>&lt;</mo><mi>i</mi><mo stretchy="false">)</mo><mo>&gt;</mo><mi>η</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">A_{x \lt i}=\{x \in V | P_{\theta}(x | x \lt i) \gt \eta \}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8607em;vertical-align:-0.17737em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mclose">}</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>η</mi><mo>=</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>ϵ</mi><mo separator="true">,</mo><mi>α</mi><mo stretchy="false">(</mo><mo>−</mo><msub><mi>h</mi><mrow><mi>η</mi><mo separator="true">,</mo><mi>x</mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\eta=min(\epsilon,\alpha(-h_{\eta,x&lt;i}))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">ϵ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord">−</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">η</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">x</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中第一行公式表示随机采样概率值截断到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span><br />
第二行中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo separator="true">,</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">\epsilon,\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">ϵ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 是超参数，通常 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><msqrt><mi>ϵ</mi></msqrt></mrow><annotation encoding="application/x-tex">\alpha=\sqrt{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.23972em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8002800000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.76028em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.23972em;"><span></span></span></span></span></span></span></span></span>，<code>h</code> 表示输出的熵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mo>−</mo><mo stretchy="false">(</mo><mi>p</mi><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h=-(p*log(p)).sum()</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">.</span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mclose">)</span></span></span></span>，<code>p</code> 表示概率</p>
<h3 id="代码表示"><a class="markdownIt-Anchor" href="#代码表示"></a> 代码表示</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EtaWarper</span>(<span class="params">transformers.LogitsWarper</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Our proposed eta sampling warper.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, epsilon</span>):</span></span><br><span class="line">    self.epsilon = epsilon</span><br><span class="line">    self.filter_value = -<span class="built_in">float</span>(<span class="string">&quot;Inf&quot;</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, input_ids, scores</span>) -&gt; torch.FloatTensor:</span></span><br><span class="line">    probabilities = scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">    entropy = - (probabilities * torch.log(probabilities)).<span class="built_in">sum</span>()</span><br><span class="line">    eta = <span class="built_in">min</span>(self.epsilon, torch.sqrt(torch.tensor(self.epsilon))*torch.exp(-entropy))</span><br><span class="line">    indices_to_remove = probabilities &lt; eta</span><br><span class="line">    max_word = torch.argmax(scores,dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 防止把候选词全删了，至少留一个</span></span><br><span class="line">    indices_to_remove[...,max_word.squeeze()] = <span class="number">0</span></span><br><span class="line">    new_scores = scores.masked_fill(indices_to_remove, self.filter_value)</span><br><span class="line">    <span class="keyword">return</span> new_scores</span><br></pre></td></tr></table></figure>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>是 <code>top-p</code> 的小升级，计算量较小，算是一个小 <code>trick</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/08/A-Contrastive-Framework-for-Neural-Text-Generation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/08/A-Contrastive-Framework-for-Neural-Text-Generation/" class="post-title-link" itemprop="url">A Contrastive Framework for Neural Text Generation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-08 12:38:20" itemprop="dateCreated datePublished" datetime="2024-08-08T12:38:20+08:00">2024-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/decoding-method/" itemprop="url" rel="index"><span itemprop="name">decoding method</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/08/A-Contrastive-Framework-for-Neural-Text-Generation/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/08/A-Contrastive-Framework-for-Neural-Text-Generation/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.06417">https://arxiv.org/pdf/2202.06417</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/yxuansu/SimCTG/tree/main/contrastive_search_explanation">https://github.com/yxuansu/SimCTG/tree/main/contrastive_search_explanation</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出一种叫做 <code>Contrastive search</code> 的解码方法，比 <code>top-k</code> / <code>top-p</code> 等随机解码方法和 <code>argmax</code> 这种贪心解码方法更优，模型输出内容语意更连贯，模型更不容易退化</li>
<li><code>Contrastive search</code> 是在 <code>top-k</code> 基础上增加了 <code>degeneration penalty</code>（退化惩罚项），算是 <code>random sampling</code> 类算法的升级版</li>
<li>但需要将候选的 <code>top-k</code> 输出追加到序列末尾作为输入重新输入到模型中输出 <code>top-k</code> 对应的 <code>hidden state</code>，因此时间复杂度较高</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="总体架构"><a class="markdownIt-Anchor" href="#总体架构"></a> 总体架构</h3>
<p><img src="https://s2.loli.net/2024/08/08/DBsgbwNrtCXhHaF.png" alt="contrastive_search_1.png" /></p>
<blockquote>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 是超参数，用来平衡模型预测结果和退化惩罚项<br />
<code>h</code> 表示 <code>hidden state</code>，用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">h_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和序列中每一个 <code>token</code> 的 <code>hidden state</code> 计算相似度<br />
由于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">h_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 只能在下一次 <code>inference</code> 时产生，所以需要在 <code>decoding</code> 阶段再推理一次，这一步比较麻烦</p>
</blockquote>
<h3 id="用一段代码来描述一下解码过程"><a class="markdownIt-Anchor" href="#用一段代码来描述一下解码过程"></a> 用一段代码来描述一下解码过程</h3>
<ol>
<li>首先在 <code>t-1</code> 时刻输入 <code>seq_len</code> 长度的序列，输出 <code>vocab_size</code> 长度的 <code>logits</code>，选 <code>top-k</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the logits in this step</span></span><br><span class="line">prev_hidden_states, logits = model.compute_logits_and_hidden_states(input_ids)</span><br><span class="line">_, seqlen, embed_dim = prev_hidden_states.size()</span><br><span class="line">_, _, vocab_size = logits.size()</span><br><span class="line">logit_for_next_step = logits[:,-<span class="number">1</span>,:]</span><br><span class="line"><span class="keyword">assert</span> logit_for_next_step.size() == torch.Size([<span class="number">1</span>, vocab_size])</span><br><span class="line"><span class="comment"># normalize with the softmax function</span></span><br><span class="line">next_probs = F.softmax(logit_for_next_step, dim = -<span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> next_probs.size() == logit_for_next_step.size()</span><br><span class="line"><span class="comment"># collecte top-k candidate tokens and their logits (model confidence)</span></span><br><span class="line">_, next_top_k_ids = torch.topk(logit_for_next_step, dim = -<span class="number">1</span>, k = beam_width)</span><br><span class="line"><span class="keyword">assert</span> next_top_k_ids.size() == torch.Size([<span class="number">1</span>, beam_width])</span><br><span class="line">        </span><br><span class="line">next_top_k_probs = torch.gather(next_probs, dim = <span class="number">1</span>, index=next_top_k_ids)</span><br><span class="line"><span class="keyword">assert</span> next_top_k_probs.size() == next_top_k_ids.size()</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>将 <code>top-k</code> 预测的每一种可能追加到原始序列，并在 <code>batch size</code> 维度 <code>concat</code> 成一个 <code>shape = [beam_width, seq_len + 1, dim]</code> 的输入</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># concatenate each candidate token with prefix</span></span><br><span class="line">expanded_context = [input_ids <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(beam_width)]</span><br><span class="line">expanded_context = torch.cat(expanded_context, dim = <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> expanded_context.size() == torch.Size([beam_width, seqlen])</span><br><span class="line">top_k_ids = top_k_ids.view(beam_width, <span class="number">1</span>)</span><br><span class="line">next_input_ids = torch.cat([expanded_context, top_k_ids], dim = -<span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> next_input_ids.size() == torch.Size([beam_width, seqlen+<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>将 <code>shape = [beam_width, seq_len + 1, dim]</code> 输入到模型中，计算得到每个 <code>token</code> 的 <code>hidden state</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feed these candidates into next round to get their hidden states</span></span><br><span class="line">new_hidden_states, next_logits = model.compute_logits_and_hidden_states(next_input_ids)</span><br><span class="line"><span class="keyword">assert</span> new_hidden_states.size() == torch.Size([beam_width, seqlen+<span class="number">1</span>, embed_dim])</span><br><span class="line">context_hidden = new_hidden_states[:,:seqlen,:]</span><br><span class="line"><span class="keyword">assert</span> context_hidden.size() == torch.Size([beam_width, seqlen, embed_dim])</span><br><span class="line">next_hidden = new_hidden_states[:,seqlen:,:]</span><br><span class="line"><span class="keyword">assert</span> next_hidden.size() == torch.Size([beam_width, <span class="number">1</span>, embed_dim])</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>计算每一个 <code>next hidden state</code> 和 <code>context hidden state</code> 之间的相关性，相关性越高，说明重复性越高，惩罚越重，选择此 <code>token</code> 作为 <code>next input id</code> 的可能性越低</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ranking</span>(<span class="params">context_hidden, next_hidden, next_top_k_ids, next_top_k_probs, alpha</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        context_hidden: beam_width x context_len x embed_dim</span></span><br><span class="line"><span class="string">        next_hidden: beam_width x 1 x embed_dim</span></span><br><span class="line"><span class="string">        next_top_k_ids: beam_width x 1</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    beam_width, context_len, embed_dim = context_hidden.size()</span><br><span class="line">    <span class="keyword">assert</span> next_hidden.size() == torch.Size([beam_width, <span class="number">1</span>, embed_dim])</span><br><span class="line">    norm_context_hidden = context_hidden / context_hidden.norm(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    norm_next_hidden = next_hidden / next_hidden.norm(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    cosine_matrix = torch.matmul(norm_context_hidden, norm_next_hidden.transpose(<span class="number">1</span>,<span class="number">2</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> cosine_matrix.size() == torch.Size([beam_width, context_len])</span><br><span class="line">    scores, _ = torch.<span class="built_in">max</span>(cosine_matrix, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> scores.size() == torch.Size([beam_width])</span><br><span class="line">    next_top_k_probs = next_top_k_probs.view(-<span class="number">1</span>)</span><br><span class="line">    scores = (<span class="number">1.0</span> - alpha) * next_top_k_probs - alpha * scores </span><br><span class="line">    _, selected_idx = torch.topk(scores, k = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> selected_idx.size() == torch.Size([<span class="number">1</span>])</span><br><span class="line">    selected_idx = selected_idx.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> selected_idx.size() == torch.Size([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    next_id = torch.gather(next_top_k_ids, dim = <span class="number">0</span>, index=selected_idx)</span><br><span class="line">    <span class="keyword">assert</span> next_id.size() == torch.Size([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> next_id</span><br></pre></td></tr></table></figure>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>道理上讲得通，但多推理的一次是实实在在的计算代价，总计算量直接翻倍，真的会有大模型选择这种解码方式吗？</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/" class="post-title-link" itemprop="url">ALiBi: Train short, test long: Attention with linear biases enables input length extrapolation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-05 19:02:10" itemprop="dateCreated datePublished" datetime="2024-08-05T19:02:10+08:00">2024-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Position-embedding/" itemprop="url" rel="index"><span itemprop="name">Position embedding</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/05/ALiBi-Train-short-test-long-Attention-with-linear-biases-enables-input-length-extrapolation/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="url"><a class="markdownIt-Anchor" href="#url"></a> URL</h2>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.12409">https://arxiv.org/pdf/2108.12409</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742">https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742</a></li>
</ul>
<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2>
<ul>
<li>本文提出一种比 <code>T5 bias</code> 更简单的 <code>position embedding</code> 方法叫做 <code>ALiBi (Attention with Linear Bias)</code>，简单好用</li>
<li>可以在短数据集上训练，在长数据集上测试，即具有外推性</li>
</ul>
<h2 id="algorithm"><a class="markdownIt-Anchor" href="#algorithm"></a> Algorithm</h2>
<h3 id="t5-bias"><a class="markdownIt-Anchor" href="#t5-bias"></a> T5 bias</h3>
<ul>
<li>先讲一下 <code>T5 bias</code> 是如何实现 <code>position embedding</code> 的，主要分三步：
<ol>
<li>计算 <code>query / key</code> 的 <code>n * n</code> 相对位置矩阵，形如： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],</span><br><span class="line"> [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8],</span><br><span class="line"> [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7],</span><br><span class="line"> [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6],</span><br><span class="line"> [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5],</span><br><span class="line"> [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4],</span><br><span class="line"> [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3],</span><br><span class="line"> [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2],</span><br><span class="line"> [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1],</span><br><span class="line"> [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0]]</span><br></pre></td></tr></table></figure>
</li>
<li>将相对位置矩阵分桶（超过 <code>num_buckets</code> 的饱和到 <code>num_buckets</code>） <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[ 0, 17, 18, 19, 20, 21, 22, 23, 24, 24],</span><br><span class="line"> [ 1,  0, 17, 18, 19, 20, 21, 22, 23, 24],</span><br><span class="line"> [ 2,  1,  0, 17, 18, 19, 20, 21, 22, 23],</span><br><span class="line"> [ 3,  2,  1,  0, 17, 18, 19, 20, 21, 22],</span><br><span class="line"> [ 4,  3,  2,  1,  0, 17, 18, 19, 20, 21],</span><br><span class="line"> [ 5,  4,  3,  2,  1,  0, 17, 18, 19, 20],</span><br><span class="line"> [ 6,  5,  4,  3,  2,  1,  0, 17, 18, 19],</span><br><span class="line"> [ 7,  6,  5,  4,  3,  2,  1,  0, 17, 18],</span><br><span class="line"> [ 8,  7,  6,  5,  4,  3,  2,  1,  0, 17],</span><br><span class="line"> [ 8,  8,  7,  6,  5,  4,  3,  2,  1,  0]]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里上三角和下三角都有值是因为 <code>encoder bidirection=True</code>，如果是 <code>decoder</code>，则如下：</p>
</blockquote>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [2, 1, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [3, 2, 1, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [4, 3, 2, 1, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [5, 4, 3, 2, 1, 0, 0, 0, 0, 0],</span><br><span class="line"> [6, 5, 4, 3, 2, 1, 0, 0, 0, 0],</span><br><span class="line"> [7, 6, 5, 4, 3, 2, 1, 0, 0, 0],</span><br><span class="line"> [8, 7, 6, 5, 4, 3, 2, 1, 0, 0],</span><br><span class="line"> [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]</span><br></pre></td></tr></table></figure>
</li>
<li>最后是将此 <code>n * n</code> 的 <code>relative position bucket</code> 通过可学习的 <code>embedding</code> 函数变成 <code>n * n * num_heads</code> 的向量，和每个头的 <code>attention score（softmax 之前）</code> 相加，然后通过逐行 <code>softmax</code> 得到 <code>attention weight</code></li>
</ol>
</li>
</ul>
<h3 id="alibi"><a class="markdownIt-Anchor" href="#alibi"></a> ALiBi</h3>
<p><img src="https://s2.loli.net/2024/08/05/R3DH5fENoz8AYQn.png" alt="alibi_1.png" /></p>
<ul>
<li>用数学公式表示：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><msup><mi>K</mi><mi>T</mi></msup><mo>+</mo><mi>m</mi><mo>⋅</mo><mo stretchy="false">[</mo><mo>−</mo><mo stretchy="false">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mo>−</mo><mn>2</mn><mo separator="true">,</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">softmax(q_iK^T+m\cdot[-(i-1),...,-2,-1,0])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.44445em;vertical-align:0em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">−</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></li>
<li><code>ALiBi</code> 的计算和 <code>T5 bias</code> 的前两步几乎一模一样</li>
<li>第三步不再使用可学习的 <code>embedding</code> 函数映射到每个头上，而是将距离矩阵的值和每个头独立的 <strong>不可学习的</strong> 常量 <code>m</code> 值相乘，然后和 <code>attention score</code> 相加</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>h</mi></msub><mo>=</mo><mfrac><mi>b</mi><mrow><mo stretchy="false">(</mo><msup><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>8</mn><mi mathvariant="normal">/</mi><mi>H</mi><mo stretchy="false">)</mo></mrow></msup><mo>⋅</mo><mi>b</mi><msup><mo stretchy="false">)</mo><mi>h</mi></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">m_h = \frac{b}{(2^{(8/H)} \cdot b)^h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4405329999999998em;vertical-align:-0.560425em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.614575em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8220357142857143em;"><span style="top:-2.8220357142857138em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">8</span><span class="mord mtight">/</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight">b</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7820285714285713em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.560425em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>
<ul>
<li><code>b</code> 是一个基数</li>
<li><code>H</code> 是注意力头的数量</li>
<li><code>h</code> 是注意力头的索引（从 <code>0</code> 到 <code>H-1</code>）</li>
</ul>
</li>
</ul>
<h2 id="thought"><a class="markdownIt-Anchor" href="#thought"></a> Thought</h2>
<ul>
<li>标准 <code>attention</code> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>e</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">pe \in \mathbb{R}^{n\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span> 已经慢慢被淘汰了，不管是 <code>RoPE / T5 Bias / ALiBi</code> 都已经逐渐演变成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>e</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">pe \in \mathbb{R}^{n\times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.771331em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span> 直接作用在 <code>attention score</code> 上了</li>
<li><code>ALiBi</code> 的外推性其实本质是强行饱和掉远距离，有点过于粗暴了…</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhangzhe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">175</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">104</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhangzhe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js', () => {
    // 初始化 Mermaid 配置
    mermaid.initialize({
      theme    : 'dark',  // 设置主题
      logLevel : 3,  // 设置日志等级
      flowchart: { curve: 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 },
      themeVariables: {
        'fontFamily': 'Microsoft YaHei, Arial, sans-serif',  // 设置中文字体
      }
    });

    // 初始化 Mermaid 图表
    mermaid.init(undefined, document.querySelectorAll('pre.mermaid'));
  }, window.mermaid);
}
</script>


  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">


  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'eK3W25jybCO5jVUrYBBpAPqM-gzGzoHsz',
      appKey     : 'F4KVyUj9wHI5c80Bhz7O2uhq',
      placeholder: "说点什么再走吧...",
      avatar     : 'hide',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
