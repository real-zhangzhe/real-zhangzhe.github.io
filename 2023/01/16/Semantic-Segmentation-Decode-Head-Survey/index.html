<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"real-zhangzhe.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Topic  本文汇总多种 语义分割算法 decode head 结构和 部分分割专用 backbone，用于理解语义分割算法的演进过程 decode head 模型来源： mmsegmentaion decode head 本文的语义分割 decode head 是指满足如下要求的网络结构：  输入为 backbone &#x2F; neck 提取的 feature map 或 feature map">
<meta property="og:type" content="article">
<meta property="og:title" content="Semantic Segmentation Algorithms Survey">
<meta property="og:url" content="https://real-zhangzhe.github.io/2023/01/16/Semantic-Segmentation-Decode-Head-Survey/index.html">
<meta property="og:site_name" content="Zhangzhe&#39;s Blog">
<meta property="og:description" content="Topic  本文汇总多种 语义分割算法 decode head 结构和 部分分割专用 backbone，用于理解语义分割算法的演进过程 decode head 模型来源： mmsegmentaion decode head 本文的语义分割 decode head 是指满足如下要求的网络结构：  输入为 backbone &#x2F; neck 提取的 feature map 或 feature map">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/95uvgr4wGKpzm8k.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/SueV71rfDNQmTUM.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/DzUsqPOx9ckKui3.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/hVstSe2EnTfDFuK.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/TioeVzgHl8yfacW.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/kFMbZexCornsY4L.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/gzonXcZwR8J9Qy1.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/v25PboYguxBfzkR.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/eL8q1laoUYJ4fDg.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/24/eq1VOcZbPWzRh3G.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/24/4VdwCTUIj6fGonq.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/24/G84QBmsxMCTFnrK.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/24/m4XCq958gt1yiAE.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/ExnclXJaKFd9qYs.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/x2MNa93RYDqOLo6.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/ReFg5H3SsImvKMb.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/ph7ErWkazxfBgwI.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/NLj9YZ7VFiJe14y.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/ndy84EM1mB7gJPX.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/Lo5XHarWQDOSeyI.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/8uPSlDdEgFRZAxb.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/92ZvHBoUJXrEkKS.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/XjqQLEJ85gsKMit.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/2e3AWOGEnfiRqdx.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/17/wn6iBDPHM9h7pKC.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/yJZBx2v1YOuPlir.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/TQgwaJMGYnSDux6.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/7YmA9BQneXJDS8P.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/dlIVOLQYiEtvaJj.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/VCFliXsh2efmM54.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/yF85ZoGsqYIKEDP.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/26/q2KaO94bMpnQdlu.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/TDshKaot8xqC2BE.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/18/f6riCoLFzTYN2RH.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/28/VfMGKkSAnR1zwBv.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/28/zDHs1mFn5Cu43RE.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/28/fBdi6THRtnpcYLI.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/28/9HqMVUDIJQg3fdi.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/MLDcPIBwT8kaoqJ.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/oSCVpNAwgGI4bBs.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/xhemlYgjH3Vw5ks.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/25/o2VTBwjG8NkUA5H.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/30/WHFi1SLMU6ZQcT5.png">
<meta property="og:image" content="https://s2.loli.net/2023/01/30/D7PkZdmbtJexFjI.png">
<meta property="article:published_time" content="2023-01-16T11:33:11.000Z">
<meta property="article:modified_time" content="2025-12-11T05:48:26.538Z">
<meta property="article:author" content="Zhangzhe">
<meta property="article:tag" content="Paper Reading">
<meta property="article:tag" content="Semantic Segmentation">
<meta property="article:tag" content="Survey">
<meta property="article:tag" content="Decode Head">
<meta property="article:tag" content="MMSegmentation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/01/17/95uvgr4wGKpzm8k.png">

<link rel="canonical" href="https://real-zhangzhe.github.io/2023/01/16/Semantic-Segmentation-Decode-Head-Survey/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Semantic Segmentation Algorithms Survey | Zhangzhe's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhangzhe's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The projection of my life.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/archives/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://real-zhangzhe.github.io/2023/01/16/Semantic-Segmentation-Decode-Head-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhangzhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhangzhe's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Semantic Segmentation Algorithms Survey
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-01-16 19:33:11" itemprop="dateCreated datePublished" datetime="2023-01-16T19:33:11+08:00">2023-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-11 13:48:26" itemprop="dateModified" datetime="2025-12-11T13:48:26+08:00">2025-12-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Semantic-Segmentation/" itemprop="url" rel="index"><span itemprop="name">Semantic Segmentation</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2023/01/16/Semantic-Segmentation-Decode-Head-Survey/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/01/16/Semantic-Segmentation-Decode-Head-Survey/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="topic"><a class="markdownIt-Anchor" href="#topic"></a> Topic</h2>
<ul>
<li>本文汇总多种 <strong>语义分割算法 <code>decode head</code></strong> 结构和 <strong>部分分割专用 <code>backbone</code></strong>，用于理解语义分割算法的演进过程</li>
<li><code>decode head</code> 模型来源： <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/__init__.py"><code>mmsegmentaion decode head</code></a></li>
<li>本文的语义分割 <code>decode head</code> 是指满足如下要求的网络结构：
<ol>
<li>输入为 <code>backbone / neck</code> 提取的 <code>feature map</code> 或 <code>feature map list</code></li>
<li>输出为 <code>segmentation</code> 结果</li>
</ol>
</li>
</ul>
<h2 id="语义分割推理过程"><a class="markdownIt-Anchor" href="#语义分割推理过程"></a> 语义分割推理过程</h2>
<h3 id="1-原始特征处理"><a class="markdownIt-Anchor" href="#1-原始特征处理"></a> 1. 原始特征处理</h3>
<ul>
<li>输入的原始特征包括两类：
<ul>
<li><code>backbone</code> 输出的 <code>feature map</code>（例如 <code>PSPNet</code> 输出）</li>
<li><code>backbone</code> 不同阶段 / <code>neck</code> (例如 <code>FPN</code>) 输出的不同尺度的 <code>feature map list</code></li>
</ul>
</li>
<li>对于 <code>feature map</code>，可以 <code>resize</code> 到输出大小再送入 <code>decode head</code>，也可以直接送入 <code>decode head</code>，根据具体算法选择</li>
<li>对于 <code>feature map list</code>，一般有两种做法，根据具体算法选择：
<ol>
<li><code>resize concat</code>: 将所有 <code>feature map</code> 全部 <code>resize</code> 到输出大小后再 <code>concat</code>（例如 <code>FCN-8s</code>）</li>
<li><code>multiple select</code>: 根据 <code>index</code> 在 <code>feature map list</code> 中索引并输出对应的 <code>feature map sub list</code></li>
</ol>
</li>
</ul>
<h3 id="2-特征解码"><a class="markdownIt-Anchor" href="#2-特征解码"></a> 2. 特征解码</h3>
<ul>
<li>将 <strong>1</strong> 中输出的 <code>feature map / feature map list</code> 转化成与输出 <strong>宽高一致</strong> 的 <code>feature map</code>，也是本文具体展开讲的内容</li>
</ul>
<h3 id="3-特征映射到分割任务空间"><a class="markdownIt-Anchor" href="#3-特征映射到分割任务空间"></a> 3. 特征映射到分割任务空间</h3>
<ul>
<li>将 <strong>2</strong> 中输出的特征映射到分割空间，具体通道数与任务定义相关（例如：二分类的语义分割输出通道为 <code>1</code> 或 <code>2</code>，<code>N</code> 分类的语义分割输出通道数为 <code>N</code>）</li>
</ul>
<h2 id="演进过程"><a class="markdownIt-Anchor" href="#演进过程"></a> 演进过程</h2>
<h3 id="第一代在-cnn-结构上创新"><a class="markdownIt-Anchor" href="#第一代在-cnn-结构上创新"></a> 第一代：在 <code>CNN</code> 结构上创新</h3>
<ul>
<li><code>FCN</code>: 2014年，出自 <code>UC Berkeley</code>，分割算法起点</li>
<li><code>PSP</code>: 2016年，出自商汤，<code>FCN</code> + 多尺度</li>
<li><code>ASPP</code>: 2017年，出自 <code>Google</code>，<code>PSP</code> 的优雅实现版（<strong><code>DeepLab V2</code></strong>、<strong><code>DeepLab V3</code></strong>）</li>
<li><code>FPN</code>: 2018年，出自 <code>FAIR</code>，<code>UNet</code> 多尺度的升级版</li>
<li><code>UperNet</code>: 2018年，出自旷视，<code>PSP</code> + <code>FPN</code> 更暴力的多尺度</li>
<li><code>DepthwiseSeparableASPP</code>: 2018年，出自 <code>Google</code>，<code>DeepLab V3</code> 结构的小改动（<strong><code>DeepLab V3+</code></strong>）</li>
<li><code>DepthwiseSeparableFCN</code>: 2019年，出自东芝 + 剑桥，<code>FCN</code> 的轻量化改造（<strong><code>Fast-SCNN</code></strong>）</li>
<li><code>PointRend</code>: 2019年，出自 <code>FAIR</code>，在其他 <code>decode head</code> 基础上级联了一个 <code>subnetwork</code> 实现了图像分割边缘的细化</li>
</ul>
<h3 id="第二代self-attention-non-local-channel-attention"><a class="markdownIt-Anchor" href="#第二代self-attention-non-local-channel-attention"></a> 第二代：<code>Self-Attention</code> (<code>Non-local</code> / <code>Channel Attention</code>)</h3>
<ul>
<li><code>Non-Local</code>: 2017年，出自 <code>FAIR</code>，<code>Self Attention</code> 经典</li>
<li><code>PSANet</code>: 2018年，出自商汤，<code>Non-local</code> 的二维 <s>狗尾续貂</s> 版</li>
<li><code>CCNet</code>: 2018年，出自地平线，<code>Non-local</code> 的低算力版，使用两个低算力的 <code>Attention</code> 替代 <code>Non-local Attention</code></li>
<li><code>DANet</code>: 2018年，出自京东，两路 <code>Non-local</code>，一路 <code>attention to postion</code> 一路 <code>attention to channel</code></li>
<li><code>EncNet</code>: 2018年，出自商汤 + <code>Amazon</code>，优化了 <code>SENet</code> 中的暴力编码方式，在分割任务中额外加入了分类辅助监督</li>
<li><code>EMANet</code>: 2019年，出自北大，<code>attention to channel</code> 和 <code>attention to postion</code> 可分离的 <code>attention</code></li>
<li><code>ANN</code>: 2019年，出自华中科技大学，简化 <code>Non-local</code> 同时引入 <code>PPM</code>，极大的降低了 <code>matmul</code> 和 <code>softmax</code> 两类算子的耗时</li>
<li><code>GCNet</code>: 2019年，出自 <code>MSRA</code>，简化版 <code>Non-local</code> + <code>SENet</code> 的缝合怪</li>
<li><code>OCRNet</code>: 2019年，出自 <code>MSRA</code>，级联结构，在其他 <code>decode head</code> 的输出结果上做了 <code>Self-Attention</code>，并在论文中从 <code>Transformer</code> 角度解释了 <code>Self-Attention</code><s>（Transformer 开始觉醒）</s></li>
<li><code>APCNet</code>: 2019年，出自商汤，复杂网络结构 + 简化矩阵乘实现的 <code>Attention</code></li>
<li><code>DMNet</code>: 2019年，出自商汤，根据输入特征的全局信息动态生成卷积核，本质也是 <code>Attention</code></li>
<li><code>LRASPP</code>: 2019年，出自 <code>Google</code>，全局 <code>scale</code> 实现的 <code>Attention</code>（<strong><code>MobileNet V3</code></strong>）</li>
<li><code>ISANet</code>: 2019年，出自 <code>MSAR</code>，使用 <code>feature map shuffle</code> 实现长范围和短范围的稀疏注意力机制</li>
<li><code>DNLNet</code>: 2020年，出自 <code>MSAR</code>，改进 <code>Non-local</code>，加入了归一化和一元分支</li>
<li><code>BiSeNet</code>: 2019年，出自旷视，在 <code>backbone</code> 之外加入了一个 <code>context branch</code>，将特征提取和 <code>attention</code> 解耦，降低了 <code>attention</code> 恐怖的计算量</li>
<li><code>BiSeNet V2</code>: 2020年，出自腾讯，<code>BiSeNet</code> 的改进</li>
<li><code>SDTC</code>: 2021年，出自美团，<code>BiSeNet</code> 系列的改进版，但由于融合了两路分支到一处，不再 <code>Bilateral</code>，所以用特征提取 <code>SDTC block</code> 命名…</li>
</ul>
<h3 id="第三代transformer"><a class="markdownIt-Anchor" href="#第三代transformer"></a> 第三代：<code>Transformer</code></h3>
<ul>
<li><code>SETR</code>: 2020年，出自腾讯，<code>Vit</code> 做 <code>backbone</code> + <code>FCN / FPN decode head</code></li>
<li><code>DPT</code>: 2021年，出自 <code>Intel</code>，<code>SETR</code> 的升级版，<code>backbone</code> 不变，<code>decode head</code> 更 <code>FPN</code> 了一些</li>
<li><code>Segmenter</code>: 2021年，出自法国 <code>INRIA</code> 研究所，用了纯 <code>Transformer</code> 架构而不是像 <code>SETR / DPT</code> 一样用 <code>Transformer Encoder + CNN Decoder</code> 架构</li>
<li><code>SegFormer</code>: 2021年，出自 <code>NVIDIA</code>，<code>SETR</code> 的高效版</li>
<li><code>KNet</code>: 2021年，出自商汤，<code>decode head</code> 融合了 <code>Channel Attention + Multi-head Attention + RNN</code>，统一了语义分割、实例分割、全景分割框架</li>
</ul>
<h2 id="algorithms"><a class="markdownIt-Anchor" href="#algorithms"></a> Algorithms</h2>
<h3 id="1-fcn"><a class="markdownIt-Anchor" href="#1-fcn"></a> 1. FCN</h3>
<ul>
<li><code>FCN</code> 全称是 <code>Fully Convolutional Networks</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1411.4038.pdf">https://arxiv.org/pdf/1411.4038.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/fcn_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/fcn_head.py</a></li>
</ul>
<h4 id="11-原始特征处理"><a class="markdownIt-Anchor" href="#11-原始特征处理"></a> 1.1 原始特征处理</h4>
<ul>
<li>原始特征处理使用了 <code>resize concat</code> 方式，将多个不同尺度（<code>backbone</code> 不同阶段）的 <code>feature map resize concat</code> 到输出尺寸，如下图所示：<br />
<img src="https://s2.loli.net/2023/01/17/95uvgr4wGKpzm8k.png" alt="FCN1.png" /><br />
<img src="https://s2.loli.net/2023/01/17/SueV71rfDNQmTUM.png" alt="FCN2.png" /></li>
</ul>
<blockquote>
<p>实验证明越多尺度融合分割效果越好</p>
</blockquote>
<h4 id="12-特征解码"><a class="markdownIt-Anchor" href="#12-特征解码"></a> 1.2 特征解码</h4>
<ul>
<li>特征解码只使用了几层普通 <code>Conv</code> + 可选择的 <code>concat input</code> （<code>shortcut</code>）结构</li>
</ul>
<h3 id="2-psp"><a class="markdownIt-Anchor" href="#2-psp"></a> 2. PSP</h3>
<ul>
<li><code>PSP</code> 全称是 <strong><code>Pyramid Scene Parsing</code></strong>（金字塔场景理解）</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.01105.pdf">https://arxiv.org/pdf/1612.01105.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/psp_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/psp_head.py</a></li>
</ul>
<h4 id="21-原始特征处理"><a class="markdownIt-Anchor" href="#21-原始特征处理"></a> 2.1 原始特征处理</h4>
<ul>
<li><code>PSPNet</code> 的原始特征是 <code>backbone</code> 最后一层的输出，所以无需原始特征处理</li>
</ul>
<h4 id="22-特征解码"><a class="markdownIt-Anchor" href="#22-特征解码"></a> 2.2 特征解码</h4>
<ul>
<li><code>PSPNet</code> 将输入特征通过 <strong><code>Pyramid Pooling Module</code></strong> 结构做了 <code>feature map</code> 不同尺度 <code>down sample</code> + <code>up sample</code>，如下图所示：<br />
<img src="https://s2.loli.net/2023/01/17/DzUsqPOx9ckKui3.png" alt="PSP.png" /></li>
</ul>
<h3 id="3-aspp"><a class="markdownIt-Anchor" href="#3-aspp"></a> 3. ASPP</h3>
<ul>
<li>ASPP 全称是 <strong><code>Atrous Spatial Pyramid Pooling</code></strong>（空洞空间金字塔池化）</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.05587.pdf">https://arxiv.org/pdf/1706.05587.pdf</a> (大名鼎鼎的 <code>DeepLab V3</code>)</li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/aspp_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/aspp_head.py</a></li>
</ul>
<h4 id="31-原始特征处理"><a class="markdownIt-Anchor" href="#31-原始特征处理"></a> 3.1 原始特征处理</h4>
<ul>
<li><code>DeepLabV3</code> 输入为单个单尺度 <code>feature map</code>，所以此步骤可省略</li>
</ul>
<h4 id="32-特征解码"><a class="markdownIt-Anchor" href="#32-特征解码"></a> 3.2 特征解码</h4>
<p><img src="https://s2.loli.net/2023/01/17/hVstSe2EnTfDFuK.png" alt="ASPP.png" /></p>
<blockquote>
<p>与 <code>PSPNet</code> 很像，<code>PSPNet</code> 是使用普通 <code>Conv</code> 去卷积多种尺度的 <code>Pooled feature map</code>；<code>ASPP</code> 是不改变 <code>feature map</code> 而是使用 <strong>不同空洞系数的 <code>Conv</code></strong></p>
</blockquote>
<h3 id="4-fpn"><a class="markdownIt-Anchor" href="#4-fpn"></a> 4. FPN</h3>
<ul>
<li><code>FPN</code> 全称是 <code>Feature Pyramid Network</code>，出自 kaiming 大神，可以用在所有和 <code>feature map scale</code> 大小相关的领域</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.02446.pdf">https://arxiv.org/pdf/1901.02446.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/fpn_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/fpn_head.py</a><br />
<img src="https://s2.loli.net/2023/01/18/TioeVzgHl8yfacW.png" alt="FPN.png" /></li>
</ul>
<h3 id="5-upernet"><a class="markdownIt-Anchor" href="#5-upernet"></a> 5. UperNet</h3>
<ul>
<li><code>UperNet</code> 的全称是 <strong><code>Unified Perceptual Parsing Network</code></strong>（统一感知解析网络），本身是多任务模型：
<ul>
<li>场景分类</li>
<li><code>objects</code> 语义分割</li>
<li><code>parts</code> 语义分割</li>
<li><code>materials</code> 语义分割</li>
<li><code>textures</code> 语义分割<br />
本文只讨论其中的 <code>objects</code> 语义分割部分</li>
</ul>
</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.10221.pdf">https://arxiv.org/pdf/1807.10221.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py</a></li>
</ul>
<h4 id="51-原始特征处理"><a class="markdownIt-Anchor" href="#51-原始特征处理"></a> 5.1 原始特征处理</h4>
<ul>
<li>本算法在 <code>decode head</code> 中内嵌使用 <code>FPN</code>（而不是以网络 <code>neck</code> 方式使用），所以 <code>feature map list</code> 格式的原始特征无需处理，直接透传到特征解码部分</li>
</ul>
<h4 id="52-特征解码"><a class="markdownIt-Anchor" href="#52-特征解码"></a> 5.2 特征解码</h4>
<p><img src="https://s2.loli.net/2023/01/17/kFMbZexCornsY4L.png" alt="UperNet.png" /></p>
<blockquote>
<p>本文只讨论图中蓝色框部分<br />
<img src="https://s2.loli.net/2023/01/17/gzonXcZwR8J9Qy1.png" alt="UperNet_2.png" /><br />
只需要看蓝色框为输出的通路，算法：</p>
<ol>
<li>在最小尺度 <code>feature map</code> 上使用 <code>PPM</code>（全称 <code>Pyramid Pooling Module</code>，来自于 <code>PSPNet</code>）</li>
<li>使用 <code>FPN</code> 融合多尺度特征</li>
</ol>
</blockquote>
<h3 id="6-depthwiseseparableaspp"><a class="markdownIt-Anchor" href="#6-depthwiseseparableaspp"></a> 6. DepthwiseSeparableASPP</h3>
<ul>
<li>在 <code>DeepLab V3</code> 引入的 <code>ASPP</code> 基础上增加了两点改进：
<ol>
<li>使用 <code>DepthwiseSeparable ASPP</code> 替代 <code>ASPP</code>，减小计算量</li>
<li>增加了一个 <code>vanilla FPN</code> 结构，避免了 <code>DeepLab V3</code> 直接上采样 8 倍预测的问题</li>
</ol>
</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.02611.pdf">https://arxiv.org/pdf/1802.02611.pdf</a> （大名鼎鼎的 <code>DeepLab V3+</code>）</li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/sep_aspp_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/sep_aspp_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/v25PboYguxBfzkR.png" alt="DASPP.png" /></li>
</ul>
<blockquote>
<p>相较于 <code>DeepLab V3</code> 在 8 倍下采样的 <code>feature map</code> 上使用 ASPP，<code>DeepLab V3+</code> 在更小尺度(16 倍下采样) <code>feature map</code> 上使用 <code>DepthwiseSeparable ASPP</code><br />
同时为了解决小尺度预测的问题，加入了一个 <code>vanilla FPN</code> 做不同尺度特征融合</p>
</blockquote>
<h3 id="7-depthwiseseparablefcn"><a class="markdownIt-Anchor" href="#7-depthwiseseparablefcn"></a> 7. DepthwiseSeparableFCN</h3>
<ul>
<li><code>FCN</code> 的轻量化实现，使用 <code>DWConv</code>(<code>Depthwise Conv</code>) 和 <code>DSConv</code>(<code>Depthwise Separable Conv</code>) 替换 <code>FCN</code> 中的普通 <code>Conv</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.04502.pdf">https://arxiv.org/pdf/1902.04502.pdf</a> （<code>Fast-SCNN</code>）</li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/sep_fcn_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/sep_fcn_head.py</a><br />
<img src="https://s2.loli.net/2023/01/18/eL8q1laoUYJ4fDg.png" alt="FastSCNN.png" /></li>
</ul>
<blockquote>
<p>图中的 <code>DWConv</code> 是指 <code>Depthwise Conv</code>（<code>ic == oc == group</code>）<br />
图中的 <code>DSConv</code> 是指 <code>Depthwise Separable Conv</code>，<code>DSConv</code> 不是一个 <code>Conv</code> 而是 <code>Depthwise Conv</code> 和 <code>Pointwise Conv</code>（<code>kernel_size == 1 and group == 1</code>） 以及激活函数 / <code>BN</code> 一起组成的一个 <code>block</code></p>
</blockquote>
<h3 id="8-pointrend"><a class="markdownIt-Anchor" href="#8-pointrend"></a> 8. PointRend</h3>
<ul>
<li><code>PointRend</code> 全称是 <code>point-base rendering</code>（基于点的渲染算法），是一个级联分割算法，实例分割和语义分割都可使用，依赖于一个其他完整的 <code>decode head</code> （例如 <code>FCN</code>）的输出，该算法提出了一个 <code>subnetwork</code>，该结构只关心目标边界点的分割，可预测更准确更 <code>sharp</code> 的目标边界</li>
<li>paper： <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.08193.pdf">https://arxiv.org/pdf/1912.08193.pdf</a></li>
<li>code： <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/point_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/point_head.py</a><br />
<img src="https://s2.loli.net/2023/01/24/eq1VOcZbPWzRh3G.png" alt="point.png" /></li>
</ul>
<blockquote>
<ul>
<li><strong>渲染</strong>：渲染（<code>render</code>）是指在电脑中使用三维制作软件将制作的模型经过纹理、绑定、动画、灯光处理后得到模型和动画的图像。三维渲染是使用计算机从数字三维场景中生成二维影像的过程</li>
<li><strong>细分表面算法</strong>：细分表面算法（<code>subdivision surface algorithm</code>）在3D计算机图形中使用，通过递归完善基本级多边形网格来创建弯曲表面<br />
<img src="https://s2.loli.net/2023/01/24/4VdwCTUIj6fGonq.png" alt="point2.png" /></li>
</ul>
</blockquote>
<ul>
<li>本文的核心思想：
<ul>
<li>将计算机图形学中的 <code>Subdivision render</code> 思想用于分割，使用 <code>coarse-to-fine</code> 思想，逐级细分，提高分割效果</li>
<li>使用非均匀采样方法，越高频的区域使用越多的采样点，提高边缘分割效果<br />
<img src="https://s2.loli.net/2023/01/24/G84QBmsxMCTFnrK.png" alt="point3.png" /><br />
<img src="https://s2.loli.net/2023/01/24/m4XCq958gt1yiAE.png" alt="point4.png" /></li>
</ul>
</li>
<li><code>Inference</code> 过程（以 <code>FCN</code> 作为 <code>prev_decode_head</code> 为例）:
<ul>
<li>输入：
<ul>
<li><code>backbone</code> 的输出 <code>x</code>，<code>shape = [batch, channels, height, width]</code></li>
<li><code>FCN</code> 的输出 <code>prev_output</code>，<code>shape = [batch, num_cls, height, width]</code></li>
</ul>
</li>
<li>输出：<code>refine</code> 后的输出，<code>shape = [batch, num_cls, 2 * subdivision_steps * height, 2 * subdivision_steps * width]</code></li>
</ul>
<ol>
<li><code>prev_output copy</code> 一份作为 <code>refined_seg_logits</code></li>
<li><code>refined_seg_logits</code> 插值放大两倍，<code>shape = [batch, num_cls, 2 * height, 2 * width]</code></li>
<li>在 <code>refined_seg_logits</code> 上挑选最 <code>hard</code> 的 <code>N</code> 个点（<code>hard</code> 的定义是：如果一个像素的 <code>top1_cls_logitis</code> 和 <code>top2_cls_logits</code> 越接近，则该点越 <code>hard</code>），输出相对坐标，<code>shape = [batch, N, 2]</code></li>
<li>根据选出的 <code>N</code> 个点的坐标在 <code>x</code> 中找到对应的点（需要插值找出），作为 <code>fine_grained_point_feats</code>，<code>shape = [batch, channels, N]</code></li>
<li>根据选出的 <code>N</code> 个点的坐标在 <code>prev_output</code> 中找到对应的点（需要插值找出），作为 <code>coarse_point_feats</code>，<code>shape = [batch, num_cls, N]</code></li>
<li><code>fine_grained_point_feats</code> 和 <code>coarse_point_feats</code> <code>concat</code> 后经过 <code>Subnetwork</code>（几层 <code>MLP</code>）映射到类别空间 <code>point_logits</code>，<code>shape = [batch, num_cls, N]</code></li>
<li>根据 <code>3</code> 中的 <code>point index</code>，将 <code>6</code> 输出的 <code>point_logits</code> 替换到 <code>1</code> 中的 <code>refined_seg_logits</code> 对应位置</li>
<li>重复 <code>2 ~ 7 subdivision_steps</code> 次，输出最终的 <code>refined_seg_logits</code>，<code>shape = [batch, num_cls, 2 * subdivision_steps * height, 2 * subdivision_steps * width]</code></li>
</ol>
</li>
<li><code>Train</code> 过程：
<ul>
<li>输入：
<ul>
<li><code>backbone</code> 的输出 <code>x</code>，<code>shape = [batch, channels, height, width]</code></li>
<li><code>FCN</code> 的输出 <code>prev_output</code>，<code>shape = [batch, num_cls, height, width]</code></li>
<li><code>gt_semantic_seg</code>，<code>shape = [batch, num_cls, height, width]</code></li>
</ul>
</li>
<li>输出：<code>loss</code></li>
<li><code>Train</code> 过程与 <code>Inference</code> 过程基本相同，区别在于：
<ul>
<li>由于 <code>topk</code> 运算对梯度反向传播不友好，所以在 <code>Train</code> 的过程中使用随机采样点的策略，没有挖掘 <code>hard case</code></li>
<li><code>Train</code> 不会引入多尺度，只会在同一尺度学习 <code>subnetwork</code> 对 <code>point</code> 的分类</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="9-non-local"><a class="markdownIt-Anchor" href="#9-non-local"></a> 9. Non-Local</h3>
<ul>
<li>出自 kaiming 大神，原论文是做三维特征理解（视频理解），二维化后用在分割上也很强</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.07971.pdf">https://arxiv.org/pdf/1711.07971.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/nl_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/nl_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/ExnclXJaKFd9qYs.png" alt="nl.png" /></li>
</ul>
<blockquote>
<p>用于 <code>2</code> 维图像，所以 <code>T == 1</code>，通过增加 <code>(HW, HW)</code> 的特征相关性矩阵给特征带来全局相关性（<code>Attention</code>）</p>
</blockquote>
<ul>
<li><code>decode head</code> 前后处理和 <code>FCN</code> 一致</li>
</ul>
<h3 id="10-psanet"><a class="markdownIt-Anchor" href="#10-psanet"></a> 10. PSANet</h3>
<ul>
<li><code>PSA</code> 的全称是 <strong><code>Point-wise Spatial Attention</code></strong></li>
<li>paper: <a target="_blank" rel="noopener" href="https://hszhao.github.io/papers/eccv18_psanet.pdf">https://hszhao.github.io/papers/eccv18_psanet.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/psa_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/psa_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/x2MNa93RYDqOLo6.png" alt="PSA.png" /></li>
</ul>
<blockquote>
<p>借鉴于 <code>Non-local</code>，强行给了比较牵强的数学解释，推理过程复杂到需要调用 <code>CUDA</code> 而不是使用 <code>pure pytorch</code></p>
</blockquote>
<h3 id="11-ccnet"><a class="markdownIt-Anchor" href="#11-ccnet"></a> 11. CCNet</h3>
<ul>
<li><code>CC</code> 的全称是 <strong><code>Criss-Cross Attention</code></strong> （十字交叉注意力机制）</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.11721.pdf">https://arxiv.org/pdf/1811.11721.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/cc_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/cc_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/ReFg5H3SsImvKMb.png" alt="CC1.png" /></li>
</ul>
<blockquote>
<p>使用两个十字交叉注意力模块的串联替代 <code>Non-local</code>，降低算力<br />
<img src="https://s2.loli.net/2023/01/17/ph7ErWkazxfBgwI.png" alt="CC2.png" /><br />
整体流程平平无奇</p>
</blockquote>
<ul>
<li><code>decode head</code> 前后处理和 <code>FCN</code> 一致</li>
</ul>
<h3 id="12-danet"><a class="markdownIt-Anchor" href="#12-danet"></a> 12. DANet</h3>
<ul>
<li><code>DANet</code> 全称是 <code>Dual Attention Network</code>（双路 <code>Attention</code> 网络）
<ul>
<li>一路在空间维度 <code>Attention</code>，照搬 <code>Non-local</code></li>
<li>一路在通道维度 <code>Attention</code>，通道维度 <code>Non-local</code></li>
</ul>
</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.02983.pdf">https://arxiv.org/pdf/1809.02983.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/da_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/da_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/NLj9YZ7VFiJe14y.png" alt="DANet.png" /><br />
<img src="https://s2.loli.net/2023/01/17/ndy84EM1mB7gJPX.png" alt="DANet2.png" /></li>
</ul>
<h3 id="13-encnet"><a class="markdownIt-Anchor" href="#13-encnet"></a> 13. EncNet</h3>
<ul>
<li><code>EncNet</code> 的全称是 <strong><code>Context Encoding Network</code></strong>（上下文编码网络），做法是对网络中间层 <code>feature map</code> 编码到分类空间，加入了分类 <code>Loss</code> 监督</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.08904.pdf">https://arxiv.org/pdf/1803.08904.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/enc_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/enc_head.py</a><br />
<img src="https://s2.loli.net/2023/01/18/Lo5XHarWQDOSeyI.png" alt="EncNet.png" /></li>
</ul>
<blockquote>
<p>对于 <code>SE-loss</code>: 监督图中包含哪些类别的像素，使用交叉熵实现<br />
对于 <code>Encode</code>:</p>
<ul>
<li>从本质上看：
<ul>
<li>上图使用的 <code>Encode</code> 和 <code>SENet</code> (<code>Squeeze and Excitation Network</code>) 对 <code>feature map per channel</code> 编码没有区别</li>
</ul>
</li>
<li>从实现层面看：
<ol>
<li><code>Encode</code> 使用了更在数学上更好解释的编码方式（而不是 <code>SENet</code> 粗暴的 <code>Global Average Pooling</code> 编码方式）</li>
<li><code>Encode</code> 编码空间比 <code>SENet</code> 更大（SENet 每个通道使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68889em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span> 空间编码，<code>Encode</code> 每个通道使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span> 空间编码）</li>
</ol>
</li>
</ul>
</blockquote>
<h3 id="14-emanet"><a class="markdownIt-Anchor" href="#14-emanet"></a> 14. EMANet</h3>
<ul>
<li><code>EMA</code> 的全称是 <code>Expectation-Maximization Attention</code>（最大期望注意力），从数学角度解释了 <code>Attention</code>，实现上也是通过多个矩阵乘实现的 <code>channel</code> 与 <code>position</code> 分离的 <code>Attention</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.13426.pdf">https://arxiv.org/pdf/1907.13426.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/ema_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/ema_head.py</a><br />
<img src="https://s2.loli.net/2023/01/18/8uPSlDdEgFRZAxb.png" alt="EMA.png" /></li>
</ul>
<h3 id="15-ann"><a class="markdownIt-Anchor" href="#15-ann"></a> 15. ANN</h3>
<ul>
<li><code>ANN</code> 全称是 <strong><code>Asymmetric Non-local</code></strong>（非对称 <code>Non-local</code>）, 简化 <code>Non-local</code> 同时引入 <code>PPM</code>，极大的降低了 <code>matmul</code> 和 <code>softmax</code> 两类算子的耗时</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.07678.pdf">https://arxiv.org/pdf/1908.07678.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/ann_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/ann_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/92ZvHBoUJXrEkKS.png" alt="ANN.png" /></li>
</ul>
<blockquote>
<p>在 <code>key</code> / <code>value</code> 上对特征进行了降维 <code>N -&gt; S</code>，由下图可知，上图的 <code>sample</code> 方法具体是指 <code>PPM</code>（<code>Pyramid Pooling Module</code>）<br />
<img src="https://s2.loli.net/2023/01/17/XjqQLEJ85gsKMit.png" alt="ANN2.png" /><br />
<code>AFNB</code> 全称是 <code>Asymmetric Fusion Non-local Block</code><br />
<code>APNB</code> 全称是 <code>Asymmetric Pyramid Non-local Block</code><br />
二者对 <code>Non-local</code> 的 <code>Self-Attention</code> 进行简化，例如 <code>share key value</code></p>
</blockquote>
<h3 id="16-gcnet"><a class="markdownIt-Anchor" href="#16-gcnet"></a> 16. GCNet</h3>
<ul>
<li><code>GCNet</code> 的全称是 <strong><code>Global Context Network</code></strong>，作者认为 <code>Non-local</code> 对全局信息把握的不够好，本文是简化版 <code>Non-local</code> + <code>SENet</code> 的缝合怪，<code>Non-local</code> 的 <code>Spatial Attention</code> 和 <code>SENet</code> 的 <code>Channel Attention</code> 结合</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.11492.pdf">https://arxiv.org/pdf/1904.11492.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/gc_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/gc_head.py</a><br />
<img src="https://s2.loli.net/2023/01/17/2e3AWOGEnfiRqdx.png" alt="GC2.png" /></li>
</ul>
<blockquote>
<p><code>Non-local</code> 结构的化简<br />
<img src="https://s2.loli.net/2023/01/17/wn6iBDPHM9h7pKC.png" alt="GC.png" /><br />
作者认为一个全局上下文建模结构如图 (a) 所示<br />
图 (b) 为简化后的 <code>Non-local</code> 结构<br />
图 © 是 <code>SENet</code> 结构<br />
图 (d) 是本文提出的 <code>GC</code> 结构</p>
</blockquote>
<ul>
<li><code>decode head</code> 前后处理和 <code>FCN</code> 一致</li>
</ul>
<h3 id="17-ocrnet"><a class="markdownIt-Anchor" href="#17-ocrnet"></a> 17. OCRNet</h3>
<ul>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.11065.pdf">https://arxiv.org/pdf/1909.11065.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/ocr_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/ocr_head.py</a></li>
<li><code>OCR</code> 的全称是 <strong><code>Object Contextual Representations</code></strong>（目标上下文表征）<s>而不是 <code>Optical Character Recognition</code>（光学字符识别）</s>，和前面的模型结构不同，<code>OCRNet</code> 是一种 <code>Cascade Encoder Decoder</code> 结构的 <code>decode head</code> ，该算法依赖于其他算法输出的分割结果，如下图所示（<code>OCRNet</code> 依赖于 <code>FCN</code> 的输出）：<br />
<img src="https://s2.loli.net/2023/01/18/yJZBx2v1YOuPlir.png" alt="OCR.png" /></li>
</ul>
<blockquote>
<p>上图中粉红色的部分即为 <code>OCRNet decode head</code><br />
<img src="https://s2.loli.net/2023/01/18/TQgwaJMGYnSDux6.png" alt="OCR2.png" /><br />
论文中给出的算法架构图，给中间结果赋予了可解释的含义</p>
</blockquote>
<h3 id="18-apcnet"><a class="markdownIt-Anchor" href="#18-apcnet"></a> 18. APCNet</h3>
<ul>
<li><code>APCNet</code> 的全称是 <code>Adaptive Pyramid Context Network</code>（自适应金字塔上下文），该算法引入了 <code>Adaptive Context Modules（ACM）</code>（自适应上下文模块），本质就是通过矩阵乘实现全局 <code>Attention</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Adaptive_Pyramid_Context_Network_for_Semantic_Segmentation_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Adaptive_Pyramid_Context_Network_for_Semantic_Segmentation_CVPR_2019_paper.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/apc_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/apc_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/7YmA9BQneXJDS8P.png" alt="APC.png" /></li>
</ul>
<h3 id="19-dmnet"><a class="markdownIt-Anchor" href="#19-dmnet"></a> 19. DMNet</h3>
<ul>
<li><code>DMNet</code> 的全称是 <code>Dynamic Multi-scale Filters Network</code>，本文根据输入特征动态获得多种尺度的卷积核参数，本质也是一种全局 <code>Attention</code> 机制</li>
<li>paper: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Dynamic_Multi-Scale_Filters_for_Semantic_Segmentation_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Dynamic_Multi-Scale_Filters_for_Semantic_Segmentation_ICCV_2019_paper.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dm_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dm_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/dlIVOLQYiEtvaJj.png" alt="DMNet2.png" /></li>
</ul>
<blockquote>
<p>之前的网络结构都是通过空洞卷积或大卷积核实现多尺度<br />
<code>DMNet</code> 通过输入特征的 <code>Adaptive Pooling</code> 生成动态卷积核实现多尺度<br />
<img src="https://s2.loli.net/2023/01/25/VCFliXsh2efmM54.png" alt="DMNet.png" /></p>
</blockquote>
<h3 id="20-lraspp"><a class="markdownIt-Anchor" href="#20-lraspp"></a> 20. LRASPP</h3>
<ul>
<li><code>LRASPP</code> 全称是 <code>Lite Reduced Atrous Spatial Pyramid Pooling</code>（轻量简化空洞空间金字塔池化），是在 <code>MobileNet V3</code> 论文中提出的结构，是和 <code>MobileNet V2</code> 提出的 <code>RASPP</code> 结构对比，更轻量效果更好；从实现上看 <code>LRASPP</code> 并没有空洞卷积和空间金字塔池化…，而是通过全局 <code>scale</code> 实现的 <code>Attention</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.02244.pdf">https://arxiv.org/pdf/1905.02244.pdf</a> (<strong><code>MobileNet V3</code></strong>)</li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dm_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dm_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/yF85ZoGsqYIKEDP.png" alt="LRASPP.png" /></li>
</ul>
<h3 id="21-isanet"><a class="markdownIt-Anchor" href="#21-isanet"></a> 21. ISANet</h3>
<ul>
<li><code>ISANet</code> 的全称是 <code>Interlaced Sparse Attention Network</code>（交错稀疏注意力网络），通过 <code>feature map shuffle</code> 实现长范围和短范围的稀疏注意力。</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.12273.pdf">https://arxiv.org/pdf/1907.12273.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/isa_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/isa_head.py</a><br />
<img src="https://s2.loli.net/2023/01/26/q2KaO94bMpnQdlu.png" alt="ISANet.png" /></li>
</ul>
<blockquote>
<ul>
<li>利用 <code>feature map</code> 重排实现长范围或短范围的稀疏注意力。</li>
</ul>
</blockquote>
<h3 id="22-dnlnet"><a class="markdownIt-Anchor" href="#22-dnlnet"></a> 22. DNLNet</h3>
<ul>
<li><code>DNL</code> 的全称是 <code>Disentangled Non-Local</code>（分离 <code>Non-local</code>），对原始 <code>Non-local</code> 做了改进，参数量和计算量更高，效果更好</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.06668.pdf">https://arxiv.org/pdf/2006.06668.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dnl_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dnl_head.py</a><br />
<img src="https://s2.loli.net/2023/01/18/TDshKaot8xqC2BE.png" alt="DNL.png" /></li>
</ul>
<blockquote>
<p><code>DNL</code> 结构（图 d）在原始 <code>Non-local</code> 结构（图 a）上做了如下改动：</p>
<ol>
<li>加入了一元 <code>Non-local</code> 分支 <code>Unary Non-local</code></li>
<li>在二元分支矩阵乘之前加入了白化操作（ <code>H*W</code> 维度减均值，相当于 <code>instance norm</code>）<br />
<img src="https://s2.loli.net/2023/01/18/f6riCoLFzTYN2RH.png" alt="DNL2.png" /><br />
由于减了均值，所以二元分支上 “+” 这一点在 <code>Attention map</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mi>W</mi><mo>×</mo><mi>H</mi><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\in \mathbb{R}^{HW\times HW}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span></span></span> 上的索引 <code>heat map</code> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\in \mathbb{R}^{H\times W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span></span></span> 变干净很多（相当只学习残差）<br />
这张图也从侧面反映了 <code>Non-local</code> 还是很强的，<code>Attention</code> 不是在讲故事</li>
</ol>
</blockquote>
<h3 id="23-bisenet"><a class="markdownIt-Anchor" href="#23-bisenet"></a> 23. BiSeNet</h3>
<ul>
<li><code>BiSeNet</code> 的全称是 <code>Bilateral Segmentation Network</code>（双边分割网络），是一个分割专用的神经网络（包括专用 <code>backbone</code> 和 <code>decode head</code>）</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00897.pdf">https://arxiv.org/pdf/1808.00897.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/bisenetv1.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/bisenetv1.py</a><br />
<img src="https://s2.loli.net/2023/01/28/VfMGKkSAnR1zwBv.png" alt="BiSenet2.png" /></li>
</ul>
<blockquote>
<ul>
<li><code>backbone</code> 主要分成两个分支 <code>spatial path</code> 和 <code>context path</code>，本质就是在基础 <code>backbone</code> 的基础上加入了一个计算量（通道数）非常小的 <code>attention branch</code> 增加上下文信息，最后融合两通道特征送入 <code>decode head</code></li>
<li><code>decode head</code> 就是基础的 <code>FCN</code></li>
</ul>
</blockquote>
<h3 id="24-bisenet-v2"><a class="markdownIt-Anchor" href="#24-bisenet-v2"></a> 24. BiSeNet V2</h3>
<ul>
<li><code>BiSeNet</code> 的改进版</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.02147.pdf">https://arxiv.org/pdf/2004.02147.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/bisenetv2.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/bisenetv2.py</a><br />
<img src="https://s2.loli.net/2023/01/28/zDHs1mFn5Cu43RE.png" alt="BiSenet_v2_1.png" /></li>
</ul>
<blockquote>
<p>对 <code>BiSeNet</code> 主要改进有:</p>
<ul>
<li><code>context branch</code> 上增加了更多更复杂的模块，可更好收集上下文信息</li>
<li><code>context branch</code> 上增加了更多监督，每个尺度上都有监督损失</li>
<li>分支融合模块设计的更加复杂</li>
</ul>
</blockquote>
<h3 id="25-sdtc"><a class="markdownIt-Anchor" href="#25-sdtc"></a> 25. SDTC</h3>
<ul>
<li><code>SDTC</code> 的全称是 <code>Short-Term Dense Concatenate network</code>，在 <code>BiSeNet</code> 系列的基础上将 <code>context branch</code> 变成训练时的监督（或者说融合两路信息到一路上）</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.13188.pdf">https://arxiv.org/pdf/2104.13188.pdf</a></li>
<li>code:
<ul>
<li>backbone: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/SDTC.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/SDTC.py</a></li>
<li>decode head: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/sdtc_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/sdtc_head.py</a><br />
<img src="https://s2.loli.net/2023/01/28/fBdi6THRtnpcYLI.png" alt="SDTC.png" /></li>
</ul>
</li>
</ul>
<blockquote>
<p>很新颖的 Loss 设计，效果和计算量都优于 <code>BiSeNet</code> 系列<br />
<img src="https://s2.loli.net/2023/01/28/9HqMVUDIJQg3fdi.png" alt="SDTC2.png" /><br />
这就是 <code>SDTC</code> 模块</p>
</blockquote>
<h3 id="26-setr"><a class="markdownIt-Anchor" href="#26-setr"></a> 26. SETR</h3>
<ul>
<li><code>SETR</code> 的全称是 <code>Segmentation Transformer</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.15840.pdf">https://arxiv.org/pdf/2012.15840.pdf</a></li>
<li>code:
<ul>
<li>backbone: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/vit.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/vit.py</a></li>
<li><code>SETR_PUP_decode_head</code>: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/setr_up_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/setr_up_head.py</a></li>
<li><code>SETR_MLA_decode_head</code>: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/setr_mla_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/setr_mla_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/MLDcPIBwT8kaoqJ.png" alt="SETR.png" /></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>本质是 <code>Vit（vision transformer）</code> 做 <code>backbone</code>，<code>FCN</code> / 类似 <code>FPN</code> 做 <code>decode head</code> 的分割算法</li>
<li>为了缩减计算量，<code>Vit</code> 会将原图剪成多个 <code>patch</code>（<code>worth 16x16 words...</code>），每个 <code>patch</code> 单独输入到 24 层 <code>Transformer Encoder</code> 中，每个 <code>patch</code> 内部独立做全局 <code>attention</code></li>
<li>剪 <code>patch</code> 带来的问题是：与其他 <code>CNN backbone + decode head</code> 结构不同，<code>Transformer backbone + decode head</code> 结构中 <code>decode head</code> 需要顺序 <code>inference</code> 每个 <code>patch feature</code>（注意图a <code>Decoder</code> 输入为多个 <code>patch feature</code>），最后拼回到整张图大小</li>
<li><code>SETR_UPU decode head</code> == <code>sequence FCN</code></li>
<li><code>SETR_MLA decode head</code> == <code>sequence FPN</code>（<code>Attention</code> 不改变输入宽高，所以不存在严格意义上的 <strong>多尺度</strong>，只是不同网络深度的特征）</li>
</ul>
</blockquote>
<h3 id="27-dptnet"><a class="markdownIt-Anchor" href="#27-dptnet"></a> 27. DPTNet</h3>
<ul>
<li><code>DPTNet</code> 的全称是 <code>Dense Prediction Transformer Network</code>，本质和 <code>SETR</code> 一样都是使用 <code>Vit</code> 做 <code>backbone</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.13413.pdf">https://arxiv.org/pdf/2103.13413.pdf</a></li>
<li>code:
<ul>
<li>backbone: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/vit.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/vit.py</a></li>
<li>decode head: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dpt_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/dpt_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/oSCVpNAwgGI4bBs.png" alt="DPT.png" /></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>本质和 <code>SETR</code> 一样都是使用 <code>Vit</code> 做 <code>backbone</code></li>
<li>和 <code>SETR</code> 不同的地方在于：
<ul>
<li>不同 <code>backbone</code> 深度特征融合方式更复杂，更接近 <code>FPN</code></li>
<li><code>decode head</code> 不再是输入 <code>sequence patch feature</code>，而是输入融合后的全图 <code>feature</code></li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="28-segmenter"><a class="markdownIt-Anchor" href="#28-segmenter"></a> 28. Segmenter</h3>
<ul>
<li><code>Segmenter</code> 全称是 <code>Segmentation Transformer</code>，用了纯 <code>Transformer</code> 架构而不是 <code>Transformer Encoder + CNN Decoder</code> 架构</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.05633.pdf">https://arxiv.org/pdf/2105.05633.pdf</a></li>
<li>code:
<ul>
<li>backbone: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/vit.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/vit.py</a></li>
<li>decode head: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/segmenter_mask_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/segmenter_mask_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/xhemlYgjH3Vw5ks.png" alt="Segmenter.png" /></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>用了纯 <code>Transformer</code> 架构（<code>Transformer Encoder + Decoder</code>），<code>SETR</code> 和 <code>DPT</code> 都是 <code>Transformer Encoder + CNN Decoder</code></li>
</ul>
</blockquote>
<h3 id="29-segformer"><a class="markdownIt-Anchor" href="#29-segformer"></a> 29. SegFormer</h3>
<ul>
<li><code>SegFormer</code> 全称也是 <code>Segmentation Transformer</code>…，是 <code>NVIDIA</code> 对 <code>SETR</code> 的高效实现版，<code>backbone</code> 和 <code>decoder head</code> 都进行了轻量化升级</li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.15203.pdf">https://arxiv.org/pdf/2105.15203.pdf</a></li>
<li>code:
<ul>
<li>backbone: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/mit.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/mit.py</a></li>
<li>decoder head: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/segformer_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/segformer_head.py</a><br />
<img src="https://s2.loli.net/2023/01/25/o2VTBwjG8NkUA5H.png" alt="SegFormer.png" /></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><code>backbone</code> 不再是标准 <code>Transformer Encoder（Vit）</code>，而是改成了更轻量化的 <code>MixVisionTransformer（Mit）</code>
<ul>
<li><code>Mit</code> 使用了更大的 <code>patch</code> 且 <code>patch</code> 之间存在 <code>overlap</code></li>
<li><code>Mit</code> 使用了 <code>coarse-to-fine</code> 的特征表示，随着 <code>Transformer Encoder</code> 变深 <code>feature map</code> 宽高变小</li>
<li><code>Mit</code> 使用了更简单的 <code>Self-Attention</code> 公式</li>
<li><code>Mit</code> 去掉了 <code>position embeding</code>，使用了 <code>Mix-FFN</code></li>
</ul>
</li>
<li><code>decode head</code> 使用了纯 <code>MLP</code>，且很自然的融合了多尺度（<s>真.多尺度</s>）</li>
</ul>
</blockquote>
<h3 id="30-knet"><a class="markdownIt-Anchor" href="#30-knet"></a> 30. KNet</h3>
<ul>
<li><code>KNet</code> 的全称是 <code>Kernel Network</code>，是一种跳出语义分割、实例分割、全景分割原有框架的一种新分割范式，用一组 <code>kernel</code> 去预测一个分割 <code>mask</code>，最多预测 <code>num_proposals</code> 个（类似 <code>DETR</code> 的策略），训练时用最优匹配的方法计算损失函数；优点是在框架上统一了所有分割任务（语义分割、实例分割、全景分割），缺点 <code>decode head</code> 就是实现复杂，融合了 <code>Channel Attention + Multi-head Attention + RNN</code></li>
<li>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.14855.pdf">https://arxiv.org/pdf/2106.14855.pdf</a></li>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/knet_head.py">https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/knet_head.py</a><br />
<img src="https://s2.loli.net/2023/01/30/WHFi1SLMU6ZQcT5.png" alt="KNet2.png" /></li>
</ul>
<blockquote>
<p>从框架上统一了三种分割方式<br />
<img src="https://s2.loli.net/2023/01/30/D7PkZdmbtJexFjI.png" alt="KNet.png" /><br />
红字标出的是每个张量的 <code>shape</code><br />
绿字标出的是每个计算过程实际是在做什么<br />
上述过程会像 <code>RNN</code> 一样循环多次去更新 <code>kernel</code>，使得结果更好（重复使用 <code>backbone</code> 的输出）</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Paper-Reading/" rel="tag"># Paper Reading</a>
              <a href="/tags/Semantic-Segmentation/" rel="tag"># Semantic Segmentation</a>
              <a href="/tags/Survey/" rel="tag"># Survey</a>
              <a href="/tags/Decode-Head/" rel="tag"># Decode Head</a>
              <a href="/tags/MMSegmentation/" rel="tag"># MMSegmentation</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/12/25/Learning-Diverse-and-Discriminative-Representations-via-the-Principle-of-Maximal-Coding-Rate-Reduction/" rel="prev" title="Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction">
      <i class="fa fa-chevron-left"></i> Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/09/tetris/" rel="next" title="tetris">
      tetris <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#topic"><span class="nav-number">1.</span> <span class="nav-text"> Topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text"> 语义分割推理过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%8E%9F%E5%A7%8B%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-number">2.1.</span> <span class="nav-text"> 1. 原始特征处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81"><span class="nav-number">2.2.</span> <span class="nav-text"> 2. 特征解码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84%E5%88%B0%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E7%A9%BA%E9%97%B4"><span class="nav-number">2.3.</span> <span class="nav-text"> 3. 特征映射到分割任务空间</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BC%94%E8%BF%9B%E8%BF%87%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text"> 演进过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E4%BB%A3%E5%9C%A8-cnn-%E7%BB%93%E6%9E%84%E4%B8%8A%E5%88%9B%E6%96%B0"><span class="nav-number">3.1.</span> <span class="nav-text"> 第一代：在 CNN 结构上创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E4%BB%A3self-attention-non-local-channel-attention"><span class="nav-number">3.2.</span> <span class="nav-text"> 第二代：Self-Attention (Non-local &#x2F; Channel Attention)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E4%BB%A3transformer"><span class="nav-number">3.3.</span> <span class="nav-text"> 第三代：Transformer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#algorithms"><span class="nav-number">4.</span> <span class="nav-text"> Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-fcn"><span class="nav-number">4.1.</span> <span class="nav-text"> 1. FCN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-%E5%8E%9F%E5%A7%8B%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-number">4.1.1.</span> <span class="nav-text"> 1.1 原始特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81"><span class="nav-number">4.1.2.</span> <span class="nav-text"> 1.2 特征解码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-psp"><span class="nav-number">4.2.</span> <span class="nav-text"> 2. PSP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#21-%E5%8E%9F%E5%A7%8B%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-number">4.2.1.</span> <span class="nav-text"> 2.1 原始特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#22-%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81"><span class="nav-number">4.2.2.</span> <span class="nav-text"> 2.2 特征解码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-aspp"><span class="nav-number">4.3.</span> <span class="nav-text"> 3. ASPP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#31-%E5%8E%9F%E5%A7%8B%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-number">4.3.1.</span> <span class="nav-text"> 3.1 原始特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#32-%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81"><span class="nav-number">4.3.2.</span> <span class="nav-text"> 3.2 特征解码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-fpn"><span class="nav-number">4.4.</span> <span class="nav-text"> 4. FPN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-upernet"><span class="nav-number">4.5.</span> <span class="nav-text"> 5. UperNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#51-%E5%8E%9F%E5%A7%8B%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86"><span class="nav-number">4.5.1.</span> <span class="nav-text"> 5.1 原始特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#52-%E7%89%B9%E5%BE%81%E8%A7%A3%E7%A0%81"><span class="nav-number">4.5.2.</span> <span class="nav-text"> 5.2 特征解码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-depthwiseseparableaspp"><span class="nav-number">4.6.</span> <span class="nav-text"> 6. DepthwiseSeparableASPP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-depthwiseseparablefcn"><span class="nav-number">4.7.</span> <span class="nav-text"> 7. DepthwiseSeparableFCN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-pointrend"><span class="nav-number">4.8.</span> <span class="nav-text"> 8. PointRend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-non-local"><span class="nav-number">4.9.</span> <span class="nav-text"> 9. Non-Local</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-psanet"><span class="nav-number">4.10.</span> <span class="nav-text"> 10. PSANet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-ccnet"><span class="nav-number">4.11.</span> <span class="nav-text"> 11. CCNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-danet"><span class="nav-number">4.12.</span> <span class="nav-text"> 12. DANet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-encnet"><span class="nav-number">4.13.</span> <span class="nav-text"> 13. EncNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-emanet"><span class="nav-number">4.14.</span> <span class="nav-text"> 14. EMANet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-ann"><span class="nav-number">4.15.</span> <span class="nav-text"> 15. ANN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-gcnet"><span class="nav-number">4.16.</span> <span class="nav-text"> 16. GCNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-ocrnet"><span class="nav-number">4.17.</span> <span class="nav-text"> 17. OCRNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-apcnet"><span class="nav-number">4.18.</span> <span class="nav-text"> 18. APCNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-dmnet"><span class="nav-number">4.19.</span> <span class="nav-text"> 19. DMNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#20-lraspp"><span class="nav-number">4.20.</span> <span class="nav-text"> 20. LRASPP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#21-isanet"><span class="nav-number">4.21.</span> <span class="nav-text"> 21. ISANet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-dnlnet"><span class="nav-number">4.22.</span> <span class="nav-text"> 22. DNLNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-bisenet"><span class="nav-number">4.23.</span> <span class="nav-text"> 23. BiSeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#24-bisenet-v2"><span class="nav-number">4.24.</span> <span class="nav-text"> 24. BiSeNet V2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#25-sdtc"><span class="nav-number">4.25.</span> <span class="nav-text"> 25. SDTC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#26-setr"><span class="nav-number">4.26.</span> <span class="nav-text"> 26. SETR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#27-dptnet"><span class="nav-number">4.27.</span> <span class="nav-text"> 27. DPTNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#28-segmenter"><span class="nav-number">4.28.</span> <span class="nav-text"> 28. Segmenter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#29-segformer"><span class="nav-number">4.29.</span> <span class="nav-text"> 29. SegFormer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#30-knet"><span class="nav-number">4.30.</span> <span class="nav-text"> 30. KNet</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhangzhe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">173</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">104</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhangzhe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js', () => {
    // 初始化 Mermaid 配置
    mermaid.initialize({
      theme    : 'dark',  // 设置主题
      logLevel : 3,  // 设置日志等级
      flowchart: { curve: 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 },
      themeVariables: {
        'fontFamily': 'Microsoft YaHei, Arial, sans-serif',  // 设置中文字体
      }
    });

    // 初始化 Mermaid 图表
    mermaid.init(undefined, document.querySelectorAll('pre.mermaid'));
  }, window.mermaid);
}
</script>


  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">


  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'eK3W25jybCO5jVUrYBBpAPqM-gzGzoHsz',
      appKey     : 'F4KVyUj9wHI5c80Bhz7O2uhq',
      placeholder: "说点什么再走吧...",
      avatar     : 'hide',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
